---
layout: post
title: "Logistic回归&SoftMax回归"
date: 2018-08-07
categories: 机器学习
tags: [MachineLearning]
---

<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<title>Logistic回归&SoftMax回归</title>
	</head>
<body>
<h2>序言</h2>
<p> 之前的文章中我们介绍了普通最小二乘线性回归算法，并进行了较为详细的推导，并通过分析其过拟合的问题，推导出了另外三个算法，Ridge回归算法，LASSO回归算法，以及弹性网络。并简要的分析了他们的优缺点。今天我们来接着介绍算法。为什么说是算法而不是回归算法呢，是因为在研究了逻辑回归和Softmax回归算法以后，惊讶的发现这两个算法是分类算法，所以这个回归算法下的说法就不是很严谨了。</p>

<h2>Logistic回归</h2>

<p>在回归算法上中我们介绍了，线性回归算法。我个人是这么拆分的，线性-回归-算法，回归指的整体的算法是回归算法而不是分类算法，线性指的是在算法回归中，假定输入输出之间关系函数是$y = kx + b$这个线性方程。</p>

<p>那么再看看Logistic回归，显然这里肯定就是换了另一个种类的方程咯。对的，不过在逻辑回归中这里有一点不太一样，它的输出只有0和1，它假定输出 $y = 1$的概率$P$，自然 $y = 0$的概率就是 $1-P$，而逻辑回归中假定的方程就是这个$P$。</p>

<p>现在我们来看看这个方程是什么，如下公式1.1所示。这个公式没有接触过的朋友可能不太好想这是一个什么函数，但是结合着我们高数的极限知识，我们肯定知道这个函数在$+\infty $的时候去趋近于1，在$-\infty$的时候趋近于0，而且是快速收敛的。建议百度sigmoid函数看看这个函数的图（Logistic函数就是sigmoid函数）。</p>

<ul>
	<li>$p=h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $		公式1.1</li>
</ul>

<p>之前在线性回归算法中我们知道了求最佳$\theta$的流程就是先写出他的似然函数（这个公式表示了预测正确的概率），然后求最大似然估计（想办法让这个预测正确的概率最大），最终通过求最大似然估计的结果，得到一种调整$\theta$的最佳方案。这个逻辑回归的似然函数为公式1.3(由公式1.2易得)。</p>

<ul>
	<li>$L(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}$	公式1.2</li>
	<li>$L(\vec y|x;\theta)=\prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}$					公式1.3</li>
</ul>

<p>之后对这个似然函数进行求导，我们都学过导数，在学导数的时候都知道，导数代表着值变化的趋势和速度。这里我们通过公式1.3对求对数，然后再对 $\theta_j $求导得到导数（公式1.4），这个导数就代表了针对$\theta_j $的正确预测的概率的变化趋势。如果让 $\theta$加上这个导数就可以保证让这个公式变大，直至最大为止。所以逻辑回归的调参如公式1.5，公式1.6所示，那么为什么是两个，在调参章节中会说明。</p>

<ul>
	<li>$\frac{\partial \ell(\theta)} {\partial \theta_j} = \sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j$ 公式1.4</li>
	<li>$\theta_j = \theta_j + \alpha \sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j$	公式1.5	</li>
	<li>$\theta_j = \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)}))x_j $公式1.6</li>
</ul>

<p>好的，到这里逻辑回归的调参我们就知道了，特别的说明的是，这里的 $alpha$就是传说中的学习速率，而这个参数我们要注意的是，它不能太大（会错过最大值而不能收敛），也不能太小（会收敛到局部最小值）。</p>

<p>那么我们有了调参，这里肯定也有一个损失函数来表示现在预测的怎么样吧，这里的思路很简单，你不是似然函数是越大越好么，那么我把你来个倒数不就好了么，最简单的做法就是给求对数之后的公式1.3，添加一个负号。所以它的损失函数如公式1.7。</p>

$loss(y^{(i)},\hat y^{(i)}) = -\ell(\theta) = \sum_{i=1}^m ln(1+e^{(1-2y^{(i)})\theta^T x^{(i)}}) $	公式1.7
<p>最后，想说从那个 只能为1或者0就可以看出逻辑回归虽然名字带了回归两个字，但是其实是一个分类算法。而接下来的SoftMax回归算法则是逻辑回归算法的一种拓展，这个在下一节里说。</p>

<h2>SoftMax回归</h2>

<p>在逻辑回归中，最终的分类结果，只有两类。这显然不是适用于很多其他的情况。所以SoftMax对逻辑回归进行了一般化，适用于K分类的问题。</p>

<p>针对K分类的问题，我们的小伙伴 $\theta$参数就不在是一个向量了，我们设第K类的参数为向量$\theta_k$， 则有n个属性的参数就成了一个二维矩阵$\theta_{k*n}$。</p>

<p>在Softmax回归中，我们设预测对第k类的概率为公式2.1。</p>

<p>$p(y=k|x;\theta)=\frac{e^{\theta_k^T x}}{\sum_{i=1}^K e^{\theta_i^T x}} $, $k=1,2 ...,K $公式2.1</p>

<p>剩下的分析同逻辑回归一样就不赘述了。</p>

<h2>机器学习调参</h2>

<p>我们在之前的学习中，了解到机器学习在迭代的过程就是不断的调整$/theta$参数的过程。有些是算法自己就调整了，有些是需要我们的人工的来调整的，这里就要引入超参这个概念了。什么事超参呢，超参就是不能通过算法自动调整的参数，比如Ridige回归和LASSO回归的 $\lambda$，弹性网络中的 $\alpha$。</p>

<p>除了这些，在Logistic回归这一章节中，我们发现了调参函数有两个，但是并没有说明为什么。在这里将进行详细介绍。在Logistic和Softmax回归中我们用到的调参方式，只要你稍微了解过深度学习就一定听过，这种调参方式就是大名鼎鼎的梯度下降算法。而这两个公式，公式1.5是批量梯度下降算法（BGD），公式1.6是随机梯度下降算法（SGD）。</p>

<p>通过公式1.6我们可以看出，每次迭代SGD调整一个$\theta_j$只需要和其中一条属性的比，而BGD每次迭代每调整一个$\theta_j$需要和所有属性的比较，所以SGD迭代速度快。</p>

<p>那么SGD每次迭代考虑的属性少会不会没有BGD准呢，这是不一定的。梯度下降算法就像大雾天气下山，我们只能看清眼前的一小部分，并以此为依据下山，我们很有可能最后在山的上的一个小坑里出不来，陷入局部最优解的问题。SGD在全局餐在多个相对最优解的情况下，SGD很有可能跳出某些局部最优解，所以不一定会比BGD坏。而BGD一定能够得到一个局部最优解（在线性回归中一定是得到一个全局最优解）。不过由于SGD少考虑一些情况，所以有随机性，因而最终结果可能会比BGD差，一般情况下我们会优先使用SGD。</p>

<p><strong>注：</strong>个人的简单理解是，SGD每次只会往某一个坐标轴方向走一步，而BGD则是结合所有坐标轴的情况，往一个空间内的一个方向走一步。所以理论上BGD很“理性”。</p>

<p>SGD和BGD出现的优缺点的情况是不是很熟悉，是不是很像Ridge回归和LASSSO回归的抉择，最终出现了弹性网络。那么是不是也有一个类似梯度下降算法的“弹性网络”呢，答案显然是有的，那就是MBGD，我不全不考虑完不就好了嘛。MBGD中每次拿b个样本的平均梯度作为更新方向，这里的b一般为10。这样子就既保证了速度，也一定程度上保证了准确度。</p>

<h2>模型效果判断</h2>

<p>最后，我们模型也训练好了，那么怎么来判定我的模型是不是合乎标准的，就像吴军老师在Google方法论说的一样，在工程中不是只有对错，只有相对的好和相对的不好，这个模型是否符合项目的需要也是有几个标准的，在训练的时候需要注意最终目标需要达到的标准是什么。常用的有MSE（公式4.1）、RMSE（公式4.2）、$R^2$（公式4.3）。</p>

<p>在看公式前先说明TSS和RSS的概念，用在算$R^2$上。</p>

<p>TSS(Total SUm of Squares)：总平方和TTS，样本之间的差异性。是伪方差的m倍。RSS：是预测值和样本值之间的差异，是MSE的m倍。</p>

$$
MSE=\frac{1}{m}\sum_{i=1}^m(y_i - \hat y_i)^2\ \ \ 公式4.1
$$
$$
RMSE=\sqrt{MSE} \ \ \ 公式4.2
$$
$$
R^2 = 1 - \frac{RSS}{TSS} = 1-\frac{\sum_{i=1}^m(y_i-\hat y_i)^2}{\sum_{i=1}^m(y_i-\overline y)^2}\ \ \ 公式4.3
$$				
<p>通过看公式我们可以得到以下的结论。</p>

<p>MSE：误差平方和，越趋近于0越拟合。</p>

<p>RMSE：就是对MSE开根号，所以和MSE的判别一样。</p>

<p>$R^2$：值域为，最优解是1，若预测值恒为样本期望则为0</p>

<p>除了这些，还有一个叫做混淆矩阵的东西，也是评价模型的一种手段，可以之后搜索看看。</p>


<h2>结语</h2>
<p>这些算法算是机器学习早期的故事，我们可以看到，算法的出现非常符合人类思维发展模式。</p>

<p>算法是由具象到抽象的，开始的时候，说到分类我们很容易想到一刀切的方式，于是出现了线性回归，与此同时我们又想出了用概率表示的方式于是出现了逻辑回归。当香农提出了信息熵之后，又出现了信息熵来表示分类是否合适的算法思路。</p>

<p>算法也是又简单到复杂的，之后随着线性回归算法的使用，我们发现出现了过拟合，于是我们用正则项来解决，于是出现了LASSO和Ridge算法。之后为了让这个“一刀切”更好，出现了SVM。而概率的那条路就走的更远一点，在考虑了特征之间的关系后，出现了贝叶斯网络，更进一步在考虑到了隐变量之后出现了HMM，慢慢的也能看见现在的神经网络的雏形。</p>

<p>最后，个人觉得算法是一种工具，如果不能灵活使用的话，充其量只能算一种思辨游戏。祝大家
HappyCoding。</p>
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>

