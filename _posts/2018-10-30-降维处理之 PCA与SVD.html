---
layout: post
title: "降维处理之 PCA 与 SVD"
date: 2018-10-30
categories: MathTools
tags: ["NLP" , "MachineLearning", "MathTools"]
---
<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>降维处理之 PCA 与 SVD</title>
	</head>
<body>
<h2>PCA</h2>
<h3>简要说明</h3>
<p>PCA 的全称为<strong>主成分分析</strong>（Principal Component Analysis）。简单的来说，PCA 的目的是将原来的坐标系旋转为新的坐标系，新的坐标系的选择是由数据本身决定的。第一个坐标系的选择是原始数据中方差最大的方向，第二个是和第一个坐标系正交（orthogonal）且方差最大的坐标轴，第三个，第四个均重复上述操作，重复次数为原始数据中特征的数目<sup><a id="ffn1" href="#fn1" class="footnote">1</a></sup>。</p>

<h3>实现手法</h3>
<p>实际上我们是通过协方差矩阵<sup><a id="ffn2" href="#fn2" class="footnote">2</a></sup>及其特征分析来求这些主成分的值的。其中特征分析就是指的是求他们的特征向量和特征值。</p>

<blockquote>
<p>协方差能导出一个<a href="https://zh.wikipedia.org/wiki/%E5%8F%98%E6%8D%A2%E7%9F%A9%E9%98%B5" title="变换矩阵">变换矩阵</a>，这个矩阵能使数据完全去相关(decorrelation)。从不同的角度看，也就是说能够找出一组最佳的基以紧凑的方式来表达数据。(完整的证明请参考<a href="https://zh.wikipedia.org/w/index.php?title=%E7%91%9E%E5%88%A9%E5%95%86&amp;action=edit&amp;redlink=1" title="瑞利商（页面不存在）">瑞利商</a>)。 这个方法在统计学中被称为<a href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90" title="主成分分析">主成分分析</a>(principal components analysis)，在图像处理中称为Karhunen-Loève 变换(KL-变换)。</p>

<p>——— Wiki</p>
</blockquote>

<p>具体实现的伪代码如下：</p>

<pre><code class="code-highlighted code-python"><span class="syntax-all syntax-comment"># 去除平均值 （使用numpy中的mean）
</span><span class="syntax-all syntax-comment"># 计算协方差矩阵 （使用numpy中的cov函数，根据定义构建）
</span><span class="syntax-all syntax-comment"># 计算协方差矩阵的特征值和特征向量（使用numpy中的eig()函数返回特征值和特征向量）
</span><span class="syntax-all syntax-comment"># 将特征值从大到小排序（使用numpy中的argsort(),得到最大的特征值对应的特征向量的位置）
</span><span class="syntax-all syntax-comment"># 保留最大的N个特征向量
</span><span class="syntax-all syntax-comment"># 将数据转换到上述N个特征向量构建的新空间
</span></code></pre>

<h2>SVD</h2>

<h3>简述</h3>

<p>SVD（Singular Value Decomposition），即奇异值分解，本质上是一种矩阵分解技术，在应用领域已经出现了进百年。矩阵分解技术指的是将一个原始矩阵表示成新的易于处理的形式，这种形式是两个或多个矩阵的乘积，可以简单的理解为因式分解。最早使用 SVD 的应用的领域为信息检索，使用 SVD 的检索方法称为隐性语义索引（Latent Semantic Index，<strong>LSI</strong>）或隐性语义分析（Latent Semantic Analysis，<strong>LSA</strong>）。</p>

<h3>细节说明</h3>

<p>SVD 将原始的数据集矩阵 $Data$ 分解成三个矩阵 $U$、 $\Sigma$ 、 $V^T$。若原始矩阵是 $m*n$ 矩阵，那么 $U$ 为 $m*m$， $\Sigma$ 为 $m*n$， $V^T$ 为 $n*n$，写成公式如下<sup><a id="ffn3" href="#fn3" class="footnote">3</a></sup>：</p>

$$
Data_{m*n} = U_{m*m}\Sigma_{m*n}V^T_{n*n} 
$$
<p>其中 $\sum$ 为对角矩阵，这些对角上的值为<strong>奇异值</strong>，这个奇异值和特征值是有关系的，它是矩阵 $Data*Data^T$ 的特征值的平方根。</p>

<p>实现方式已经很成熟啦，使用numpy的库就好啦，函数如下：</p>

<pre><code class="code-highlighted code-python"><span class="syntax-all syntax-keyword">from</span> numpy <span class="syntax-all syntax-keyword">import</span> <span class="syntax-all syntax-keyword">*</span>
<span class="syntax-all syntax-constant">U</span>,Sigma,<span class="syntax-all syntax-constant">VT</span> <span class="syntax-all syntax-keyword">=</span> linalg.svd(Matrix)    </code></pre>

<p><em>注：详细实现过程参考《Numerical Linear Algebra》<sup><a id="ffn4" href="#fn4" class="footnote">4</a></sup></em></p>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<ol id="footnotes">
	<li id="fn1">《机器学习实战》 P243 <a href="#ffn1">&#x21A9;&#xFE0E;</a></li>
	<li id="fn2">https://zh.wikipedia.org/wiki/协方差矩阵 <a href="#ffn2">&#x21A9;&#xFE0E;</a></li>
	<li id="fn3">《机器学习实战》 P254 <a href="#ffn3">&#x21A9;&#xFE0E;</a></li>
	<li id="fn4">L.Trefethen and D.Bau III, Numerical Linear Algebra(SIAM: Society for Industrial and Applied Mathematics, 1997) <a href="#ffn4">&#x21A9;&#xFE0E;</a></li>
</ol></body>
</html>

