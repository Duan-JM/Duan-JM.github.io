---
layout: post
title: "决策树&ID3,C4.5,CART"
date: 2018-08-09
categories: 机器学习
tags: [MachineLearning]
---
<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>决策树&amp;ID3,C4.5,CART</title>
	</head>
<body>
<h3>信息熵</h3>
<p><strong>信息量:</strong>指的是一个样本/事件所蕴含的信息，如果一个事件的概率越大，那么就 可以认为该事件所蕴含的信息越少。</p>

<p><strong>信息熵:</strong>一个系统越是有序， 信息熵就越低，一个系统越是混乱，信息熵就越高，所以 信息熵被认为是一个系统有序程度的度量。</p>

<p><strong>信息熵就是用来描述系统信息量的不确定度。</strong></p>

$$
H(x) = - \sum_{i=1}^m p_i log_2(p_i)
$$
<p><strong>条件熵：</strong> 给定条件X的情况下，所有不同x值情况下Y的信息熵的平均值叫做条件熵。</p>

$$
H(Y|X) = \sum_{j=1}P(X = v_j)H(Y|X = v_j)
$$
$$
H(Y|X) = H(X,Y)-H(X)
$$
<h3>决策树定义</h3>

<p>决策树(Decision Tree)是在已知各种情况发生概率的基础上，通过构建决策树来 进行分析的一种方式，是一种直观应用概率分析的一种图解法。决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系。决策树是一种树形结构， 其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节 点代表一种类别;决策树是一种非常常用的有监督的分类算法。</p>

<h3>构建过程</h3>

<p>构建步骤如下:</p>

<ol>
	<li>将所有的特征看成一个一个的节点;</li>
	<li>遍历每个特征的每一种分割方式，找到最好的分割点;将数据划分为不同的子节点，eg: N1、 N2....Nm;计算划分之后所有子节点的&#39;纯度&#39;信息;</li>
	<li>对第二步产生的分割，选择出最优的特征以及最优的划分方式;得出最终的子节点: N1、N2....Nm 4. 对子节点N1、N2....Nm分别继续执行2-3步，直到每个最终的子节点都足够&#39;纯&#39;。</li>
</ol>

<h3>特征属性类型</h3>

<p>根据特征属性的类型不同，在构建决策树的时候，采用不同的方式，具体如下: </p>

<ul>
	<li> 属性是离散值，而且不要求生成的是二叉决策树，此时一个属性就是一个分支</li>
	<li> 属性是离散值，而且要求生成的是二叉决策树，此时使用属性划分的子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支</li>
	<li>属性是连续值，可以确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支</li>
</ul>

<h3>决策树分割属性选择</h3>

<ul>
	<li>决策树算法是一种“贪心”算法策略，只考虑在当前数据特征情况下的最好分割方式，不能进行回溯操作。</li>
	<li>对于整体的数据集而言，按照所有的特征属性进行划分操作，对所有划分操作的结果集的“纯度”进行比较，选择“纯度”越高的特征属性作为当前需要分割的数据集进行分割操作，持续迭代，直到得到最终结果。决策树是通过“纯度”来选择分割特征属性点的。</li>
</ul>

<h3>量化纯度</h3>

<p>基尼系数：$Gini = 1 - \sum_{i=1}^nP(i)^2$</p>

<p>信息熵：$-\sum_{i=1}^n P(i)log_2(P(i))$</p>

<p>误差率：$Error = 1 - max_{i=1}^n \{P(i)\}$</p>

<p></p>

<p>上面是基础的一些系数，在剪枝和判断的时候需要一个种体现变化的系数。</p>

$$
Gain = \Delta = H(D)-H(D|A)
$$
<p>可以这么理解，是否决策树以A为节点划分后，信息量增加了。也可以间接体现决策树越往下发生该事件的概率越小信息量越大。</p>

<h3>停止条件</h3>

<p>决策树构建的过程是以恶递归的过程，以下是两个停止条件。</p>

<ol>
	<li>每个字节点只有一种类型时停止条件。（容易过拟合）</li>
	<li>节点中记录数小于某个阀值的时候，或者迭代次数达到给定值的时候停止构建。</li>
</ol>

<h3>决策树的评估</h3>

$$
loss = \sum_{t=1}^{leaf} \frac{|D_t|}{D}H(t)
$$
<p>其中$H(t)$前的参数$\frac{D_t}{d}$主要的目的其实是给信息熵加权值。</p>

<h3>算法对比</h3>

<h4>ID3算法优缺点</h4>

<p>ID3算法是决策树的一个经典的构造算法，内部使用<strong>信息熵</strong>以及<strong>信息增益</strong>来进行 构建;每次迭代选择信息增益最大的特征属性作为分割属性。</p>

<p>优点：</p>

<ul>
	<li>决策树构建速度快，实现简单。</li>
</ul>

<p>缺点：</p>

<ul>
	<li>计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优</li>
	<li>ID3算法不是递增算法 ID3算法是单变量决策树，对于特征属性之间的关系不会考虑 抗噪性差</li>
	<li>只适合小规模数据集，需要将数据放到内存中</li>
</ul>

<h4>C4.5</h4>

<p>在ID3算法的基础上，进行算法优化提出的一种算法(C4.5)。其使用<strong>信息增益率</strong>来取代ID3算法中的信息增益，在树的构造过程中<strong>会进行剪枝操作进行优化</strong>。能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。信息增益率公式如下：</p>

$$
Gain\_ratio(A) = \frac{Gain(A)}{H(A)}
$$
<p>优点：</p>

<ul>
	<li>产生的规则易于理解</li>
	<li>准确率较高实现简单</li>
</ul>

<p>缺点：</p>

<ul>
	<li>对数据集需要进行多次顺序扫描和排序，所以效率较低 </li>
	<li>只适合小规模数据集，需要将数据放到内存中</li>
</ul>

<h4>CART</h4>

<p>使用基尼系数作为数据纯度的量化指标来构建的决策树算法就叫做 CART(Classification And Regression Tree，分类回归树)算法。CART算法使用GINI增益作为分割属性选择的标准，选择GINI增益最大的作为当前数据集的分割属性。可用于分类和回归两类问题。<strong>注意的是：CART构建是二叉树，同时GINI系数的计算不牵扯对数运算比较快</strong></p>

<p>GINI增益公式如下：</p>

$$
Gain = \Delta = Gini(D) - Gini(D|A)
$$
<h4>总结</h4>

<ul>
	<li>ID3和C4.5算法均只适合在小规模数据集上使用</li>
	<li>ID3和C4.5算法都是单变量决策树</li>
	<li>当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差</li>
	<li>决策树分类一般情况只适合小数据量的情况(数据可以放内存)</li>
	<li>CART算法是三种算法中最常用的一种决策树构建算法。</li>
	<li>CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树。</li>
</ul>

<p>三种算法的区别仅仅只是对于当前树的评价标准不同而已，ID3使用信息增益、 C4.5使用信息增益率、CART使用基尼系数。</p>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>

