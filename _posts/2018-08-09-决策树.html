---
layout: post
title: "决策树"
date: 2018-08-09
categories: 机器学习
tags: [MachineLearning]
---
<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<title>决策树&amp;ID3,C4.6,CART</title>
	</head>
<body>

<h2>1、 前言</h2>
<p>之前我们已经说了，机器学习的从线性回归，概率这个出发点发展的算法。这次我们讲从第三个出发点，使用信息熵的算法，决策树。</p>

<h2>2、 信息熵</h2>

<p>首先我们来介绍什么是信息熵，信息熵是1948年，香农引入信息熵。一个系统<strong>越是有序， 信息熵就越低</strong>，一个系统越是混乱，信息熵就越高，所以信息熵被认为是一个系统有序程度的度量。举个例子，太阳从东边升起这个规律人人都知道，所以信息熵低，而GAN算法是怎么推导的知道的人就不多，所以它的信息熵高。</p>

<p><em>注：一个事件发生的概率大，那么它含有的信息少。</em></p>

<p>一言已概之就是：<strong>信息熵就是用来描述系统信息量的不确定度。</strong>其公式如下：</p>

$$
H(x) = - \sum_{i=1}^m p_i log_2(p_i)
$$
<p><strong>条件熵的定义为：</strong> 给定条件X的情况下，所有不同x值情况下Y的信息熵的平均值叫做条件熵。</p>

$$
H(Y|X) = \sum_{j=1}P(X = v_j)H(Y|X = v_j)
$$
$$
H(Y|X) = H(X,Y)-H(X)
$$
<h2>3、 纵览决策树算法</h2>

<p>在了解了这个算法的关键评判标准后，我们来看下决策树是什么，决策树 ( Decision Tree ) 是在已知各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的一种<strong>图解法</strong>。决策树是一种树形结构， 其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节点代表一种类别。</p>

<p>换句话说，决策树的使用是一种模拟了简单的人类思考过程的思路。通过许多<code>if...else...</code>进行判断最后得到一个结论。</p>

<h3>3.1、 决策树构建过程</h3>

<p>构建步骤如下:</p>

<ol>
	<li>将所有的特征看成一个一个的节点;</li>
	<li>遍历每个特征的每一种分割方式，找到最好的分割点;将数据划分为不同的子节点 $N_1, N_2,...,N_m$。计算划分之后所有子节点的<strong>纯度</strong>信息。</li>
	<li>对第二步产生的分割，选择出<strong>最优的特征以及最优的划分方式</strong>。得出最终的子节点: $N_1,N_2,...,N_m $ </li>
	<li>对子节点分别继续执行2-3步，直到每个最终的子节点都足够<strong>纯</strong>。</li>
</ol>

<p><em>注：这里的纯度指的是每个字节点的类别尽量相同。注意构建过程中的选择最优特征的步骤决定了后期算法的不同</em></p>

<h3>3.2、划分方式的选择</h3>

<p>在上述 3.1 中提到了两个关键性的事情，叫做划分方式和选择最优特征，这里我们就要讨论的是划分方式。众所周知，数据分为离散的和连续的，显然我们对他们的处理有些不同。</p>

<p>如果属性是离散值，且要求是二叉树，我们就可以按照一般的逻辑方式，按照<code>属于此子集</code>和<code>不属于此子集</code>分成两个分支。如果没有要求是二叉树则一个属性就是一个分支。</p>

<p>如果是属性为连续值，则可以确定一个值作为分裂点<code>split_point</code>，按照<code>&gt;split_point</code>和<code>&lt;=split_point</code>生成两个分支。</p>

<h3>3.3、 决策树分割属性选择</h3>

<p>之前我们说了划分方式，那么我们如何选择最优的特征呢。答案是比较<strong>纯度</strong>。首先决策树算法是一种“贪心”算法策略，<strong>只考虑在当前数据特征情况下</strong>的最好分割方式，且不能进行回溯操作。而对于整体的数据集而言，通过查看每个特征属性划分后，<strong>纯度的变化进行比较</strong>，选择能让数据集变得更纯的特征属性进行分割。之后重复上述步骤直到满足条件。</p>

<p><em>注：题外话，虽然我们很少需要自己造轮子，但是还是需要知道树的结构是符合递归规律的，一般而言树的构建都可以使用递归算法</em></p>

<p>那么纯度是什么，纯度其实就是一种判断决策树是否向着正确方向前进的判断。粗鄙的类比，就是母猪配种，选择最合适的方式将不同种的猪分开，尽量保证每波猪都是纯种的。纯度的判断标准不同也决定了之后的算法的名称不同，其标准如下三种：</p>

<ul>
	<li>基尼系数：$Gini = 1 - \sum_{i=1}^nP(i)^2$

		<ul>
			<li>信息熵：$-\sum_{i=1}^n P(i)log_2(P(i))$</li>
			<li>误差率：$Error = 1 - max_{i=1}^n \{P(i)\}$</li>
		</ul></li>
</ul>

<p><em>注 1：这里我说的是算法名称不同，其实更多的想指代他们是一个系列的算法，和LR与Lasso和Ridge的关系一样</em></p>

<p><em>注 2：如上公式都体现了纯度值越小信息量越大这个理念</em></p>

<p>上面是基础的一些系数，在剪枝和判断的时候需要一个种体现变化的系数，于是出现了如下公式来作为C4.5和ID3的判别标准。简单的理解为，这个公式表达了以A属性划分后，信息量增加的量，或者说指代的是纯度变纯了多少。</p>

$$
Gain = \Delta = H(D)-H(D|A)
$$
<h3>3.4、 停止条件</h3>

<p>决策树构建的过程是不断递归的过程，就像 (是) <code>while</code> 循环一样，必须有跳出循环的条件。以下是两个停止条件。</p>

<ol>
	<li>每个字节点只有一种类型时停止条件。（容易过拟合）</li>
	<li>节点中记录数小于某个阀值的时候，或者迭代次数达到给定值的时候停止构建。</li>
</ol>

<h3>3.3、 决策树的评估</h3>

$$
loss = \sum_{t=1}^{leaf} \frac{|D_t|}{D}H(t)
$$
<p>其中$H(t)$前的参数$\frac{D_t}{D}$主要的目的其实是给信息熵加权值，代表着节点中的样本点越多它越重要。</p>

<h2>4、 算法对比</h2>

<p>其实到这里决策树的核心就介绍完了，现在来看实际中的算法应用，也是面试种可能（不太可能）问到的算法，即ID3，C4.5，CART。其中CART最重要，会在之后的GBDT算法中，代表着那个DecisionTree。</p>

<h3>4.1 、ID3算法</h3>

<p>ID3算法是决策树的一个经典的构造算法，每次迭代选择分割属性的方式为，使用<strong>信息增益</strong>作为评判标准。</p>

<p><strong>优点为：</strong>决策树构建速度快，实现简单。</p>

<p><strong>缺点为：</strong>首先，计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优。其次，ID3算法不是递增算法，是单变量决策树，对于特征属性之间的关系不会考虑，<strong>抗噪性差</strong>。最后由于它的数据要扔到内存中，所以<strong>只适合小规模数据集</strong>。</p>

<h3>4.2、C4.5算法</h3>

<p>在ID3算法的基础上，进行算法优化提出的一种算法 ( C4.5 ) 。其使用<strong>信息增益率</strong>来取代ID3算法中的信息增益，同时在树的构造过程中<strong>会进行剪枝操作进行优化</strong>。能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。信息增益率公式如下：</p>

$$
Gain\_ratio(A) = \frac{Gain(A)}{H(A)}
$$
<p><strong>优点：</strong>产生的规则易于理解，同时准确率较高且实现简单</p>

<p><strong>缺点：</strong>由于采用了剪枝优化，所以对数据集<strong>需要进行多次顺序扫描和排序</strong>，所以效率较低 ，和ID3一样同样需要将数据放在内存，故只适合小规模数据集。</p>

<p><em>注：这里的能够处理连续值和剪枝优化都算是后期加的，只是Sklearn中没有给ID3赋予这个功能。他们的核心差别就在是增益率还是增益。剪枝优化会在决策树优化中讲</em></p>

<h3>CART</h3>

<p>好了，轮到了这个决策树中最重要的算法了，它使用基尼系数作为数据纯度的量化指标来构建的决策树。CART拆开是，Classification And Regression Tree，中文是分类回归树。这么叫的原因是它可以用来做分类和回归两类问题。<strong>值得注意的是：CART构建是二叉树</strong></p>

<p>其GINI增益公式如下：</p>

$$
Gain = \Delta = Gini(D) - Gini(D|A)
$$
<p><em>注：GINI系数的计算不牵扯信息熵运算中的对数运算，故速度比较快</em></p>

<h2>总结</h2>

<ol>
	<li>ID3和C4.5基本上是一回事，所以他们都是单变量决策树，都只使用在小规模数据集</li>
	<li>C4.5算是ID3的优化，所以当属性取值较多的时候，可以考虑C4.5而不是ID3</li>
	<li>决策树的树是存在内存中的，所以一般只适用于小数据量。

		<p><em>注：一般你要是看到了类似B+树的结构，一般不是完全在内存中</em></p></li>
	<li>CART是最常用的，尤其是后期集成算法GBDT中用的就是CART。CART是二叉树，ID3和C4.5不一定是。</li>
</ol>

<h2>最后</h2>

<p>这掌的重点在于知道决策树的生成规律，而三种算法的区别仅仅只是对于树形成节点的规则不同而已，ID3使用信息增益、 C4.5使用信息增益率、CART使用基尼系数。这一章依旧是一个轻松愉悦的章节，内容非常easy。之后我们会开始进入基础算法之后的升级版就困难不少。</p>

<p>BTW，没想到有这么多人关注专栏，再次感谢大家。</p>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>

