---
layout: post
title: "微软新工具 NNI 使用指南之 Tuner 篇"
date: 2018-12-16
categories: NNI
tags: ["NNI", "AutoML", "工具"]
---
<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>微软新工具 NNI 使用指南之 Tuner 篇</title>
	</head>
<body>
<figure><img src="https://upload-images.jianshu.io/upload_images/1083955-7c9ad7544cf87c62.PNG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240"/></figure>
<h2>什么是 Tuner</h2>
<p>在开始之间我们首先需要了解什么是 Tuner。正如之前的博文在 NNI 使用体验中提到的，通俗的来讲，Tuner的作用为让机器自动决定下一次测试的超参设置或下一次尝试的模型结构。 而这篇文章根据学术届的分类将其分为超参调优 (Hyperparameter Optimization)和网络结构搜索 (Neural Architecture Search) 两个部分来介绍。并在每部分结尾处简单介绍一些 NNI 尚未实现但出现在最新顶会中有趣的算法。</p>
<p><strong>注：本文中出现的所有引用均可以在该<a href="https://github.com/VDeamoV/autoML_papers">仓库</a> 内找到</strong></p>

<h2>Hyperparameter Optimization</h2>

<p>HO(Hyperparameter Optimization) 为超参调优。简单的来说，该算法仅仅是是使用一系列操作针对超参集中选择最优的超参，但未对原有模型结构进行调优。准确的定义如下：</p>

<blockquote>
<p>In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.</p>

<p>From Wikipedia</p>
</blockquote>

<h3>Anneal</h3>

<p>Anneal Tuner 来源于<a href="https://www.mathworks.com/help/gads/what-is-simulated-annealing.html">模拟退火算法</a> SA(Simulated Annealing)，该算法是一种通用的概率演算法，常用来在一定时间内寻找在一个很大的搜索空间中的近似最优解。该算法类似于贪心算法和遗传算法的结合，其先对上一步中尝试的超参组合进行随机扰动产生新解，之后若该新解有更好的结果则接受新解，若结果变差则按 Metropolis 准则以一定概率接受新解。</p>

<p>根据 NNI 的 <a href="https://github.com/Microsoft/nni/blob/master/zh_CN/docs/Builtin_Tuner.md#Anneal">Anneal 说明文档</a>，建议在每个 Trial 的时间不长，并且有足够的计算资源，或搜索空间的变量能从一些先验分布中采样的情况下使用，但是结果和 Random Search 相当。</p>

<h3>Batch Tuner（手动批处理模式）</h3>

<p>Batch Tuner 的使用场景为，用户只想让 NNI 作为一个批处理工具的场景。该 Tuner 让用</p>

<p>户自行写入要尝试的超参组合，之后 NNI 会按照设置好的超参组合进行尝试。</p>

<blockquote>
<p>优点：节省手工启动测试程序的步骤</p>

<p>缺点：NNI 仅作为一个批处理工具</p>

<p><em>注：该方法存在的意义在于，可以节省两次调参调整中间的间隔时间</em></p>
</blockquote>

<h3>Grid Search</h3>

<p>网格搜索是一个非常符合人类直观思路的方法，即穷居法。它穷尽了所有种可能的超参排列方式，再依次进行尝试后找到最优方案。过程和我们在中学时学的排列组合一样，三个超参分别有三种取值 3、4、5，那么三种超参就有 3*4*5 = 60 种取值方式。显然这种搜索方式是不科学的，非常容易<strong>组合爆炸</strong>，是一种非常不高效的调参方式。</p>

<blockquote>
<p>优点：该算法考虑到了搜索空间内所有的参数组合</p>

<p>缺点：存在组合爆炸的问额</p>

<p><strong>注：强烈不推荐使用</strong></p>
</blockquote>

<h3>Random Search</h3>

<p>该思想来源于神经网络三巨头 Bengio 在 JMLR 2012的工作 [1]，这篇论文的核心贡献是从经验和理论上证明了，随机搜索超参的方式相比传统的网格搜索，能在更短的时间内找到一样或更好的模型参数。该算法也从此成为各个优化算法的对比标准。</p>

<blockquote>
<p>优点：容易理解</p>

<p>缺点：高维搜索空间表现一般</p>
</blockquote>

<h3>贝叶斯优化系列</h3>

<p>实际上 Grid Search 和 Random Search 都是非常普通的方法，同时暴力搜索和随机搜索当然也很难体现算法设计者的智慧。而接下来要讲的贝叶斯优化系列则“很可能”存在与人工经验调惨相媲美的实力。</p>

<ol>
	<li>Bayesian Optimization

		<p>贝叶斯优化 (Bayesian Optimization)，这个工作最初是由 J Snoek et.[2] 在 NIPS 2012 中提出的，并随后多次进行改进 [3, 4]。它要求已经存在几个样本点，并且通过高斯过程回归（<strong>假设超参数间符合联合高斯分布</strong>）计算前面 n 个点的后验概率分布，得到每一个超参数在每一个取值点的期望均值和方差，其中均值代表这个点最终的期望效果，均值越大表示模型最终指标越大，方差表示这个点的效果不确定性，方差越大表示这个点不确定是否可能取得最大值非常值得去探索，具体的细节分析可以参见这篇<a href="https://zhuanlan.zhihu.com/p/29779000">博客</a></p>

		<p>但是这个算法仅仅在低纬空间表现优于 Random Search，而在高维空间和 Random Search 表现相当。</p>

		<blockquote>
		<p>优点：在低维空间显著优于 Random Search</p>

		<p>缺点：在高维空间仅和 Random Search 相当</p>

		<p><em>注：这个算法的另一个名称为 Spearmint[2]</em></p>
		</blockquote></li>
	<li>TPE

		<p>TPE 算法来源于 Y Bengio 在顶会 NIPS2011 的工作 [7]。 TPE 依旧属于贝叶斯优化，其和 SMAC 也有着很深的渊源。其为一种基于树状结构 Parzen 密度估计的非标准贝叶斯优化算法。相较于其他贝叶斯优化算法，TPE 在高维空间表现最佳。</p>

		<blockquote>
		<p>优点 1：相比其他贝叶斯优化算法，高维空间的情况下效果更好</p>

		<p>优点 2：相比 BO 速度有显著提升</p>

		<p>缺点：在高维空间的情况下，与 Random Search 效果相当</p>

		<p><strong>注：所有贝叶斯优化算法在高维空间下表现均和 Random Search 相当 [6]</strong></p>
		</blockquote></li>
	<li>SMAC

		<p>SMAC 算法出自于期刊 LION 2011[5]，其论文中表明，由于先前的 SMBO 算法，不支持离散型变量。SMAC 提出使用 Random Forest 将条件概率 p(y|λ) 建模为高斯分布，其中 λ 为超参的选择。这使得它能够很好的支持离散型变量，并在离散型变量和连续型变量的混合的时候有着不错的表现 [6]。</p>

		<blockquote>
		<p>优点 1：能很好的支持离散型变量，并针对高维空间有一定改善</p>

		<p>优点 2：相比 BO 速度有显著提升</p>

		<p>缺点：效果不稳定，高维空间表现和 Random Search 相当</p>
		</blockquote></li>
</ol>

<h3>HyperBand</h3>

<p>HyperBand 来源于非常新的来自 JMLR 2018 的工作 [6]，实质为 <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">Multi-Armed Bandit</a> 问题。</p>

<p>其解决的问题为如何平衡“探索”(exploration) 和“利用”(exploitation)。该算法相比之前的算法，最突出的特点为，其限定了资源的总量，优化算法的问题转化为如何在给定资源的情况下，更好的利用资源找出最优解的问题。这些可以体现在超参的设计当中。</p>

<blockquote>
<p>优点 1：考虑到了资源有限的情况</p>

<p>优点 2：可以和其他的算法，如 TPE 等进行融合（当前 NNI 不支持）[6]</p>

<p>缺点：算法筛选结果的方式为每多步之后，保留 TopK 的结果，其将导致一些开始收敛较慢的参数配置被淘汰。</p>
</blockquote>

<h3>Metis</h3>

<p>Metis 为微软提供的自动调参服务，在论文 [8]
中指出，之前的贝叶斯优化算法存在两个较大的问题，一方面贝叶斯优化算法，存在<strong>过度采样</strong>的问题，这在资源密集型的算法，如深度学习的情况下是一笔很重的开销。另一方面贝叶斯优化和高斯处理算法均假设，问题是<strong>理想的无噪声</strong>或<strong>仅受高斯分布的噪声影响</strong>，但实际情况比这个假设要复杂。而 Metis 在一定程度上解决了这个问题。Metis 的本质是随机搜索，为了最小化调整配置时造成的系统性能损失，该算法当且仅当预测的配置可以带来高于一定程度的优化时才会切换配置。</p>

<blockquote>
<p>优点 1：在系统配置选择等存在非高斯噪声的情况下，Metis 显著优于 TPE 算法</p>

<p>优点 2：能为接下来的尝试提供候选</p>

<p>缺点：训练时间基本由样本点的数量决定，且呈立方级增长（复杂度为 $O(N^{3}+N^{2}D)$ 其中 N 为样本点数量，D 为样本点的维数）。配置选择时间同样基本由样本点的数目决定，其复杂度为 $O(N^{2}+ND)$ $\Rightarrow$ 和样本点的数据量高度相关且复杂度高</p>

<p><strong>注：NNI 中 Metis 只支持 <code>choice</code>, <code>quniform</code>, <code>uniform</code> 和 <code>randint</code> 类型。</strong></p>
</blockquote>

<h2>Neural Architecture Search</h2>

<h3>什么是 NAS</h3>

<p>和上一节中的 HO 不同的是，在超参调整中，NAS 调整的超参会影响到模型的结构。意在通过大量尝试探索一种更为合理的网络结构。而这些超参的调整已经超出了之前介绍的贝叶斯优化系列的能力范畴。在 NAS 中遗传算法系列和强化学习系列有着不错的表现。</p>

<h3>Naive Evolution</h3>

<p>Naive Evolution 出自 ICML 2017 的工作 [9]，其将遗传算法引入模型结构的搜索。根据论文中的描述，该方法为一种典型的<a href="https://zh.wikipedia.org/zh-hans/%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95)">遗传算法</a>，其通过设置一定量的初始种群，经过“突变”（更改超参）后根据“自然筛选”（筛选出表现优秀的模型）保留优异的个体。论文中是针对图片分类的模型探索，支持多种“突变”，细节可参见原论文 [9]。除了使用遗传算法这个特点外，其还有一个特点是支持<strong>权重迁移</strong>的模型，这个特点会使得每个突变个体只需要少量的 epoch 来训练。而这个优点也在 Morphism 算法中得以体现。但是该算法也存在陷入局部最优解的问题，论文中描述该问题主要可以通过增大以下两个参数解决。</p>

<ul>
	<li>population size

		<p>这个参数的增大会让初始种群个体数量更多，这样子可以通过增加会避免模型陷入局部最优解。</p></li>
	<li>the number of training steps per individual

		<p>这个参数的增大会让每一个个体的表现得到最大的发挥，从而不会错淘汰掉潜力优秀的个体</p></li>
</ul>

<p><strong>注：遗憾的是 NNI 中还未能让用户自行调整这两个参数</strong></p>

<blockquote>
<p>优点 1: 使用遗传算法进行模型结构的搜索</p>

<p>优点 2: 对支持权重迁移的模型，运算的速度会有显著提升</p>

<p>缺点：同所有的遗传算法一样，优于需要尝试大量的突变个体，需要大量的计算资源</p>
</blockquote>

<h3>Network Morphism</h3>

<p>Network Morphism 来自另一个开源自动调参工具 <a href="https://autokeras.com">Auto-keras</a> 的其中一篇 arXiv 上的文章 [10]。该算法的设计的初衷是减少调参的计算损耗。为了提高算法的效率，其引入了一个新的神经网络核 (Neural Network Kernel) 和 一个树形采样函数，来用贝叶斯优化的方式探索搜索空间。</p>

<p>除此之外，另一点提高效率的方式为引入了 Morphism Operation。该操作是在已经训练好的模型上进行调整，这样新生成的网络结构就只需要少量的训练就能达到好的效果。这个方式在 Naive Evolution 也有使用。就论文中在 MNIST，FASHION，CIFAR-10 三个数据集上的实验结果看，其显著好于 Random Search，SPMT[11]，SMAC，SEA[12]，NASBOT[13]。</p>

<blockquote>
<p>优点 1: 在探索网络结构的时候引入了贝叶斯优化，提高了搜索效率</p>

<p>优点 2: Morphism 操作，保留了之前的训练的结果，让新的变种网络训练的更快</p>

<p>缺点：当前版本不支持 RNN 的网络模型探索</p>
</blockquote>

<h3>ENAS</h3>

<p>ENAS 源自 CVPR 2018 的一个工作 [14]，其使用 RNN 作为控制器然后根据收敛精度调整 RNN，</p>

<p>论文中在 Cifar10 上训练 RNN，之后再应用到 ImageNet 上，效果很惊人（应用到 Fast-RCNN 上之后居然有 4% 的准确率提升）。但是该算法<strong>需要很多预先的人为的前提设定</strong>，同时<strong>速度还是很慢</strong>, 而 ENAS 的主要工作为在其工作上，增加参数共享的方式，避免新产生的模型重训练，加快了训练速度。</p>

<blockquote>
<p>优点：相较于其他的 NAS 算法，该算法速度非常快</p>

<p>缺点：同它的改进的模型一样，其需要设置每个 Cell 的 Block 等多个前提设定的参数</p>

<p><strong>注：相较于其他算法，这个算法更有趣，论文中的效果现实也是最有意思最好的</strong></p>
</blockquote>

<h2>NNI 未实现但很有趣的算法</h2>

<h3>NASBOT</h3>

<p>NASBOT 源自 NIPS2018 的一个工作 [13]， 其核心就在于计算 OTMANN distance。该方法使用了 layer masses 和 path length 来定义 OTMANN distance。</p>

<p>三者的定义如下：</p>

<ul>
	<li>layer masses：在比较两个神经网络时匹配的层的个数</li>
	<li>path length：两个层之前的距离，如 (2, 5, 8, 13) 是一条从 layer2 到 layer13 的一条长度为 3 的路径</li>
	<li>OTMANN distance：通过最优化算法得出的神经网络之间的距离</li>
</ul>

<p>就结果而言论文中表明 NASBOT 在 Cifar10 数据集上，在<strong>计算量</strong>和<strong>运行时间</strong>方面显著优于其他 tuner 算法，如随机搜索，进化算法等。</p>

<blockquote>
<p>优点 1：对 MLPs 和 CNN 的支持效果较好</p>

<p>优点 2：训练需要的总计算量较小</p>

<p>缺点：在寻找下一个网络架构的用时较长</p>
</blockquote>

<blockquote>
  <p><strong>注：该算法的 python 实现：<a href="https://github.com/kirthevasank/nasbot">源码链接</a></strong></p>
</blockquote>

<h3>DARTS</h3>

<p>DARTS 为在 arXiv 上的一篇很有意思的工作 [15]，其正在等待 ICLR 2019 的审核，详情可以参见 OpenReview 该<a href="https://openreview.net/forum?id=S1eYHoC5FX">论文的链接</a>。该工作的中心思想为<strong>科学选择神经网络结构，将神经网络结构作为参数进行优化</strong>。该论文在 Cifar10，ImageNet 等数据集上进行了大量的实验，发现此算法适用于图像分类的高性能卷积结构和语言建模的循环神经网络结构，并提出此方法的<strong>效率高于 ENAS</strong>。</p>

<blockquote>
<p>优点 1：将模型结构视为参数，扩大搜索空间</p>

<p>优点 2：较高的可迁移性</p>

<p>缺点：参数过多，对算力和数据量的要求较高 </p>

<p>注：该论文的复现可以参考作者的源码：<a href="https://github.com/quark0/darts">源码链接</a></p>
</blockquote>

<h2>参考论文</h2>

<ul>
	<li>[1] Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.</li>
	<li>[2] Snoek J, Larochelle H, Adams R P. Practical bayesian optimization of machine learning algorithms[C]//Advances in neural information processing systems. 2012: 2951-2959.</li>
	<li>[3] Swersky K, Snoek J, Adams R P. Multi-task bayesian optimization[C]//Advances in neural information processing systems. 2013: 2004-2012.</li>
	<li>[4] Snoek J, Rippel O, Swersky K, et al. Scalable bayesian optimization using deep neural networks[C]//International Conference on Machine Learning. 2015: 2171-2180.</li>
	<li>[5] Hutter F, Hoos H H, Leyton-Brown K. Sequential model-based optimization for general algorithm configuration[C]//International Conference on Learning and Intelligent Optimization. Springer, Berlin, Heidelberg, 2011: 507-523.</li>
	<li>[6] Li L, Jamieson K, DeSalvo G, et al. Hyperband: A novel bandit-based approach to hyperparameter optimization[J]. arXiv preprint arXiv:1603.06560, 2018: 1-48.</li>
	<li>[7] Bergstra J S, Bardenet R, Bengio Y, et al. Algorithms for hyper-parameter optimization[C]//Advances in neural information processing systems. 2011: 2546-2554.</li>
	<li>[8] Li Z L, Liang C J M, He W, et al. Metis: robustly optimizing tail latencies of cloud systems[C]//Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference. USENIX Association, 2018: 981-992.</li>
	<li>[9] Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y. L., Tan, J., ... &amp; Kurakin, A. (2017, July). Large-Scale Evolution of Image Classifiers. In International Conference on Machine Learning (pp. 2902-2911).</li>
	<li>[10] Jin H, Song Q, Hu X. Efficient neural architecture search with network morphism[J]. arXiv preprint arXiv:1806.10282, 2018.</li>
	<li>[11] Snoek, J., Larochelle, H., and Adams, R. P.Practical bayesian optimization of machine learning algorithms.In Advances in Neural Information Processing Systems, pp. 2951–2959, 2012.</li>
	<li>[12] Elsken, T., Metzen, J. H., and Hutter, F. Neural architec- ture search: A survey. arXiv preprint arXiv:1808.05377, 2018.</li>
	<li>[13] Kandasamy, K., Neiswanger, W., Schneider, J., Poczos, B., and Xing, E. Neural architecture search with bayesian optimisation and optimal transport. NIPS, 2018.</li>
	<li>[14] Zoph B, Vasudevan V, Shlens J, et al. Learning transferable architectures for scalable image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 8697-8710.</li>
	<li>[15] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018).</li>
</ul>

</body>
</html>

