---
layout: post
title: "决策树"
date: 2018-08-09
categories: 机器学习
tags: [MachineLearning]
---
## 1、 前言
之前我们已经说了，机器学习的从线性回归，概率这个出发点发展的算法。这次我们讲从第三个出发点，使用信息熵的算法，决策树。

## 2、 信息熵
首先我们来介绍什么是信息熵，信息熵是1948年，香农引入信息熵。一个系统**越是有序， 信息熵就越低**，一个系统越是混乱，信息熵就越高，所以信息熵被认为是一个系统有序程度的度量。举个例子，太阳从东边升起这个规律人人都知道，所以信息熵低，而GAN算法是怎么推导的知道的人就不多，所以它的信息熵高。
*注：一个事件发生的概率大，那么它含有的信息少。*

一言已概之就是：**信息熵就是用来描述系统信息量的不确定度。**其公式如下：

$$
H(x) = - \sum_{i=1}^m p_i log_2(p_i)
$$

**条件熵的定义为：** 给定条件X的情况下，所有不同x值情况下Y的信息熵的平均值叫做条件熵。

$$
H(Y|X) = \sum_{j=1}P(X = v_j)H(Y|X = v_j)
$$

$$
H(Y|X) = H(X,Y)-H(X)
$$

## 3、 纵览决策树算法
在了解了这个算法的关键评判标准后，我们来看下决策树是什么，决策树 ( Decision Tree ) 是在已知各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的一种**图解法**。决策树是一种树形结构， 其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节点代表一种类别。
换句话说，决策树的使用是一种模拟了简单的人类思考过程的思路。通过许多`if...else...`进行判断最后得到一个结论。

### 3.1、 决策树构建过程
构建步骤如下:
1. 将所有的特征看成一个一个的节点;
2. 遍历每个特征的每一种分割方式，找到最好的分割点;将数据划分为不同的子节点 $N_1, N_2,...,N_m$。计算划分之后所有子节点的**纯度**信息。
3. 对第二步产生的分割，选择出**最优的特征以及最优的划分方式**。得出最终的子节点: $N_1,N_2,...,N_m $ 
4. 对子节点分别继续执行2-3步，直到每个最终的子节点都足够**纯**。
*注：这里的纯度指的是每个字节点的类别尽量相同。注意构建过程中的选择最优特征的步骤决定了后期算法的不同*

### 3.2、划分方式的选择
在上述 3.1 中提到了两个关键性的事情，叫做划分方式和选择最优特征，这里我们就要讨论的是划分方式。众所周知，数据分为离散的和连续的，显然我们对他们的处理有些不同。

如果属性是离散值，且要求是二叉树，我们就可以按照一般的逻辑方式，按照`属于此子集`和`不属于此子集`分成两个分支。如果没有要求是二叉树则一个属性就是一个分支。

如果是属性为连续值，则可以确定一个值作为分裂点`split_point`，按照`>split_point`和`<=split_point`生成两个分支。

### 3.3、 决策树分割属性选择
之前我们说了划分方式，那么我们如何选择最优的特征呢。答案是比较**纯度**。首先决策树算法是一种“贪心”算法策略，**只考虑在当前数据特征情况下**的最好分割方式，且不能进行回溯操作。而对于整体的数据集而言，通过查看每个特征属性划分后，**纯度的变化进行比较**，选择能让数据集变得更纯的特征属性进行分割。之后重复上述步骤直到满足条件。
*注：题外话，虽然我们很少需要自己造轮子，但是还是需要知道树的结构是符合递归规律的，一般而言树的构建都可以使用递归算法*

那么纯度是什么，纯度其实就是一种判断决策树是否向着正确方向前进的判断。粗鄙的类比，就是母猪配种，选择最合适的方式将不同种的猪分开，尽量保证每波猪都是纯种的。纯度的判断标准不同也决定了之后的算法的名称不同，其标准如下三种：
- 基尼系数：$Gini = 1 - \sum_{i=1}^nP(i)^2$
- 信息熵：$-\sum_{i=1}^n P(i)log_2(P(i))$
- 误差率：$Error = 1 - max_{i=1}^n \{P(i)\}$
*注 1：这里我说的是算法名称不同，其实更多的想指代他们是一个系列的算法，和LR与Lasso和Ridge的关系一样*
*注 2：如上公式都体现了纯度值越小信息量越大这个理念*
上面是基础的一些系数，在剪枝和判断的时候需要一个种体现变化的系数，于是出现了如下公式来作为C4.5和ID3的判别标准。简单的理解为，这个公式表达了以A属性划分后，信息量增加的量，或者说指代的是纯度变纯了多少。

$$
Gain = \Delta = H(D)-H(D|A)
$$

### 3.4、 停止条件
决策树构建的过程是不断递归的过程，就像 (是) `while` 循环一样，必须有跳出循环的条件。以下是两个停止条件。
1. 每个字节点只有一种类型时停止条件。（容易过拟合）
2. 节点中记录数小于某个阀值的时候，或者迭代次数达到给定值的时候停止构建。

### 3.5、 决策树的评估

$$
loss = \sum_{t=1}^{leaf} \frac{|D_t|}{D}H(t)
$$

其中$H(t)$前的参数$\frac{D_t}{D}$主要的目的其实是给信息熵加权值，代表着节点中的样本点越多它越重要。

## 4、 算法对比
其实到这里决策树的核心就介绍完了，现在来看实际中的算法应用，也是面试种可能（不太可能）问到的算法，即ID3，C4.5，CART。其中CART最重要，会在之后的GBDT算法中，代表着那个DecisionTree。

### 4.1 、ID3算法
ID3算法是决策树的一个经典的构造算法，每次迭代选择分割属性的方式为，使用**信息增益**作为评判标准。
**优点为：**决策树构建速度快，实现简单。
**缺点为：**首先，计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优。其次，ID3算法不是递增算法，是单变量决策树，对于特征属性之间的关系不会考虑，**抗噪性差**。最后由于它的数据要扔到内存中，所以**只适合小规模数据集**。

### 4.2、C4.5算法
在ID3算法的基础上，进行算法优化提出的一种算法 ( C4.5 ) 。其使用**信息增益率**来取代ID3算法中的信息增益，同时在树的构造过程中**会进行剪枝操作进行优化**。能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。信息增益率公式如下：

$$
Gain\_ratio(A) = \frac{Gain(A)}{H(A)}
$$

**优点：**产生的规则易于理解，同时准确率较高且实现简单
**缺点：**由于采用了剪枝优化，所以对数据集**需要进行多次顺序扫描和排序**，所以效率较低 ，和ID3一样同样需要将数据放在内存，故只适合小规模数据集。

*注：这里的能够处理连续值和剪枝优化都算是后期加的，只是Sklearn中没有给ID3赋予这个功能。他们的核心差别就在是增益率还是增益。剪枝优化会在决策树优化中讲*

### CART
好了，轮到了这个决策树中最重要的算法了，它使用基尼系数作为数据纯度的量化指标来构建的决策树。CART拆开是，Classification And Regression Tree，中文是分类回归树。这么叫的原因是它可以用来做分类和回归两类问题。**值得注意的是：CART构建是二叉树**

其GINI增益公式如下：

$$
Gain = \Delta = Gini(D) - Gini(D|A)
$$

*注：GINI系数的计算不牵扯信息熵运算中的对数运算，故速度比较快*

## 总结
1. ID3和C4.5基本上是一回事，所以他们都是单变量决策树，都只使用在小规模数据集
2. C4.5算是ID3的优化，所以当属性取值较多的时候，可以考虑C4.5而不是ID3
3. 决策树的树是存在内存中的，所以一般只适用于小数据量。
	*注：一般你要是看到了类似B+树的结构，一般不是完全在内存中*
4. CART是最常用的，尤其是后期集成算法GBDT中用的就是CART。CART是二叉树，ID3和C4.5不一定是。

## 最后
这掌的重点在于知道决策树的生成规律，而三种算法的区别仅仅只是对于树形成节点的规则不同而已，ID3使用信息增益、 C4.5使用信息增益率、CART使用基尼系数。这一章依旧是一个轻松愉悦的章节，内容非常easy。之后我们会开始进入基础算法之后的升级版就困难不少。

BTW，没想到有这么多人关注专栏，再次感谢大家。
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
