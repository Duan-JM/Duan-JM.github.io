---
layout: post
title: "第四课 无模型的预测"
date: 2019-03-11
categories: ReinforceLearning
tags: ["ReinforceLearning", "强化学习"]
---
<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>第四课 无模型的预测</title>
	</head>
<body>
<p>这一课帅小哥主要讲的内容是预测的部分，在第五课会加入控制的部分。其中预测的部分主要是两个相似的算法，一个为 Monte-Carlo（MC），另一个为 Temporal-Difference（TD）。两者的区别主要在于，<strong>MC 为需要在出现终止状态后，才能得到 Reward，而 TD 则是实时的</strong>。</p>

<h2>Monte-Carlo</h2>

<p>Monte-Carlo强化学习指的是，在不清楚 MDP 状态转移和奖励方案的情况下，直接通过经历完整的 episode 来学习状态的 value。一般而言，一个状态的 value 为，其在多个 episode 下的 value 的平均值。</p>

<p>注：episode 指的是<strong>不定的起始状态</strong>开始，直到<strong>某一特定的终止状态</strong>结束。</p>

<p>其评价每个状态的 value 的主要算法如下两种：</p>

<ul>
	<li>首次访问 Monte-Carlo 策略评估

		<p>首先，固定一个策略 $\pi$ ，之后使用这个策略进行多个完整的 episode。对于每个 episode，当且仅当状态第一次出现时才列入计算：</p>

<pre><code class="code-highlighted code-python">N(s) <span class="syntax-all syntax-keyword">=</span> N(s) <span class="syntax-all syntax-keyword">+</span> <span class="syntax-all syntax-constant">1</span> <span class="syntax-all syntax-comment"># 状态计数 +1
</span>S(s) <span class="syntax-all syntax-keyword">=</span> S(s) <span class="syntax-all syntax-keyword">+</span> G_t <span class="syntax-all syntax-comment"># 总收获更新
</span>V(s) <span class="syntax-all syntax-keyword">=</span> V(s) <span class="syntax-all syntax-keyword">/</span> N(s) <span class="syntax-all syntax-comment"># 状态的 value 更新
</span></code></pre>

		<p><em>注：当 N 很大时， $V(s)$ 就是我们所求估计</em></p></li>
	<li>每次访问 Monte-Carlo 策略评估

		<p>同首次访问 MC 算法一致，但是这个算法中，<strong>每次出现在状态转移链中的状态</strong>都会更新。</p></li>
</ul>

<h2>Temporal-Difference</h2>

<p>首先，在开始 TD 算法前，小哥补充了一个 baby math，即一个求平均值的操作其实是可以写成一种迭代的样式如下：</p>

$$
\begin{split}
\mu_k &= \frac{1}{k} \sum_{j=1}^k x_j  \\
&= \mu_{k-1} + \frac{1}{k}(x_k-\mu_{k-1})
\end{split}
$$
<p>再抽象点就得到了以下公式：</p>

$$
\begin{split}
V(S_t) &= V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t)) \\
& = V(S_t) + \alpha(G_t - V(S_t))
\end{split}  
$$
<p>而 TD 算法就是从这里开始的。TD 算法和 MC 算法不同的地方在于，其不用完成整个 episode 才得到状态的 value，即它可以学到一个不完整的 episode，通过自身的引导（bootstrapping）来猜测结果。其公式如下：</p>

$$
V(S_T) = V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))
$$
<p><em>注：其中 $R_{t+1}$ 为离开该状态的时候的即刻奖励， $S_{t+1}$ 为下一状态的预估状态价值。</em></p>

<h2>TD 与 MC 的对比</h2>

<ol>
	<li>TD 可以在知道最终结果前就可以学习，MC 必须在 episode 结束后才知道。

		<p><em>注：小哥举了个例子，说你不能在被车撞死后再重来。</em></p></li>
	<li>MC 是基于某一个策略的无偏估计，而 TD 则是有偏估计。（毕竟 TD 瞎猜了）</li>
	<li>MC 没有 bias，但是有较高的变异性（Variance），对初值不敏感，而 TD 有 bias，低变异性，但效率高。</li>
</ol>

<p><em>注：MDP、TD 和 MC 都是计算状态 value 的方案</em></p>

<h2>参考资料</h2>

<ul>
	<li><a href="https://blog.csdn.net/dukuku5038/article/details/84557798">Joey 老师的教程 04</a> </li>
	<li><a href="https://space.bilibili.com/74997410/video">David Silver 的强化学习课系列</a></li>
</ul>

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>

