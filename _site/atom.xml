<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-08-24T21:40:40+08:00</updated><id>http://localhost:4000/</id><title type="html">Pre-Demo-Field</title><subtitle>Coding Life Coding Fun</subtitle><author><name>DeamoV</name></author><entry><title type="html">集成学习</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" rel="alternate" type="text/html" title="集成学习" /><published>2018-08-24T00:00:00+08:00</published><updated>2018-08-24T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;集成学习&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;集成学习一句话版本&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;集成学习的思想是将若干个学习器（分类器&amp;amp;回归器）组合之后产生新的学习器。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在学习这一章节中，老师提到了这个说法，我觉得非常言简意赅就直接引用了过来。集成学习算法的成功在于保证若分类器（错误率略小于0.5，即勉强比瞎猜好一点）的多样性，且集成不稳定的算法也能得到一种比较明显的提升。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注：深度学习其实也可以看作是一种集成学习&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;集成学习的作用&lt;/h2&gt;

&lt;p&gt;采用集成学习的原因有以下四点：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;分类器间存在一定的差异性，这会导致分类的边界不同，也就是说分类器是一个比较专精的专家，它有它自己一定的适用范围和特长。那么通过一定的策略将多个弱分类器合并后，就可以拓展模型的适用范围，减少整体的错误率，实现更好的效果。

		&lt;p&gt;&lt;em&gt;注：不严谨的类比的话，就像弹性网络模型就可以看作是由LASSO回归和Ridge回归组成的集成学习。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;对于数据集过大或者过小，过大会导致训练一个模型太慢，过小则会导致训练不充分，在这种情况下可以分别对数据集进行划分和有放回的操作产生不同的数据子集，然后使用数据子集训练不同的分类器，最终再将不同的分类器合并成为一个大的分类器。

		&lt;p&gt;&lt;em&gt;注：这种方案的优势就在于，提高了准确度和训练速度，使得之前很难利用的数据得到了充分的利用&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，然后再进行模型的融合。

		&lt;p&gt;&lt;em&gt;注：这种特性就好比当初素描老师教我们画圆一样，画一个正方形，再用一堆小直线一点一点切成圆形。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;对于多个异构的特征集的时候，很难进行融合，那么可以考虑每个数据集构建一个分类模型，然后将多个模型融合。

		&lt;p&gt;&lt;em&gt;注：简单的来说就是公司有两个人都很厉害，但是偏偏不凑巧两个人打架，就不能把他们放一个部门里，得放不同部门一样。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;集成学习的三种思想&lt;/h2&gt;

&lt;h3&gt;Bagging&lt;/h3&gt;

&lt;h4&gt;Bagging算法思想&lt;/h4&gt;

&lt;p&gt;Bagging，这个名字就是从袋子里取的意思，本身便很形象的说明了这个算法的核心思想，即在原始数据集上通过&lt;strong&gt;有放回的抽样&lt;/strong&gt;的方式，重新选择出S个新数据集来分别训练S个分类器，随后在预测的时候采用&lt;strong&gt;多数投票&lt;/strong&gt;或者&lt;strong&gt;求均值&lt;/strong&gt;的方式来判断预测结果。&lt;/p&gt;

&lt;h4&gt;Bagging适用弱学习器的范围&lt;/h4&gt;

&lt;p&gt;基本的弱学习器都能用，如Linear、Ridge、Lasso、 Logistic、Softmax、ID3、C4.5、CART、SVM、KNN。&lt;/p&gt;

&lt;h3&gt;Boosting&lt;/h3&gt;

&lt;h4&gt;Boosting算法思想&lt;/h4&gt;

&lt;p&gt;提升学习（Boosting），这个名字也很形象，在赛车游戏中氮气加速有时候界面就描述是boost，也就是越加越快，每次都比上一次更快，也就是说同Bagging是不一样的，Boosting是会根据其他的弱分类器的结果来&lt;strong&gt;更改数据集&lt;/strong&gt;再喂给下一个弱分类器。准确的描述为，Boosting算法每一步产生&lt;strong&gt;弱预测模型&lt;/strong&gt;(如决策树)，并&lt;strong&gt;加权累加&lt;/strong&gt;到总模型中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;它的意义在于如果一个问题存在弱预测模型，那么可以通过提升技术的办法得到一个强预测模型。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1: 如果每一步的弱预测模型的生成都是依据损失函数的梯度方式的，那么就称为梯度提升(Gradient boosting)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2：Boosting这个集成学习的思想就有点深度网络的意思了。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;Boosting适用范围&lt;/h4&gt;

&lt;p&gt;提升学习适用于&lt;strong&gt;回归&lt;/strong&gt;和&lt;strong&gt;分类&lt;/strong&gt;的问题。&lt;/p&gt;

&lt;h3&gt;Stacking&lt;/h3&gt;

&lt;p&gt;之前提到了Bagging是把训练集拆成不同的子集训练多个学习器投票，而Boosting是根据学习器学习的结果来改动数据集，经过多层改动后试图获得一个更好的预测效果。Bagging和Boosting这两个集成学习其实并没有通过训练结果来改变弱分类器的参数。相对比而言，Stacking就激进许多，当然也复杂和困难许多，它首先训练出多个不同的模型，然后再以之前&lt;strong&gt;训练的各个模型的输出作为输入来新训练一个新的模型&lt;/strong&gt;，换句话说，Stacking算法根据模型的输出是允许改其他分类器的参数甚至结构的，也正是因为这点&lt;code&gt;sklearn&lt;/code&gt;中很少有stacking的内置的算法。&lt;/p&gt;

&lt;h2&gt;1、Bagging算法&lt;/h2&gt;

&lt;h3&gt;随机森林(Random Forest)&lt;/h3&gt;

&lt;p&gt;随机森林的思路很简单如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;从样本集中用Bootstrap采样选出n个样本;&lt;/li&gt;
	&lt;li&gt;从所有属性中随机选择K个属性，选择出最佳分割属性作为节点创建决策树&lt;/li&gt;
	&lt;li&gt;重复以上两步m次，即建立m棵决策树&lt;/li&gt;
	&lt;li&gt;这m个决策树形成随机森林，通过投票表决结果决定数据属于那一类&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注：RF算法在实际应用中具有比较好的特性，应用也比较广泛，主要应用在&lt;strong&gt;分类、 回归、特征转换、异常点检测&lt;/strong&gt;等。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RF算法分析&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RF的主要优点：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;训练可以并行化，对于大规模样本的训练具有速度的优势。&lt;/li&gt;
	&lt;li&gt;由于进行随机选择决策树划分特征列表，这样在样本维度比较高的时候，仍然具有比较高的训练性能。&lt;/li&gt;
	&lt;li&gt;给以给出各个特征的重要性列表。&lt;/li&gt;
	&lt;li&gt;由于存在随机抽样，训练出来的模型方差小，泛化能力强;。&lt;/li&gt;
	&lt;li&gt;RF实现简单。&lt;/li&gt;
	&lt;li&gt;对于部分特征的缺失不敏感。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;RF的主要缺点：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;在某些噪音比较大的特征上，RF模型容易陷入过拟合。&lt;/li&gt;
	&lt;li&gt;取值比较多的划分特征对RF的决策会产生更大的影响，从而有可能影响模型的效果。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;RF的变种&lt;/h3&gt;

&lt;h4&gt;Extra Tree&lt;/h4&gt;

&lt;p&gt;Extra Tree是RF的一个相当激进的变种，原理基本和RF一样，区别如下:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;RF会随机采样来作为子决策树的训练集，而Extra Tree每个子决策树&lt;strong&gt;采用原始数据&lt;/strong&gt;集训练;&lt;/li&gt;
	&lt;li&gt;RF在选择划分特征点的时候会和传统决策树一样，会基于信息增益、信息增益率、 基尼系数、均方差等原则来选择最优特征值。而Extra Tree会&lt;strong&gt;随机的选择一个特征值&lt;/strong&gt;来划分决策树。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Extra Tree因为是随机选择特征值的划分点，这样会导致决策树的规模一般大于RF所生成的决策树。也就是说Extra Tree模型的方差相对于RF进一步减少。在某些情况下，Extra Tree的泛化能力比RF的强。&lt;/p&gt;

&lt;h4&gt;Totally Random Trees Embedding&lt;/h4&gt;

&lt;p&gt;TRTE算法主要进行了两部分操作，第一部分是对数据进行操作，第二部分是对生成的决策树的位置信息转换成向量信息以供之后构建特征编码使用。抛开数据集上的操作，TRTE算法对RF的变种在于如何参考最终生成的多个决策树来给出预测结果。&lt;/p&gt;

&lt;p&gt;RF是采用投票的方式，而TRTE算法中，每个决策树会&lt;strong&gt;生成一个编码来对应叶子结点的位置信息&lt;/strong&gt;，那么把所有的决策树对应&lt;strong&gt;相同的分类的编码合并&lt;/strong&gt;起来，就可以用这一合并后的编码来代表它的特征了，预测时待预测样本经过这些决策树的预测也会得到这样一个合并后的编码，通过同训练好的类别的编码之间的差距的大小来预测这个样本应该属于哪一个类别。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;详细的说明说下：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;TRTE是一种非监督的数据转化方式。将&lt;strong&gt;低维的数据集映射到高维&lt;/strong&gt;，从而让映射 到高维的数据更好的应用于分类回归模型。&lt;/li&gt;
	&lt;li&gt;TRTE算法的转换过程类似RF算法的方法，建立T个决策树来拟合数据。当决策树构建完成后，数据集里的每个数据在T个决策树中叶子节点的位置就定下来了， 将位置信息转换为向量就完成了特征转换操作，这个转换过程有点像霍夫曼编码的过程。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;Isolation Forest&lt;/h4&gt;

&lt;p&gt;这个算法是用来异常点检测的，正如isolation这个名字，是找出非正常的点，而这些非正常的点显然是特征比较明确的，故不需要太多的数据，也不需要太大规模的决策树。&lt;/p&gt;

&lt;p&gt;它和RF算法有以下几个差别：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;在随机采样的过程中，一般只需要少量数据即可。&lt;/li&gt;
	&lt;li&gt;在进行决策树构建过程中，IForest算法会随机选择一个划分特征，并对划分特征随机选择一个划分阈值。&lt;/li&gt;
	&lt;li&gt;IForest算法构建的决策树一般深度max_depth是比较小的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;算法思路如下：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于异常点的判断，则是将测试样本x拟合到T棵决策树上。计算在每棵树上该样本的叶子节点的深度$h_t(x)$ 。从而计算出&lt;strong&gt;平均深度&lt;/strong&gt; $h(x) $ 。然后就可以使用下列公式计算样本点x的异常概率值，$p(x,m)$的取值范围为$[0,1]$ ，越接近于1，则是异常点的概率越大。&lt;/p&gt;

$$
p(x,m) = 2^{-\frac{h(x)}{c(m)}}  
$$
$$
c(m) = 2\ln(m-1)+\xi - 2\frac{m-1}{m}\ \ \ \ m为样本个数，\xi为欧拉常数
$$
&lt;p&gt;&lt;em&gt;注：这个公式可以简单的理解为越是出现在越深的层数，这个事件越不可能发生，足够深的情况基本上就可以判断为不可能发生是异常点&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;2、Boosting算法&lt;/h2&gt;

&lt;h3&gt;Adaboost&lt;/h3&gt;

&lt;h4&gt;总览&lt;/h4&gt;

&lt;p&gt;Adaboost全名为Adaptive Boosting，每轮迭代中会在训练集上产生一个新的学习器，然后使用该学习器对所有样本进行预测，以评估每个样本的重要性 (Informative)。换句话来讲就是，算法会为每个样本赋予一个权重，每次用训练好的学习器标注/预测各个样本，如果某个样本点&lt;strong&gt;被预测的越正确，则将其权重降低&lt;/strong&gt;，否则提高样本的权重。权重越高的样本在下一个迭代训练中所占的比重就越大，也就是说越难区分的样本在训练过程中会变得越重要。&lt;/p&gt;

&lt;p&gt;整个算法的迭代的结束条件就是错误率足够小或者达到一定的迭代次数为止。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：整体的过程很像，分豆子，先把我们直接能看出来的区别的豆子分开，留下不太能区分开来的豆子，然后交给母上大人帮忙再分这种感觉。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;细节描述&lt;/h4&gt;

&lt;p&gt;首先再重新强调下，从线性回归开始的两种思想，第一种是，&lt;strong&gt;设计出一个损失函数来代表预测结果，之后根据其应该为极小值和凸函数的特性，求原公式中的参数&lt;/strong&gt;，一般是用导数等于0这种方式。第二种思想则是，当有多个变量共同作用结果的时候，我们给每个变量前加参数，&lt;strong&gt;也就是权值来控制变量的影响结果的能力&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;这两种贯穿了几乎所有机器学习的思想，当然在Adaboost中也不会例外，整体的步骤分两部分：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;每次迭代都为新的弱学习器加权重，并根据损失函数计算得到这个权重。&lt;/li&gt;
	&lt;li&gt;根据这个新的学习器的预测结果，对每个样本特征的权重进行调整。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;算法构建之权重系数&lt;/h4&gt;

&lt;p&gt;假设我们打算用的最终分类器为$G(x)$，第m次迭代用的弱分类器为$G_m(x)$，并给分类器前加权重$\alpha_m$已保证分类准的分类器得到足够的重视。于是得到下面公式3.1，公式3.2。&lt;/p&gt;

$$
f(x) = \sum_{m=1}^M \alpha_m G_m(x)\ \ \ 公式3.1 
$$
$$
G(x) = sign(f(x)) = sign[\sum_{m=1}^M \alpha_m G_m(x)]\ \ \ 公式3.2
$$
&lt;p&gt;有了一个对算法整体的数学表达以后，我们就可以根据它写出AdaBoost的损失函数如下公式3.3：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：到现在为止大家应该对$e^{(-y_i f(x))}$这种公式的函数图不陌生了，就不赘述了&lt;/em&gt;&lt;/p&gt;

$$
loss = \frac{1}{n}\sum_{i=1}^n I(G(x_i) \neq y_i) \leq \frac{1}{n}\sum_{i=1}^n e^{(-y_i f(x_i))}\ \ \ 公式3.3 
$$
&lt;p&gt;有了损失函数了，那么$\alpha$在哪里呢，是要像SVM一样找个算法一起求么，显然不是了，如果那样子的话估计就不是集成算法了，它是一步一步求的，它&lt;strong&gt;只关心当前最好结果&lt;/strong&gt;，类比算法中的贪心算法。于是，我们讨论第k-1轮和第k论迭代的关系：&lt;/p&gt;

$$
f_{k-1}(x) = \ sum_{j=1}^{k-1}\alpha_j G_j(x) \ \ \ 第k-1轮的函数  
$$
$$
f_k(x) = \sum_{j=1}^k \alpha_j G_j(x) = f_{k-1}(x) + \alpha_k G_k(x) \ \ \ 第k轮函数
$$
&lt;p&gt;根据loss函数的构成方法，我们很容易写出第k轮含有$\alpha_k$的公式如下公式3.4，之后对其进行求导就可以得到$\alpha_k$的公式3.5：&lt;/p&gt;

$$
loss(\alpha_k,G_k(x)) = \frac{1}{n} \sum_{i=1}^n e^{(-y_i(f_{m-1}(x)+\alpha_k G_k(x)))} \ \ \ 公式3.4
$$
$$
\begin{split}
&amp;\alpha_k^* = \frac{1}{x}\ln(\frac{1-\varepsilon_k}{\varepsilon_k})\ \ \ 公式3.5 \\
&amp;\overline w_{ki} = e^{(-y_i f_{k-1}(x))} \\
&amp;\varepsilon_k = \frac{1}{n}\sum_{i=1}^n \overline w_{ki}I(y_i \neq G_m(x_i))
\end{split}
$$
&lt;p&gt;于是至此，我们就将弱分类器简单的连接在一起了，做好了下一步对数据样本特征值的权重调整的准备。&lt;/p&gt;

&lt;h4&gt;算法构建之样本特征权重调整&lt;/h4&gt;

&lt;p&gt;首先我们设定第k轮的数据集的权重分布为$D_k = (w_{k,1}, w_{k,2},... ,w_{k,n},)$。同时每次的$D_k$都是由$D_{k-1}$通过某种规律计算得到的，这种计算公式如下公式3.6：&lt;/p&gt;

$$
\begin{split} 
&amp;w_k = \frac{w_{k-1,i}}{Z_{k-1}}e^{-\alpha_{k-1}y_i G_{k-1}(x_i)}\ \ \ 公式3.6 \\
&amp;Z_k = \sum_{i=1}^n w_{k,i}e^{-\alpha_k y_i G_k(x_i)} 
\end{split}
$$
&lt;p&gt;可以看到这种&lt;strong&gt;第k次迭代开始前&lt;/strong&gt;的数据的权重的调整是在&lt;strong&gt;根据第k-1次迭代中预测结果&lt;/strong&gt;来进行调整的，换句话说，第k次迭代的数据集被第k-1次的训练修正了。&lt;/p&gt;

&lt;h4&gt;算法构建之总览&lt;/h4&gt;

&lt;p&gt;我们现在知道了每个弱分类器的权值是怎么够建的，也知道了Boosting算法中调整数据的部分是怎么调整的，算法的零件已经齐全，现在拼接起来如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;初始化数据集权重为$\frac{1}{n}$，n为特征的个数。&lt;/li&gt;
	&lt;li&gt;加入弱分类器，并根据数据集feed进模型确定这个新加入的弱分类的权重。&lt;/li&gt;
	&lt;li&gt;根据最终训练的结果，调整数据集中不同特征的权值&lt;/li&gt;
	&lt;li&gt;重复2，3步骤直到符合结束条件，一般为达到预计准确度，或者为达到规定迭代次数。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Gradient Boosting&lt;/h3&gt;

&lt;p&gt;首先值得注意的是，GBDT算法，它有很多&lt;strong&gt;别名如GBT，GTB，BGRT，GBDT，MART&lt;/strong&gt;，初学者很容易把它们当作是多个算法，比如我（笑。&lt;/p&gt;

&lt;p&gt;言归正传GBDT全名为Gradient Boosting Decision Tree。它也是Boosting算法的一种，它的算法推导相比之前算法的较为复杂，详细公式推导参考&lt;a href=&quot;https://blog.csdn.net/yangxudong/article/details/53872141&quot;&gt;这篇文章&lt;/a&gt;，这里就不赘述了。&lt;/p&gt;

&lt;p&gt;算法大体的步骤如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;算法每次迭代生成一颗新的决策树 

		&lt;p&gt;&lt;em&gt;注：GBDT的核心其实是找出一堆决策树，然后让他们的结果累加得到最终的预测值&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数和二阶导数&lt;/li&gt;
	&lt;li&gt;通过贪心策略生成新的决策树，通过等式计算每个叶节点对应的预测值

		&lt;p&gt;&lt;em&gt;注：这步是通过目标函数求导得到的，需要利用第二步中的二阶导数和一阶导数，同时等式的推导中用到了泰勒公式&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;把新生成的决策树 $f_t(x) $ 添加到模型中：$\widehat{y}_i^{t} = \widehat{y}_i^{t-1}+f_t(x_i)$

		&lt;p&gt;&lt;em&gt;注：这里我们会将模型替换为$\widehat{y}_i^{t} = \widehat{y}_i^{t-1}+\xi f_t(x_i)$，这里的$\xi$ 称之为步长或者学习率。增加ϵ因子的目的是为了避免模型过拟合。*
		&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;GBDT总结&lt;/h4&gt;

&lt;p&gt;GBDT优点如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;可以处理连续值和离散值&lt;/li&gt;
	&lt;li&gt;在相对少的调参情况下，模型的预测效果也会不错&lt;/li&gt;
	&lt;li&gt;模型的鲁棒性比较强。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;由于弱学习器之间存在关联关系，难以并行训练模型&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Bagging和Boosting的总结&lt;/h2&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;strong&gt;样本选择：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging算法是有放回的随机采样&lt;/li&gt;
			&lt;li&gt;Boosting算法是每一轮训练集不变，只是训练集中的每个样例在分类器中的权重发生变化，而权重根据上一轮的分类结果进行调整&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;样例权重：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging使用随机抽样，样例的权重&lt;/li&gt;
			&lt;li&gt;Boosting根据错误率不断的调整样例的权重值， 错误率越大则权重越大&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;预测函数：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging所有预测模型的权重相等&lt;/li&gt;
			&lt;li&gt;Boosting算法对于误差小的分类器具有更大的权重&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;并行计算：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging算法可以并行生成各个基模型&lt;/li&gt;
			&lt;li&gt;Boosting理论上只能顺序生产，因为后一个模型需要前一个模型的结果&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;Bagging是减少模型的&lt;strong&gt;variance(方差)&lt;/strong&gt;，Boosting是减少模型的&lt;strong&gt;Bias(偏度)&lt;/strong&gt;。&lt;/li&gt;
	&lt;li&gt;Bagging里每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合。Boosting里每个分类模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合。&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">集成学习 集成学习一句话版本 集成学习的思想是将若干个学习器（分类器&amp;amp;回归器）组合之后产生新的学习器。 在学习这一章节中，老师提到了这个说法，我觉得非常言简意赅就直接引用了过来。集成学习算法的成功在于保证若分类器（错误率略小于0.5，即勉强比瞎猜好一点）的多样性，且集成不稳定的算法也能得到一种比较明显的提升。 注：深度学习其实也可以看作是一种集成学习</summary></entry><entry><title type="html">贝叶斯算法</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" rel="alternate" type="text/html" title="贝叶斯算法" /><published>2018-08-19T00:00:00+08:00</published><updated>2018-08-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;贝叶斯算法&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;知识前置&lt;/h2&gt;
&lt;p&gt;这个章节的机器学习，其实更像是一种概率论的学习，同时这也是机器学习和数据分析中非常重要的一环。如果学习遇到了困难非常推荐参考张宇考研概率论部分的内容。同时这一章的算法，也是在文本分类中使用的比较多的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;名词解释：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;先验概率：$P(A)$&lt;/li&gt;
	&lt;li&gt;条件概率：$P(A|B)$&lt;/li&gt;
	&lt;li&gt;后验概率：$P(B|A)$&lt;/li&gt;
	&lt;li&gt;全概率：$P(B) = \sum_{i=1}^n P(A_i)*P(B|A_i)$&lt;/li&gt;
	&lt;li&gt;贝叶斯公式：$P(A|B) = \frac{P(A)*P(B|A)}{\sum_{i=1}^n P(B|A_i)*P(A_i)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;概率分布：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;高斯分布：简单的来说它的分布呈现的是正态分布的样子。&lt;a href=&quot;https://blog.csdn.net/renwudao24/article/details/44463489&quot;&gt;参考链接&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;伯努利分布：伯努利分布是0-1分布，简单的来说就是那种仍硬币的概率分布。&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83&quot;&gt;参考链接&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;多项式分布：是伯努利分布的推广，不再是只有两种情况，有多种情况的概率分布。&lt;a href=&quot;https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83&quot;&gt;参考链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;贝叶斯核心思想：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;找出在特征出现时，各个标签出现的概率，选择概率最大的作为其分类。&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;朴素贝叶斯&lt;/h2&gt;

&lt;p&gt;我们来“望文生义”的理解这个算法，贝叶斯指的就是上面的贝叶斯公式，而朴素则指的是“&lt;strong&gt;特征之间是独立的&lt;/strong&gt;”这个朴素假设。&lt;/p&gt;

&lt;p&gt;假设有给定样本X，其特征向量为$(x_1,x_2,...,x_m)$，同时类别为$y$。算法中使用公式2.1表达在当前特征下将类别y预测正确的概率。由于特征属性之间是假定独立的，所以$P(x_1,x_2,...x_m)$是可以直接拆开的，故根据这个特性优化，得到公式2.2。由于样本给定的情况下，$P(x_1,x_2,...,x_m)$的值不变，故研究概率最大的问题只需要研究公式2.2等号右侧上面的部分，最终写出预测函数公式2.3。&lt;/p&gt;

$$
P(y|x_1,x_2,...,x_m) = \frac{P(y)P(x_1,x_2,...,x_m|y)}{P(x_1,x_2,...,x_m)}\ \ \ 公式2.1  
$$
$$
P(y|x_1,x_2,...,x_m) = \frac{P(y)\prod_{i=1}^m P(x_i|y)}{P(x_1,x_2,...,x_m)}\ \ \ 公式2.2
$$
$$
\hat{y} = arg\ max_y P(y) \prod_{i=1}^m P(x_i|y) \ \ \ 公式2.3
$$
&lt;p&gt;到这里，算法的流程就很显而易见了，和softmax算法类似，让预测正确的概率最大即可，具体计算流程如下：&lt;/p&gt;

&lt;p&gt;设$x = {a_1,a_2,...a_m}$为带分类项，其中a为x的一个特征属性，类别集合$C={y_1,y_2,...y_n}$&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;分别计算所有的$P(y_i|x)$，使用上述公式2.3&lt;/li&gt;
	&lt;li&gt;选择$P(y_i|x)$最大的$y_i$作为x的类型&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;其他朴素贝叶斯&lt;/h2&gt;

&lt;h3&gt;高斯朴素贝叶斯&lt;/h3&gt;

&lt;p&gt;在上述贝叶斯算法中的特征是离散的，那么考虑特征属虚连续值时，且分布服从高斯分布的情况下。用高斯公式（公式3.1）代替原来计算概率的公式。那么根据训练集中，对应的类别下的属性的均值和标准差，对比待分类数据中的特征项划分的各个均值和标准差，即可得到预测类型。&lt;/p&gt;

$$
p(x_k|y_k) = g(x_k,\eta_{y_k},\sigma_{y_k}) = \frac{1}{\sqrt{2 \pi}\sigma}e^{-\frac{(x-\eta_{y_k})^2}{2\sigma_{y_k}^2}}\ \ \ 公式3.1
$$
&lt;h3&gt;伯努利朴素贝叶斯&lt;/h3&gt;

&lt;p&gt;特征值的取值是布尔型的，是有true和false，符合伯努利分布，那么其$P（x_i|y_k）$的表达式如下公式3.3。&lt;/p&gt;

$$
P（x_i|y_k）= P(x_i = 1 | y_k)*x_i + (1-P(x_i=1|y_k))(1-x_k)\ \ \ 公式3.2
$$
&lt;p&gt;&lt;em&gt;注：这意味着没有某个特征也可以是一个特征，其中公式3.2其实是把两个不同条件的概率公式融合在一起了，这种方法也在逻辑回归中使用过&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;多项式朴素贝叶斯&lt;/h3&gt;

&lt;p&gt;特征属性分布服从多项分布时，得到如下公式3.3，公式的来源简单的来说就是已知盒子中红球和所有球的总个数，求从盒中摸到红球的概率差不多。&lt;/p&gt;

&lt;p&gt;其中$N_{y_k x_i} $为类别$y_k$下，特征$x_i$出现的次数，$N_{y_k}$ 指的是类别 $y_k$ 下，所有特征出现的次数。&lt;/p&gt;

$$
P(x_i|y_k) = \frac{N_{y_k x_i} + \alpha}{N_{y_k} + \alpha n}  
$$
&lt;p&gt;&lt;em&gt;注：待预测样本中的特征xi在训练时可能没有出现，如果没有出现，则$N_{y_k x_i} $ 值为0，如果直接拿来计算该样本属于某个分类的概率，结果都将是0。所以在分子中加入α，在分母中加入αn可以解决这个问题。&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;贝叶斯网络&lt;/h2&gt;

&lt;p&gt;由于之前朴素贝叶斯，前提条件是假定特征值之间没有关系，这显然是不现实的而贝叶斯网络正是解决这个问题的。其&lt;strong&gt;关键方法是图模型&lt;/strong&gt;，我们构建一个图模型，把具有因果联系的各个变量联系在一起。贝叶斯网络的有向无换图中的节点表示随机变量，连接节点的箭头表示因果关系。&lt;/p&gt;

&lt;p&gt;简单的来说贝叶斯网络就是模拟人的认知思维推理模式的，用一组条件概率以及有向无换图对不确定关系推理关系建模。&lt;/p&gt;

&lt;p&gt;而这种方式在深度学习之前是很受欢迎的，它和之后的隐马尔可夫被使用作为提取特征的工具，而现在渐渐的过度到了深度学习。&lt;/p&gt;

&lt;h3&gt;贝叶斯网络工作原理&lt;/h3&gt;

&lt;p&gt;首先贝叶斯网络的实质就是建立一个有向无环图，其中方向代表因果关系。仔细思考一下，为什么是有向无环图，是因为如果是有环的话，就会有节点是自己依赖于自己，显然这样是有问题的。&lt;/p&gt;

&lt;p&gt;具体贝叶斯工作的核心原理可以理解为，根据人已知的经验或者其他手段，规定一些完全没有依赖于其他事件的事件发生的概率，随后根据制作的贝叶斯网络（因果关系图）推算出不同事件发生的概率。这个过程有点像是在做一个概率论的期末考试题，已知A，B，C的概率和ABCD之间转换的关系，问在发生了BC条件下，发生D的概率。大体就是这样一种感觉。&lt;/p&gt;

&lt;p&gt;事例如下图：&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fuiulz6amfj30iy0ge74k.jpg&quot;/&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;其中$x_1,x_2,x_3$独立，则$x_6,x_7$独立&lt;/strong&gt;，$x_1,x_2,x_3,...,x_7$的联合概率分布如下：&lt;/p&gt;

$$
p(x_1,x_2,...,x_7) = p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)p(x_5|x_1,x_3)p(x_6|x_4)p(x_7|x_4,X_5)
$$
&lt;p&gt;实际上这部分的概率计算，其实就是根据初始条件和转移方式，求的目标的概率这样的过程。和之前常用的最大似然估计算法对比，贝叶斯的这一系列算法考虑了先验概率，而最大似然估计算法没有，在最大似然估计算法中其实相当于默认了先验概率是相同的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：最大后验概率MAP其实可以看作是贝叶斯算法和最大似然估计算法结合的应用&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">贝叶斯算法 知识前置 这个章节的机器学习，其实更像是一种概率论的学习，同时这也是机器学习和数据分析中非常重要的一环。如果学习遇到了困难非常推荐参考张宇考研概率论部分的内容。同时这一章的算法，也是在文本分类中使用的比较多的。 名词解释：</summary></entry><entry><title type="html">聚类算法（下）</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8B/" rel="alternate" type="text/html" title="聚类算法（下）" /><published>2018-08-19T00:00:00+08:00</published><updated>2018-08-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95(%E4%B8%8B)</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8B/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;聚类算法（下）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;聚类算法上中讲了大名鼎鼎的K-Means算法及其优化变种，在这篇中几种讲述两位两种不同思路的聚类算法。&lt;/p&gt;
&lt;h2&gt;层聚类算法&lt;/h2&gt;
&lt;h3&gt;传统层聚类算法—AGNES和DIANA算法&lt;/h3&gt;
&lt;p&gt;层次聚类和K-Means的思路不太一样，它的思路有点像是决策树，按照层次进行分解，知道满足某种条件为止，传统的层次聚类分为自底而上，和自上而下两类：&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;凝聚的层次聚类:

		&lt;p&gt;这类算法是采用&lt;strong&gt;采用自底向上&lt;/strong&gt;的策略，其中的代表便是&lt;strong&gt;AGNES算法&lt;/strong&gt;(AGglomerative Nesting)，它的核心思想是：最初将每个对象作为一个簇，然后这些簇根据某些准则被一步一步合并，两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定。聚类的合并过程反复进行直到所有的对象满足簇数目。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;分裂的层次聚类：

		&lt;p&gt;和凝聚的层次聚类相反，这种是采用&lt;strong&gt;自顶向下&lt;/strong&gt;的策略，代表算法为&lt;strong&gt;DIANA算法&lt;/strong&gt;(DIvisive Analysis)。其核心思想是：首先将所有对象置于一个簇中，然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式 距离)，直到达到某个终结条件(簇数目或者簇距离达到阈值)。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在AGNES算法中都提到了，簇是根据某些原则进行分裂或者合并的，而这个原则就是&lt;strong&gt;簇间距离&lt;/strong&gt;。计算簇间距离的方法有最小距离（SL聚类），最大距离（CL聚类）以及平均距离（AL聚类），具体的说明如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;最小距离（SL聚类）

		&lt;p&gt;选择两个聚簇中最近的两个样本之间的距离（&lt;strong&gt;S&lt;/strong&gt;ingle/Word-&lt;strong&gt;L&lt;/strong&gt;inkage）&lt;/p&gt;

		&lt;p&gt;&lt;em&gt;注：得到的模型容易形成链式结构&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最大距离（CL聚类）

		&lt;p&gt;选择两个聚簇中最圆的两个眼本的距离（&lt;strong&gt;C&lt;/strong&gt;omplete-&lt;strong&gt;L&lt;/strong&gt;inkage）&lt;/p&gt;

		&lt;p&gt;&lt;em&gt;注：如果出现了异常值的话，那他们的构建很容易受这个异常值的影响。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;平均距离（AL聚类）

		&lt;p&gt;选择两个聚类中的平均值（Average-Linkage聚类算法）或者中值（Median-Linkage聚类法）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;AGNES和DIANA算法优缺点如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;简单，理解容易。&lt;/li&gt;
	&lt;li&gt;合并点/分裂点选择不太容易。&lt;/li&gt;
	&lt;li&gt;合并/分类的操作不能进行撤销。&lt;/li&gt;
	&lt;li&gt;由于执行效率较低$O(t*n^2)$，$t$为迭代次数，$n$为样本点数。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;层次聚类优化算法&lt;/h3&gt;

&lt;p&gt;之前我们看到了传统的层次聚类算法，由于其执行效率太低，且不能动构建的的特点，显然不适合大数据集。于是我们在此基础上引入了&lt;strong&gt;BIRCH算法&lt;/strong&gt;和&lt;strong&gt;CURE算法&lt;/strong&gt;。&lt;/p&gt;

&lt;h4&gt;BIRCH算法&lt;/h4&gt;

&lt;p&gt;BIRCH (balanced iterative reducing and clustering using hierarchies) 算法，英文的全称翻译过来以后是平衡迭代削减聚类算法，其构成和我们考研数据结构中学过的B+树非常的类似，甚至很多特性都是相同的，具体的说它构建的树叫做CF（Cluster Feature）-Tree。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;节点，即簇的结构：

		&lt;p&gt;既然是树，那么就不得不提它的节点的结构了。在BIRCH构建CF树的过程中，每个节点等于说是存放了它之下所有节点的特征，于是他在节点中存放了如下的三部分数据。&lt;/p&gt;

		&lt;ul&gt;
			&lt;li&gt;N，指在这个节点中有多少个样本点。&lt;/li&gt;
			&lt;li&gt;LS，指的是这个节点中的样本相应特征的和。&lt;/li&gt;
			&lt;li&gt;LS，指的是这个节点中的样本相应特征的特征的平方和。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;节点之间，节点和子节点，以及叶子结点之间的关系

		&lt;p&gt;节点和其子节点是包含的关系，也就是父节点中的N，LS以及SS是其所有子节点的和。而相应的样本点的具体信息指包含在底层节点中（叶子结点的子节点），同时叶子结点构成一个单项链表，同时有一个指针指向其表头。这点的特性是&lt;strong&gt;同B+树高度一致的&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最多子女个数，以及分裂判定

		&lt;p&gt;和B+树一样，对于树构建中的分叉个数是有限制的，这个限制需要提前给出，即&lt;strong&gt;分支因子&lt;/strong&gt;。同时，&lt;strong&gt;值得注意的是&lt;/strong&gt;，一般而言在构建节点簇的中心点的时候，一般选用第一个进入这个节点的样本点作为中心点，然后根据指定的该簇和中心点限定的距离，即&lt;strong&gt;类直径&lt;/strong&gt;，其往往通过LS和SS算出。判断新入的点是否可以划入该簇，而分裂节点的时候，往往以这个初始点进行分割。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;综上我们可以看出，BIRCH算法的本质&lt;strong&gt;其实就是动态的插入样本点，然后动态的根据规则构建一个类B+树。&lt;/strong&gt;它的优点是动态建树且效率高是线性效率，即每个样本点都是一次性插入的，同时也节省内存，所以非常适合大数据集。不过遗憾的是它也是采用距离作为分类标准，故&lt;strong&gt;只适合分布呈凸形或者球形的数据集&lt;/strong&gt;、且需要给定聚类个数和簇之间的相关参数，而&lt;strong&gt;这些对节点CF的限制可能导致簇类结果和真实不太一致&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1：BIRCH不依赖给定的待分类簇数量K，但是给定了K值最好，若不一定K值，最终CF-Tree的叶子结点树木就是最终分类的簇的数目。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2:BIRCH算法在训练大规模数据集的时候，和mini-batch K-Means相比，BIRCH算法更加适合类别数量K比较多的情况。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注3：由于类直径是通过LS和SS算出的，所以当特征维度超过20～30左右的时候，不建议使用该算法。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;CURE算法（使用代表点的聚类法）&lt;/h4&gt;

&lt;p&gt;CURE（Clustering Using REpresentatives），该算法先把每个数据点看成一类，然后合并距离最近的类直至类个数为所要求的个数为止。但是和AGNES算法的区别是：取消了使用所有点，或用中心点+距离来表示一个类，而是从每个类中抽取固定数量、 分布较好的点作为此类的代表点，并将这些代表点乘以一个适当的收缩因子，使它们更加靠近类中心点。代表点的收缩特性可以调整模型可以匹配那些非球形的场景，而且&lt;strong&gt;收缩因子的使用可以减少噪音对聚类的影响&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;CURE算法的优点是能够处理&lt;strong&gt;非球形分布&lt;/strong&gt;的应用场景，同时彩娱乐随机抽样和分区的方式可以提高算法的执行效率。&lt;/p&gt;

&lt;h2&gt;密度聚类算法&lt;/h2&gt;

&lt;p&gt;密度聚类方法的指导思想是：只要样本点的密度大于某个阀值，则将该样本添加到最近的簇中。这类算法可以克服基于距离的算法只能发现凸聚类的缺点，可以发现任意形状的聚类，而且对噪声数据不敏感。不过这种计算的复杂度高，计算量大。&lt;/p&gt;

&lt;p&gt;密度聚类算法的常用算法有&lt;strong&gt;DBSCAN&lt;/strong&gt;和&lt;strong&gt;密度最大值算法&lt;/strong&gt;。&lt;/p&gt;

&lt;h3&gt;DBSCAN算法&lt;/h3&gt;

&lt;p&gt;DBSCAN（Density-Based Spatial Clustering of Applications with Noise），将簇定义为密度相连的点的最大集合，能够将足够高密度的区域划分为簇，并且在具有噪声的空间数据上能够发现任意形状的簇。其核心思路&lt;strong&gt;是用一个点的ε邻域内的邻居点数衡量该点所在空间的密度&lt;/strong&gt;，该算法可以找出形状不规则的cluster，而且聚类的时候事先不需要给定cluster的数量。&lt;/p&gt;

&lt;h4&gt;DBSCAN算法流程&lt;/h4&gt;

&lt;p&gt;它的算法流程如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;如果一个点$x$的$\varepsilon$领域内包含m个对象，则创建一个x作为&lt;strong&gt;核心对象&lt;/strong&gt;的新簇。&lt;/li&gt;
	&lt;li&gt;寻找并合并核心对象&lt;strong&gt;直接密度可达&lt;/strong&gt;的对象&lt;/li&gt;
	&lt;li&gt;没有新点可以更新簇的时候，算法结束&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1. 每个簇至少包含一个&lt;strong&gt;核心对象&lt;/strong&gt;；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2. 非核心对象可以是簇的一部分，构成簇的边缘；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;3. 包含过少对象的簇被认为是噪声；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;4. 最大的密度相连对象的集合C为密度聚类中的一个簇，它满足两个属性，Maximality和Connectivity，Maximality指的是若$x$属于C，$y$从$x$&lt;strong&gt;密度可达&lt;/strong&gt;，那么$y$也属于C，Connectivity指的是，若$x$和$y$都属于C，那么$x$和$y$是&lt;strong&gt;密度相连&lt;/strong&gt;的。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;DBSCAN相关名词解释&lt;/h4&gt;

&lt;p&gt;其中提到的定义有$\varepsilon$领域，密度，MinPts，核心点，边界点，噪音点，直接密度可达，密度可达，密度相连。他们的解释如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;$\varepsilon$邻域($\varepsilon$ neighborhood）：给定对象在半径$\varepsilon$的区域。&lt;/li&gt;
	&lt;li&gt;密度(density)：在$\varepsilon$领域中的$x$的密度，是一个整数依赖于半径$\varepsilon$，$N_{\varepsilon}(X) $指的是半径内的点的个数。

		$$
		p(x) = |N_{\varepsilon}(X)|  
		$$&lt;/li&gt;
	&lt;li&gt;MinPts：指得是判定该点是不是核心点的时候使用的阀值，记为M&lt;/li&gt;
	&lt;li&gt;核心点（core point）：如果$p(x) \geq M$ ,那么称$x$为$X$的核心点，记由$X$中所有核心点构成的集合为$X_c$，并记$X_nc$ 表示由$X$中所有非核心点构成的集合。通俗的来说， 核心点是密度达到一定阀值的的点。&lt;/li&gt;
	&lt;li&gt;边界点（border point）：如果非核心点$x$的$\varepsilon$邻域中存在核心点，那么认为$x$为$X$的边界点。通俗来讲就是密度特别稠密的边缘地带，也就是簇的边缘部分。&lt;/li&gt;
	&lt;li&gt;噪音点（noise point）：集合中除了边界点和核心点之外的点都是噪音点，所有噪音点组成的集合叫做$X_noi$，显然这些点就是对应稀疏区域的点。&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;直接密度可达&lt;/strong&gt;：这个是密度聚类中最重要的概念，它指的是给定一个对象集合 $X$，如果$y$是在$x$的$\varepsilon$邻域内，而且$x$是一个核心对象，可以说对象y从对象$x$出发是直接密度可达的&lt;/li&gt;
	&lt;li&gt;密度可达：如果存在一个对象链$p_1, p_2,...,p_m $ ，如果满足$p_{i+1}$是从$p_i$&lt;strong&gt;直接密度可达&lt;/strong&gt;的，那么称$p_m$是从$p1$密度可达的，简单的来说就像铁链环环相扣差不多。&lt;/li&gt;
	&lt;li&gt;密度相连：在集合$X$中，如果存在一个对象$o$，使得对象$x$和$y$是从$o$关于$\varepsilon$和$m$密度可达的，那么对象$x$和$y$是关于$\varepsilon$和$m$密度相连的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;DBSCAN算法优缺点&lt;/h4&gt;

&lt;p&gt;优点: &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;不需要事先给定cluster的数目&lt;/li&gt;
	&lt;li&gt;可以发现&lt;strong&gt;任意形状的cluster&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;能够&lt;strong&gt;找出数据中的噪音，且对噪音不敏感&lt;/strong&gt; &lt;/li&gt;
	&lt;li&gt;算法只需要&lt;strong&gt;两个输入参数&lt;/strong&gt; &lt;/li&gt;
	&lt;li&gt;聚类结果几乎不依赖节点的遍历顺序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;DBSCAN算法聚类效果依赖距离公式的选取，最常用的距离公式为欧几里得距离。但是对于高维数据，由于维数太多，距离的度量已变得不是那么重要&lt;/li&gt;
	&lt;li&gt;DBSCAN算法不适合数据集中密度差异很小的情况&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;MDCA密度最大值聚类算法&lt;/h3&gt;

&lt;p&gt;MDCA(Maximum Density Clustering Application)算法基于密度的思想引入划分聚类中，能够自动确定簇数量并发现任意形状的簇。另外MDCA一般不保留噪声，因此也避免了阈值选择不当情况下造成的对象丢弃情况。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：MDCA的算法和AGNES非常相像，不同的是最初的初始簇确定是通过密度来确定的。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;MDCA算法思路&lt;/h4&gt;

&lt;p&gt;MDCA算法核心一共分三步，划分、合并簇以及处理剩余节点三部分。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;将数据集划分为基本簇：

		&lt;ul&gt;
			&lt;li&gt;对数据集X选取最大密度点$P_{max}$ ，形成以最大密度点为核心的新簇$C_i$，按照距离排序计算出序列$S_{p_max}$,对序列的前M个样本数据进行循环判断，如果节点的密度大于等于$density_0$ ，那么将当前节点添加$C_i$中。&lt;/li&gt;
			&lt;li&gt;循环处理剩下的数据集X，选择最大密度点$P_{max}$，并构建基本簇$C_{i+1}$，直到X中剩余的样本数据的密度均小于$deansity_0$。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;使用凝聚层次聚类的思想，合并较近的基本簇，得到最终的簇划分：

		&lt;ul&gt;
			&lt;li&gt;在所有簇中选择距离最近的两个簇进行合并，合并要求是：簇间距小于等于$dist_0$，如果所有簇中没有簇间距小于$dist_0$的时候，结束合并操作&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;处理剩余节点，归入最近的簇

		&lt;ul&gt;
			&lt;li&gt;最常用、最简单的方式是：将剩余样本对象归入到最近的簇。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;MDCA算法名词解释&lt;/h4&gt;

&lt;p&gt;最大密度点：如字面意思，就是密度最大的点，密度计算公式一般取DBSCAN算法中的密度计算公式。&lt;/p&gt;

&lt;p&gt;有序序列$S_{p_{max}}$：根据所有对象与最大密度点的距离进行排序。&lt;/p&gt;

&lt;p&gt;密度阈值$density_0$：当节点的密度值大于密度阈值的时候，认为该节点属于一个 比较固定的簇，在第一次构建基本簇的时候，就将这些节点添加到对应簇中，如果小于这个值的时候，暂时认为该节点为噪声节点。&lt;/p&gt;

&lt;p&gt;簇间距离：对于两个簇C1和C2之间的距离，采用两个簇中最近两个节点之间的距离作为簇间距离。&lt;/p&gt;

&lt;p&gt;M值：初始簇中最多数据样本个数&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">聚类算法（下） 聚类算法上中讲了大名鼎鼎的K-Means算法及其优化变种，在这篇中几种讲述两位两种不同思路的聚类算法。 层聚类算法 传统层聚类算法—AGNES和DIANA算法 层次聚类和K-Means的思路不太一样，它的思路有点像是决策树，按照层次进行分解，知道满足某种条件为止，传统的层次聚类分为自底而上，和自上而下两类： 凝聚的层次聚类:</summary></entry><entry><title type="html">聚类算法（上）</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8A/" rel="alternate" type="text/html" title="聚类算法（上）" /><published>2018-08-19T00:00:00+08:00</published><updated>2018-08-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95(%E4%B8%8A)</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8A/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;聚类算法（上）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;聚类算法很多，所以和讲回归算法一样，分成了上下，上中主要讲了传统的K-Means算法以及其相应的优化算法入K-Means++，K-Means||和Canopy等。下中主要讲了另外两种的思路的聚类算法，即层次聚类和密度聚类。&lt;/p&gt;
&lt;h2&gt;什么是聚类&lt;/h2&gt;
&lt;p&gt;聚类算就是怼大量未知标注的数据集，按照数据&lt;strong&gt;内部存在的数据特征&lt;/strong&gt;将数据集&lt;strong&gt;划分为多个不同的类别&lt;/strong&gt;，使类别内的数据比较相似，类别之间的数据相似度比较小，属于&lt;strong&gt;无监督学习&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;从定义就可以看出，聚类算法的关键在于计算样本之间的&lt;strong&gt;相似度&lt;/strong&gt;，也称为&lt;strong&gt;样本间的距离&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;相似度/距离计算公式&lt;/h2&gt;
&lt;p&gt;说到聚类算法，那肯定核心就是计算距离的公式了，目前常用的有以下几种。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;闵可夫斯基距离（Minkowski）&lt;/strong&gt;：公式2.1&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;当p为1的时候是曼哈顿距离（Manhattan）：公式2.2&lt;/li&gt;
	&lt;li&gt;当p为2的时候是欧式距离（Euclidean）：公式2.3

		&lt;ul&gt;
			&lt;li&gt;标准化欧式距离：

				&lt;p&gt;这个距离的计算方式如同其字面意思，标准化欧式距离就是对欧式距离的标准化。标准化的正常定义为，$X^* = \frac{X - \overline X}{s}$，这个$s$指的就是方差，而方差的计算公式为$s = \sqrt{\frac{\sum_{i=1}^n(x_i - \overline X)^2}{n}}$，所以其标准化公式如下公式2.5。&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;当p为无穷大的以后是切比雪夫距离（Chebyshev）：公式2.4&lt;/li&gt;
&lt;/ul&gt;

$$
dist(X,Y)= \sqrt[p]{\sum_{i=1}^{n} |x_i - y_i|^p}\ \ \ 公式2.1
$$
$$
M\_dist=\sum_{i=1}^n|x_i-y_i| \ \ \ 公式2.2
$$
$$
E\_dist = \sqrt{\sum_{i=1}^n|x_i-y_i|^2} \ \ \ 公式2.3
$$
$$
C\_dist = max_i(|x_i-y_i|)\ \ \ 公式2.4
$$
$$
S\_E\_D = \sqrt{\sum_{i=1}^n(\frac{x_i-y_i}{s_i})^2}\ \ \ 公式2.5 
$$
&lt;p&gt;&lt;strong&gt;夹角余弦相似度（Cosine）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;使用这个公式的时候，需要注意的是，这里的相似之的是&lt;strong&gt;同一个方向上&lt;/strong&gt;的，而同一个方向上的两个点可能距离是非常远的。比如一个吻张灏总分别出现单词A 10次，单词B 20次，另一个文章中出现单词A 100次，单词B 200次，这时候如果使用欧几里得距离的话，这两个文章是不相似的，然而显然这两个单词的比例相似很能说明这两个文章其实是有关系的，所以在文章的相似度的判别中使用夹角余弦相似度比较合适，公式如下2.6。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;个人理解为，其是从距离以外的衡量相似度的另一个维度的指标&lt;/strong&gt;。&lt;/p&gt;

$$
\cos(\theta)  = \frac{\sum_{k=1}^n x_{1k}x_{2k}}{\sqrt{\sum_{k=1}^n x_{1k}^2} * \sqrt{\sum_{k=1}^n x_{2k}^2}} = \frac{a^T · b}{|a||b|}\ \ \ 公式2.6
$$
&lt;p&gt;&lt;strong&gt;KL距离（相对熵）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;思考下条件熵的定义，简单的来说就是在放生一件事情的时候，发生另一件事的概率。公式如下公式2.7.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：这里书的概率不是实指概率，而是熵表达的含义。这个公式其实就是条件熵的公式。&lt;/em&gt;&lt;/p&gt;

$$
D(P|Q)=\sum_x P(x)\log(\frac{P(x)}{Q(x)})\ \ \ 公式2.7
$$
&lt;p&gt;&lt;strong&gt;杰卡德相似系数(Jaccard)&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;这个很好理解，它的核心就是使用两个集合的交集和并集的比率来代表两者的相似度，也就是说重合的越多越相似。公式如下，公式2.8.&lt;/p&gt;

$$
J(A,B) = \frac{|A\bigcap B|}{|A \bigcup B|}  
$$
$$
dist(A,B) = 1-J(A,B) \ \ \ 公式2.8 
$$

&lt;p&gt;&lt;strong&gt;Pearson相关系数&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;这个就是考研数学中的相关系数，表达就是两者之间的想关系，所以直接拿来用就好了，公式如下公式2.9。&lt;/p&gt;

$$
\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{E[(X-E(X))(Y-E(Y))]}{\sqrt{D(X)}\sqrt{D(Y)}} = \frac{\sum_{i=1}^n(X_i - \mu_x)(Y_i - \mu_Y)}{\sqrt{\sum_{i=1}^n(X_i - \mu_X)^2}*\sqrt{\sum_{i=1}^n(Y_i - \mu_Y)^2}}  
$$
$$
dist(X,Y) = 1 - \rho_{XY}\ \ \ 公式2.9
$$

&lt;h2&gt;聚类的思想&lt;/h2&gt;

&lt;p&gt;给定一个有M个对象的数据集，构建一个具有k个簇的模型，其中k&amp;lt;=M。满足 以下条件:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;每个簇至少包含一个对象&lt;/li&gt;
	&lt;li&gt;每个对象属于且仅属于一个簇 &lt;/li&gt;
	&lt;li&gt;将满足上述条件的k个簇成为一个合理的聚类划分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本思想:&lt;/p&gt;

&lt;p&gt;对于给定的类别数目k，首先给定初始划分，通过迭代改变样本和簇的隶属关系，使的每次处理后得到的划分方式比上一次的好，即&lt;strong&gt;总的数据集之间的距离和变小了&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;K-Means 系列&lt;/h2&gt;

&lt;h3&gt;K-Means算法&lt;/h3&gt;

&lt;p&gt;K-means的核心算法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 假设输入样本T为x1,x2,x3,...,xm
&lt;/span&gt;
初始化k个类别的中心点a1,a2,a3,&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;...&lt;/span&gt;,ak
&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;not&lt;/span&gt; EndCondition :
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;1&lt;/span&gt;.根据每个样本和中心点的欧几里得距离，选择最近的中心点作为自己的类别
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;2&lt;/span&gt;.更新每个类别的中心点aj，为隶属该类别的所有的样本的均值

&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# EndCondition有迭代次数，最小平方误差MSE，簇中心点变化率。
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再循环中的第二步，我们移动了中心点的位置，把中心点移到了隶属于该中心点类别的所有样本的中间，并使用样本的均值作为位置。这样子看似是拍脑袋想的移动策略，其实是可以推导出来的。正如聚类算法思想所指出的，我们要让所有的点到自己的分类的中心点的欧几里得距离最小，所以我们设置目标放称为公式4.1，公式中的1/2是为了之后求导运算方便。我们为了让目标函数尽可能的小，所以使用了之前一直在使用的思考方式，对其使用梯度下降算法，求导后得到公式4.2，之后令其等于0，就得到了公式4.3。&lt;/p&gt;

$$
J(a_1,a_2,a_3,...,a_k) = \frac{1}{2}\sum_{j=1}^K \sum_{i=1}^n (x_i - a_j)^2 \ \ \ 公式4.1
$$
$$
\frac{\partial J}{\partial a_j} = \sum_{i=1}^{N_j}(x_i-a_j)\ \ \ 公式4.2
$$

$$
a_j = \frac{1}{N}\sum_{i=1}^{N_j} x_i \ \ \ 公式4.3 
$$
&lt;p&gt;最后这个看似不错的算法，其实有着不小的缺点，那就是&lt;strong&gt;初值敏感&lt;/strong&gt;。我们来仔细想一想，如果两个不小心随机生成的初值落到了一个类别中，两者的距离还特别近，这中情况下就很难正确分类了。除此之外，由于移动策略中使用的是均值，也就是说如果集合中含有非常大的误差点的话，这样子会是中心点的设置偏离正确点很远，所以很多时候我们改用&lt;strong&gt;中值来更新中心点&lt;/strong&gt;，这就是我们说的K-Mediods聚类，即K中值聚类。&lt;/p&gt;

&lt;p&gt;总结下K-means算法&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;理解容易，聚类效果不错&lt;/li&gt;
	&lt;li&gt;处理大数据集的时候，该算法可以保证较好的伸缩性和高效率&lt;/li&gt;
	&lt;li&gt;当簇近似高斯分布的时候，效果非常不错&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt; K值是用户给定的，在进行数据处理前，K值是未知的，不同的K值得到的结果也不一样&lt;/li&gt;
	&lt;li&gt;对初始簇中心点是敏感的 &lt;/li&gt;
	&lt;li&gt;不适合发现非凸形状的簇或者大小差别较大的簇 &lt;/li&gt;
	&lt;li&gt;特殊值(离群值)对模型的影响比较大&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;二分K-Means算法&lt;/h3&gt;

&lt;p&gt;由于K-Means对初始中心点非常敏感，我们这里就尝试着通过二分法弱化初始中心点。这种算法的具体步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 把所有样本数据作为一个簇放到队列中
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;not&lt;/span&gt; EndCondition:
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;1&lt;/span&gt;.从队列中选择一个簇，使用K&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;-&lt;/span&gt;means划分为两个簇
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;2&lt;/span&gt;.将划分好的两个簇放回队列
&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# EndCondition 为簇的数量，最小平方误差，迭代次数等
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 选择簇的手段有两种1.使用SSE 2.选择数据量最多的簇
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在这个算法中提到了SSE，这个可以是簇内所有样本点，到其中心点的距离的总和，代表着簇内的点是不是高度相关。计算公式如下公式4.4。&lt;/p&gt;

$$
SSE = \sum_{i=1}^n w_i(y_i - \hat y_i)^2\ \ \ 公式4.4
$$
&lt;p&gt;可以看出在这种算法下，很好的避开了，两个中心点都在一起的情况。&lt;/p&gt;

&lt;h3&gt;K-Means++和K-Means||&lt;/h3&gt;

&lt;p&gt;K-Means++做的改善，是直接对初始点的生成位置的选择进行优化的，他的初始点生成策略如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;从数据集中任选一个节点作为第一个聚类中心&lt;/li&gt;
	&lt;li&gt;对数据集中的每个点x，计算x到所有已有聚类中心点的距离和D(X)，基于D(X)采用线性概 率选择出下一个聚类中心点(距离较远的一个点成为新增的一个聚类中心点)&lt;/li&gt;
	&lt;li&gt;重复步骤2直到找到k个聚类中心点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看出，K-Means++企图生成相聚距离较远的几个中心点。但是缺点也是显而易见的，由于聚类中心点选择过程中的内在有序性，在扩展方面存在着性能方面的问题，即&lt;strong&gt;第k个聚类中心点的选择依赖前k-1个聚类中心点的值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;而K-Means||就是针对K-Means++缺点作出了的优化，主要思路是改变每次遍历时候的取样规则，并非按照K-Means++算法每次遍历只获取一个样本，而是每次获取 K个样本，重复该取样操作$O(\log{n}w	z)$ 次，然后再将这些抽样出来的样本聚类出K个点，最后使用这K个点作为K-Means算法的初始聚簇中心点。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：一般5次重复采用就可以保证一个比较好的聚簇中心点。&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Canopy算法&lt;/h3&gt;

&lt;p&gt;Canopy属于一种“粗略地”聚类算法，简单的来说就是，不那么追求自动获得最优解，而是引入了一种人为规定的先验值进行聚类，具体步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 给定样本列表L=x1,x,2...,xm以及先验值r1和r2(r1 &amp;gt; r2)
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;L&lt;/span&gt;:
	计算P到所有聚簇中心点的距离(如果不存在聚簇中心，那么此时点P形成一个新的聚簇)，并选择出最小距离D(&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;P&lt;/span&gt;,aj)
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;&amp;lt;&lt;/span&gt; r1:
		&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 表示该节点属于该聚簇
&lt;/span&gt;		添加到该聚簇列表中
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;&amp;lt;&lt;/span&gt; r2:
		&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 表示该节点不仅仅属于该聚簇，还表示和当前聚簇中心点非常近，
&lt;/span&gt;		将该聚簇的中心点设置为P，并将P从列表L中删除
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;&amp;gt;&lt;/span&gt; r1:
		节点P形成一个新的聚簇
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; EndCondition:
		&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 结束条件为直到列表L中的元素数据不再有变化或者元素数量为0的时候
&lt;/span&gt;		&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;break&lt;/span&gt;		&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;注：Canopy算法得到的最终结果的值，聚簇之间是可能存在重叠的，但是不会存在 某个对象不属于任何聚簇的情况&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;显然，这种算法虽然快，但是很难生成满足我们应用的模型，所以通常我们将它作为解决K-Means初值敏感的方案，他们合在一起就是Canopy+K-Means算法。&lt;/p&gt;

&lt;p&gt;顺序就是先使用Canopy算法获得K个聚类中心，然后用这K个聚类中心作为K-Means算法。这样子就很好的解决了K-Means初值敏感的问题。&lt;/p&gt;

&lt;h3&gt;Mini Batch K-Means算法&lt;/h3&gt;

&lt;p&gt;Mini Batch K-Means算法是K-Means算法的一种优化变种，采用小规模的数据子集，来减少计算时间。其中采用小规模的数据子集指的是每次训练使用的数据集是在训练算法的时候随机抽取的数据子集。Mini Batch K-Means算法可以减少K-Means算法的收敛时间，而且产生的结果效果只是略差于标准K-Means算法。&lt;/p&gt;

&lt;p&gt;它的算法步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 首先抽取部分数据集，使用K-Means算法构建出K个聚簇点的模型
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;not&lt;/span&gt; EndCondition:
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;1&lt;/span&gt;.抽取训练数据集中的部分数据集样本数据，并将其添加到模型中，分配给距离最近的聚簇中心点
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;2&lt;/span&gt;.更新聚簇的中心点值
&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# EndCondtion同K-Means一样，可以理解为不停的进行K-Means算法。
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;聚类算法衡量标准&lt;/h2&gt;

&lt;p&gt;聚类算法的衡量标准有很多，包括均一性、完整性、V-measure、调整兰德系数（ARI ，Adjusted Rnd Index）、调整互信息(AMI，Adjusted Mutual Information)以及轮廓系数等等。&lt;/p&gt;

&lt;h3&gt;均一性、完整性以及V-measure&lt;/h3&gt;

&lt;p&gt;均一性：一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率，即每个聚簇中正确分类的样本数占该聚簇总样本数的比例和。其公式如下公式5.1。&lt;/p&gt;

$$
p = \frac{1}{k}\sum_{i=1}^k \frac{N(C_i == K_i)}{N(K_i)}\ \ \ 公式5.1  
$$
&lt;p&gt;完整性：同类别样本被归类到相同簇中，则满足完整性。每个聚簇中正确分类的样本数占该类型的总样本数比例的和，通俗的来说就是，我们已分类类别中，分类正确的个数。&lt;/p&gt;

&lt;p&gt;其公式如下，公式5.2：&lt;/p&gt;

$$
r = \frac{1}{k}\sum_{i=1}^k \frac{N(C_i == K_i)}{N(C_i)}\ \ \ 公式5.2 
$$
&lt;p&gt;在实际的情况中，均一性和完整性是往往不能兼得的，就好像抓特务时的矛盾一样，到底是保证每个抓的人都是特务，还是宁可错抓也不放过一个特务，之间的取舍很难把握。所以再一次贯彻，鱼和熊掌不可兼得，我们就加权，于是得到的就是V-measure，其公式如下公式5.3：&lt;/p&gt;

$$
V_\beta = \frac{(1+\beta^2)·pr}{\beta^2·p + r}\ \ \ 公式5.3  
$$
&lt;h3&gt;调整蓝德系数ARI&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;兰德系数（RI，Rand index）&lt;/strong&gt;，我用中文看了不少讲兰德系数的博客，其中的文字说明几乎都是相同的，对个人的理解帮助不是特别大，于是用英文查的。最终理解了这个系数的参数的意思，想看英文说明的，个人觉得还挺好懂的参考&lt;a href=&quot;https://en.wikipedia.org/wiki/Rand_index&quot;&gt;这里&lt;/a&gt;。以下是我个人的讲解。&lt;/p&gt;

&lt;p&gt;首先，将原数据集中的元素进行两两配对形成一个新的数据集，我们称之为S数据集。这时候，我们将原数据集，根据两种不同的策略分别划分成r份和s份，并对这两个数据集命名为X和Y。在这里我们可以看出，X和Y的元素是相同的，只是他们的划分方式不同。&lt;/p&gt;

&lt;p&gt;接下来我们来思考，S数据集中，每个元素中的两个样本，在X和Y中只有两种可能，就是两个样本都在一个子集中，或者不在一个子集中，那么对于S中的一个元素，只有四种可能性。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;两个样本都在X的一个子集中，也同时在Y的一个子集中，这些元素的个数是a&lt;/li&gt;
	&lt;li&gt;两个样本横跨X的不同子集，也同时在Y中横跨Y的不同子集，这些元素的个数是b&lt;/li&gt;
	&lt;li&gt;两个样本都在X的一个子集中，但在Y中横跨Y的不同子集，同理数量为c&lt;/li&gt;
	&lt;li&gt;两个样本横跨X的不同子集，但在Y的的一个子集中，同理数量为d&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;有了上述的理解，我们再看蓝得系数公式，公式5.4，我们就不难理解了。RI的取值在$[0,1]$之间，越靠近1代表越相似。&lt;/p&gt;

$$
RI = \frac{a+b}{a+b+c+d} = \frac{a+b}{C_2^n} \ \ \ 公式5.4  
$$
&lt;p&gt;接下来引入，&lt;strong&gt;调整兰德系数(ARI，Adjusted Rnd Index)&lt;/strong&gt;，ARI取值范围$[-1,1]$，值越大，表示聚类结果和真实情况越吻合。从广义的角度来将，ARI是衡量两个数据分布的吻合程度的，公式5.5如下：&lt;/p&gt;

$$
ARI = \frac{RI - E[RI]}{max(RI) - E[RI]}\ \ \ 公式5.5
$$
&lt;h3&gt;调整互信息(AMI，Adjusted Mutual Information)&lt;/h3&gt;

&lt;p&gt;调整互信息，整体的流程很像ARI，AMI则是对MI进行调整。而MI是使用信息熵来描述的。那么互信息表示了什么呢，首先先看下&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF&quot;&gt;维基百科的定义&lt;/a&gt;：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;独立的(H(X),H(Y)), 联合的(H(X,Y)), 以及一对带有互信息 I(X; Y) 的相互关联的子系统 X,Y 的条件熵。在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息是点间互信息（PMI）的期望值。互信息最常用的单位是bit。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;简单的来说，这个公式代表着两个子系统X和Y的相似度，但是这里的相似度是从信息熵的角度出发的，&lt;strong&gt;它越大代表着两者的差异越大&lt;/strong&gt;，其计算公式以及相关的公式如下公式5.6，公式5.7所示。&lt;/p&gt;

$$
MI(X;Y) = \sum_{y \in Y}\sum_{x \in X}p(x,y)\log{\frac{p(x,y)}{p(x)p(y)}}\ \ \ 公式5.6
$$
$$
\begin{split}
MI(X;Y) &amp;= H(X) - H(X|Y)\\
&amp;=H(Y) - H(Y|X)\\
&amp;=H(X) + H(Y) - H(X,Y)\\
&amp;=H(X,Y) - H(X|Y) - H(Y|X) \ \ \ 公式5.7
\end{split}
$$
&lt;h3&gt;轮廓系数&lt;/h3&gt;

&lt;p&gt;之前我们说到的衡量指标都是有标签的，这里的轮廓系数则是不包含标签的评价指标。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;簇内不相似度：&lt;/strong&gt;

		&lt;p&gt;计算样本i到同簇其它样本的平均距离为$a_i$ 。$a_i$越小，表示样本$i$越应该被聚类到该簇，而簇C中的所有样本的$a_i$的均值被称为簇C的簇不相似度。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;簇间不相似度：&lt;/strong&gt;

		&lt;p&gt;计算样本i到其它簇$C_j$ 的所有样本的平均距离bij， $b_i=min{bi1,bi2,...,bik}$ ，$b_i$ 越大，表示该样本$i$越不属于其它簇。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;轮廓系数：&lt;/strong&gt;

		&lt;p&gt;$s_i$值越接近1表示样本i聚类越合理，越接近-1，表示样本i应该分类到另外的簇中，近似为0，表示样本i应该在边界上。所有样本的si的均值被成为聚类结果的轮廓系数。&lt;/p&gt;

		$$
		s_i = \frac{b_i - a_i}{max\{a_i,b_i\}}
		$$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">聚类算法（上） 聚类算法很多，所以和讲回归算法一样，分成了上下，上中主要讲了传统的K-Means算法以及其相应的优化算法入K-Means++，K-Means||和Canopy等。下中主要讲了另外两种的思路的聚类算法，即层次聚类和密度聚类。 什么是聚类 聚类算就是怼大量未知标注的数据集，按照数据内部存在的数据特征将数据集划分为多个不同的类别，使类别内的数据比较相似，类别之间的数据相似度比较小，属于无监督学习。 从定义就可以看出，聚类算法的关键在于计算样本之间的相似度，也称为样本间的距离。 相似度/距离计算公式 说到聚类算法，那肯定核心就是计算距离的公式了，目前常用的有以下几种。 闵可夫斯基距离（Minkowski）：公式2.1</summary></entry><entry><title type="html">BaiduPCS-Go的使用</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/17/BaiduPCS-Go%E7%9A%84%E4%BD%BF%E7%94%A8/" rel="alternate" type="text/html" title="BaiduPCS-Go的使用" /><published>2018-08-17T00:00:00+08:00</published><updated>2018-08-17T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/17/BaiduPCS-Go%E7%9A%84%E4%BD%BF%E7%94%A8</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/17/BaiduPCS-Go%E7%9A%84%E4%BD%BF%E7%94%A8/">&lt;h2 id=&quot;why-baidupcs-go&quot;&gt;Why BaiduPCS-Go&lt;/h2&gt;
&lt;p&gt;BaiduPCS-Go是一个用Go语言编的命令行版的百度网盘，我们可以类比mas和Appstore的关系。那么为什么要用这样一个安装比较麻烦，还要记命令行的百度网盘的替代品，直接用百度网盘客户端不好么？
这还真的是不好，百度网盘在mac下是一个十足的阉割版，最常用的功能中，Mac版缺失了以下几种功能：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;没有分享功能&lt;/strong&gt;：mac下的客户端的分享功能居然是需要通过浏览器打开，太不优雅了。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;没有离线下载任务&lt;/strong&gt;：直接导致不能下载磁力链接。
如果你和我一样平时一样习惯终端操作，这个工具的学习成本超级低，同时它还有一定的提升下载速度的功效。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;使用指南&quot;&gt;使用指南&lt;/h2&gt;
&lt;h3 id=&quot;安装&quot;&gt;安装&lt;/h3&gt;
&lt;p&gt;Mac一般是预装了go的，如果没有的话，使用&lt;code class=&quot;highlighter-rouge&quot;&gt;brew install go&lt;/code&gt;来安装。除了go我们还需要安装git，同样使用&lt;code class=&quot;highlighter-rouge&quot;&gt;brew install git&lt;/code&gt;。在拥有了git和go以后，执行下面的指令即可。&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go get -u -v github.com/iikira/BaiduPCS-Go
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;注：在安装途中，有提示说其安装到了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;~/go/bin&lt;/code&gt;的目录，也就是说这个工具的执行文件在&lt;code class=&quot;highlighter-rouge&quot;&gt;~/go/bin&lt;/code&gt;这个目录。
为了之后我们能够全局使用这个指令，于是我们将&lt;code class=&quot;highlighter-rouge&quot;&gt;export PATH=&quot;/Users/deamov/go/bin:$PATH&quot;&lt;/code&gt;添加到配置环境变量的文件中，如果没有使用zsh的话在&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;中，如果用的是zsh的话在&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.zshrc&lt;/code&gt;中。
&lt;em&gt;注：deamov是我的电脑的用户名，至此安装便结束了。&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;常用操作说明&quot;&gt;常用操作说明&lt;/h3&gt;
&lt;h4 id=&quot;登陆&quot;&gt;登陆&lt;/h4&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BaiduPCS-Go
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;简单一行指令就可以登录了，如果之前已经登陆过账号的话，现在就已经可以开始进行下载等操作了，如下效果图。
 &lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fucuk0k8hnj310c0lojst.jpg&quot; alt=&quot;&quot; /&gt;
第一次使用需要有登陆的操作，输入&lt;code class=&quot;highlighter-rouge&quot;&gt;login&lt;/code&gt;即可登陆，尊许提示依次输入账户和密码即可，如果需要验证码，则会输出一个链接，打开就可以看到验证码了。&lt;/p&gt;
&lt;h4 id=&quot;基本操作&quot;&gt;基本操作&lt;/h4&gt;
&lt;p&gt;基本的移动目录的方式和linux的操作一样，&lt;code class=&quot;highlighter-rouge&quot;&gt;ls&lt;/code&gt;是现实当前目录的文件，&lt;code class=&quot;highlighter-rouge&quot;&gt;rm&lt;/code&gt;是删除命令，&lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt;是切换目录，创建目录是&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir&lt;/code&gt;，拷贝是&lt;code class=&quot;highlighter-rouge&quot;&gt;cp&lt;/code&gt;，值得一提的是它支持Tab补全。和平时使用的终端命令不同的有如下几个指令。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;搜索：&lt;/strong&gt;平时我们使用的&lt;code class=&quot;highlighter-rouge&quot;&gt;grep&lt;/code&gt;在这里是不能使用的，我们用&lt;code class=&quot;highlighter-rouge&quot;&gt;search&lt;/code&gt;关键词来搜索。
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  search 关键字 &lt;span class=&quot;c&quot;&gt;# 搜索当前工作目录的文件&lt;/span&gt;
  search -path&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/ 关键字 &lt;span class=&quot;c&quot;&gt;# 搜索根目录的文件&lt;/span&gt;
  search -r 关键字	&lt;span class=&quot;c&quot;&gt;# 递归搜索当前工作目录的文件 &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;下载：&lt;/strong&gt;记住是download就好啦
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  BaiduPCS-Go download &amp;lt;网盘文件或目录的路径1&amp;gt; 
  BaiduPCS-Go d &amp;lt;网盘文件或目录的路径1&amp;gt; &amp;lt;文件或目录2&amp;gt; &amp;lt;文件或目录3&amp;gt; ...
  &lt;span class=&quot;c&quot;&gt;# 当然支持多文件下载咯，下载目录默认在~/Download文件夹中&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;离线下载: &lt;/strong&gt;支持http/https/ftp/电驴/磁力链协议
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;c&quot;&gt;# 将百度和腾讯主页, 离线下载到根目录 /&lt;/span&gt;
  offlinedl add -path&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/ http://baidu.com http://qq.com

  &lt;span class=&quot;c&quot;&gt;# 添加磁力链接任务&lt;/span&gt;
  offlinedl add magnet:?xt&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;urn:btih:xxx

  &lt;span class=&quot;c&quot;&gt;# 查询任务ID为 12345 的离线下载任务状态&lt;/span&gt;
  offlinedl query 12345

  &lt;span class=&quot;c&quot;&gt;# 取消任务ID为 12345 的离线下载任务&lt;/span&gt;
  offlinedl cancel 12345 
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;h4 id=&quot;分享share&quot;&gt;分享share&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;查看分享内容&lt;/strong&gt;
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  share list
  share l
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;取消分享&lt;/strong&gt;
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  share cancel &amp;lt;shareid_1&amp;gt;
  share c &amp;lt;shareid_1&amp;gt;
  &lt;span class=&quot;c&quot;&gt;# 遗憾的是只能支持通过shareid来取消分享&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;h4 id=&quot;上传同名文件会被覆盖&quot;&gt;上传：同名文件会被覆盖&lt;/h4&gt;
    &lt;p&gt;&lt;em&gt;注：需要退出BaiduPCS-Go使用，否则本地文件目录不能自动补全&lt;/em&gt;&lt;/p&gt;
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$BaiduPCS&lt;/span&gt;-Go upload &amp;lt;本地文件/目录的路径1&amp;gt; &amp;lt;文件/目录2&amp;gt; &amp;lt;文件/目录3&amp;gt; ... &amp;lt;目标目录&amp;gt;
&lt;span class=&quot;nv&quot;&gt;$BaiduPCS&lt;/span&gt;-Go u &amp;lt;本地文件/目录的路径1&amp;gt; &amp;lt;文件/目录2&amp;gt; &amp;lt;文件/目录3&amp;gt; ... &amp;lt;目标目录&amp;gt;
&lt;span class=&quot;c&quot;&gt;# Example&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$BaiduPCS&lt;/span&gt;-Go upload ~/Downloads/1.mp4 /Video
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;h3 id=&quot;其他&quot;&gt;其他&lt;/h3&gt;
    &lt;p&gt;这个工具很强大，还可以通过设置下载线程数等等操作来提升下载速度，更多详细的操作请参考它的&lt;a href=&quot;https://github.com/iikira/BaiduPCS-Go&quot;&gt;官网&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>DeamoV</name></author><category term="百度网盘" /><category term="下载" /><category term="教程" /><summary type="html">Why BaiduPCS-Go BaiduPCS-Go是一个用Go语言编的命令行版的百度网盘，我们可以类比mas和Appstore的关系。那么为什么要用这样一个安装比较麻烦，还要记命令行的百度网盘的替代品，直接用百度网盘客户端不好么？ 这还真的是不好，百度网盘在mac下是一个十足的阉割版，最常用的功能中，Mac版缺失了以下几种功能： 没有分享功能：mac下的客户端的分享功能居然是需要通过浏览器打开，太不优雅了。 没有离线下载任务：直接导致不能下载磁力链接。 如果你和我一样平时一样习惯终端操作，这个工具的学习成本超级低，同时它还有一定的提升下载速度的功效。</summary></entry><entry><title type="html">Tobias的小粉丝在此</title><link href="http://localhost:4000/%E6%9D%82%E9%A1%B9/2018/08/16/Tobias%E5%B0%8F%E8%BF%B7%E5%BC%9F%E5%9C%A8%E6%AD%A4/" rel="alternate" type="text/html" title="Tobias的小粉丝在此" /><published>2018-08-16T00:00:00+08:00</published><updated>2018-08-16T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9D%82%E9%A1%B9/2018/08/16/Tobias%E5%B0%8F%E8%BF%B7%E5%BC%9F%E5%9C%A8%E6%AD%A4</id><content type="html" xml:base="http://localhost:4000/%E6%9D%82%E9%A1%B9/2018/08/16/Tobias%E5%B0%8F%E8%BF%B7%E5%BC%9F%E5%9C%A8%E6%AD%A4/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;Tobias的小粉丝在此&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;序言&lt;/h2&gt;
&lt;p&gt;最近迷上了吉他，当然不是指那种一周速成的把妹弹唱啦。为了防止大家对吉他有一种特别简单，把妹专用道具的奇怪印象。特别提一个小知识，古典吉他在世界公认的十大难学的乐器中排第三，顺便一提，钢琴排第五。&lt;/p&gt;

&lt;h2&gt;正文&lt;/h2&gt;

&lt;p&gt;不过，非常遗憾最近喜欢上的是并不是尼龙弦的古典吉他，而是它的另一个兄弟指弹吉他中的Percussive Fingerstyle。Percussive FingerStyle起源于80到90年代，它通过快速击打琴弦，琴身，以及琴的边缘制造装饰音。现在比较有名的几个这个风格的吉他手有Andy McKee和Tommy Emmanuel，而Tobias Rauscher也是这个领域的新秀，也是目前我最喜欢的吉他手之一。值得一提的是，他是14岁开始自学的吉他，开始的时候主要弹奏摇滚和重金属，知道2010年终于开始了Solo Acoustic Guitar的道路。和相似的同时用多个吉他，甚至奇怪形状吉他的Luca Stricagnoli不同，他只使用一把吉他，视觉效果上没有了那份笨重，同时Tobias的笑容也能让人感受到他对于吉他的热爱。有兴趣的小伙伴可以在网易云搜他的曲子。惭愧的说，第一次听他的曲子也是在网易云里面听到了，结果整整单曲循环了好几天（笑。&lt;/p&gt;

&lt;h2&gt;其他&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;www.tobiasguitar.com&quot;&gt;Tobias的官网在此&lt;/a&gt;&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="FingerStyle" /><category term="Guitar" /><summary type="html">Tobias的小粉丝在此 序言 最近迷上了吉他，当然不是指那种一周速成的把妹弹唱啦。为了防止大家对吉他有一种特别简单，把妹专用道具的奇怪印象。特别提一个小知识，古典吉他在世界公认的十大难学的乐器中排第三，顺便一提，钢琴排第五。</summary></entry><entry><title type="html">支持向量机SVM</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/" rel="alternate" type="text/html" title="支持向量机SVM" /><published>2018-08-13T00:00:00+08:00</published><updated>2018-08-13T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;支持向量机（Support Vector Machine）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;SVM的优点和缺点&lt;/h2&gt;
&lt;ul&gt;
	&lt;li&gt;优点：泛化错误率低，计算开销不大，结果易解释&lt;/li&gt;
	&lt;li&gt;缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。&lt;/li&gt;
	&lt;li&gt;数据类型：数值型和标称型数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SVM算法原理&lt;/h2&gt;

&lt;p&gt;首先我们都知道，为了划分二维的数据需要一根线，划分三维数据需要一个面。这里线是一维，面是二维，同理继续推出对N维的数据需要使用N-1维的对象进行分割，线性回归和SVM本质都是通过找这个超平面来达到分类的效果。&lt;/p&gt;

&lt;p&gt;具体的来说SVM是在优化线性回归中的$kx+b$模型。在线性回归中只需要考虑有一个分割超平面能进行分类即可，而SVM则想找出所有能分类的分割超平面中最优的超平面，即&lt;strong&gt;所有点都到分割超平面的距离最大&lt;/strong&gt;，而支持向量指的就是离超平面最近的那些点。&lt;/p&gt;

&lt;p&gt;超平面的公式为公式2.1。所以点A到分割超平面的距离为公式2.2。这里我们为了方便计算引入类别标签为-1和+1。所以保证所有的最小间隔的点最大化的公式为公式2.3。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1：-1和+1是为了保证预测正确的时候，$y(x_i)*label_i$都是一个很大的正值。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2：$arg\ max_{w,b}$的含义是，得到w和b使得后面式子取最大值&lt;/em&gt;&lt;/p&gt;

$$
y(x) = w^TX+b\ \ \ 公式2.1
$$

$$
\frac{|w^TX+b|}{||w||}\ \ \ 公式2.2
$$

$$
arg\ max_{w,b}\{min_i(label_i*(w^Tx_i+b)*\frac{1}{||w||})\}\ \ \ 公式2.3
$$
&lt;p&gt;显然我们不能直接求解上面的式子。需要化简下它。由于公式2.1在正确预测时，同$label$的乘积大于1。所以我们可以拆分公式2.3为公式2.4，约束条件为公式2.5。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：这里的约束条件公式2.5中，要对每一个式子前都要加系数，即拉格朗日数乘子$\alpha_i$&lt;/em&gt;。&lt;/p&gt;

$$
arg\ min_{w,b}\ ||w||\ \ \ 公式2.4
$$
$$
st.\ \ label_i*(w^Tx_i+b) \geq 1 \ \ \ 公式2.5
$$
&lt;p&gt;对为了方便求导计算在公式2.4前加上$\frac{1}{2}$这个系数。之后使用拉格朗日乘子法得到公式2.6，并进行计算。根据KKT条件中的偏导数等于0得到公式2.7和公式2.8。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;祝：这里需要注意的是拉格朗日数乘子的正负号，这个同不等式的符号有关&lt;/em&gt;&lt;/p&gt;

$$
L(w,b,\alpha)= \frac{1}{2}||w||^2-\sum_{i=1}^n\alpha_i*[label_i*(w^Tx_i+b)-1]\ \ \ 公式2.6
$$
$$
\sum_{i=1}^{n}\alpha_i label_i x_i = w\ \ \ 公式2.7
$$
$$
\sum_{i=1}^{n}\alpha_i label_i = 0\ \ \ 公式2.8
$$
&lt;p&gt;将公式2.7，公式2.8代入公式2.6化简，再根据对偶问题得到最终公式2.9，根据KKT，其约束条件为公式2.10。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1：KKT条件在SMO算法中统一进行讲解。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2：b是由公式2.8消掉的。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注3：在拉格朗日乘子法应用在这里，我们可以把$||w||$，写作$max_\alpha\ L(w,b,\alpha)$，所以原式可以写作$min_{w,b}\ max_\alpha\ L(w,b,\alpha)$，根据对偶问题，就可以变成$max_\alpha\ min_{w,b}\ L(w,b,\alpha)$，这也是能把公式2.7和公式2.8代入公式2.6的原因，也是公式2.9种是$max_\alpha$的原因。具体证明在KKT中的附上的博客中。&lt;/em&gt;&lt;/p&gt;

$$
max_\alpha\ \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{m}label_i*label_j*a_i*a_j\langle x_i·x_j\rangle\ \ \ 公式2.9
$$

$$
\alpha_i \geq 0\ \  且\ \sum_{i=1}^{m}\alpha_i*label_i = 0\ \ \ 公式2.10
$$
&lt;p&gt;&lt;em&gt;注：这里$\langle x_i·x_j\rangle$是两者向量积的运算，是从$x_i^T*x_j$得到的。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;这么看来我们算出了$\alpha$就能算出超平面，所以SVM的工作就是算出这些$\alpha$，而SMO算法就是求$\alpha$的典型算法。&lt;/p&gt;

&lt;h2&gt;对SVM引入线性不可分&lt;/h2&gt;

&lt;p&gt;由于数据都不那么干净，所以我们不应该假设数据能够100%的线性可分。我们通过对判定的公式，公式2.5，引入松弛变量$\xi_i\geq 0$，得到其引入了松弛因子的形式，如下公式3.1。&lt;/p&gt;

$$
y_i(w*x_i+b)\geq1-\xi_i\ \ \ 公式3.1
$$
&lt;p&gt;同时对于目标函数公式2.4也需要调整，我们将$\xi$引入目标函数并对其设置权值，得到公式3.2，也因此其约束条件变为公式3.1，公式3.2。&lt;/p&gt;

$$
min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i\ \ \ 公式3.2
$$
$$
\begin{split}
st.\ \ \ &amp;y_i(w*x_i+b)\geq 1 - \xi_i\\
&amp;\xi \geq 0
\end{split}
$$
&lt;p&gt;故拉格朗日函数$L(w,b,\xi,\alpha,\mu)$为如下公式3.3，其中$\alpha$，$\mu$为拉格朗日数乘子。&lt;/p&gt;

$$
L(w,b,\xi,\alpha,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^n\alpha_i*[label_i*(w^Tx_i+b)-1+\xi_i]-\sum_{i=1}^n\mu_i\xi_i\ \ \ 公式3.3
$$
&lt;p&gt;和之前的操作一样，对其进行求偏导操作后，类似的得到了相同的公式2.7，公式2.8，不同的是这里对$\xi$的求到后对$\alpha$有了限制，得到了公式3.4，由于$\mu\geq0$所以有$\alpha_i$的取值范围$0 \leq \alpha_i \leq C$。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：注意这里的$\alpha$取值，之后SMO会用&lt;/em&gt;&lt;/p&gt;

$$
C-\alpha_i-\mu_i = 0\ \ \ 公式3.4  
$$
&lt;p&gt;最终目标函数还是同之前推到的相同，即公式2.9，约束条件中只有$\alpha$的取值变为了，$0 \leq \alpha_i \leq C$。这样有了目标函数了以后，之后可以根据梯度下降算法求得最终的$\alpha$&lt;/p&gt;

&lt;h2&gt;SMO（Sequential Minimal Optimization）&lt;/h2&gt;

&lt;h3&gt;KKT条件 &lt;/h3&gt;

&lt;p&gt;求$f(x)$ 极值的时候我们这里讨论三种情况。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;没有约束条件：&lt;/li&gt;
	&lt;li&gt;有一个等式$h(x)$的约束条件：

		&lt;p&gt;使用拉格朗日乘子法（Lagrange Multiplier），也就是我们在高数中求极值常用的。设置一个拉格朗日系数$\alpha_1$，得到如下公式，之后对$x$和$\alpha_1$用求导的方式求极值即可。&lt;/p&gt;

		$$
		L(x, \alpha) = f(x) + \alpha*h(x)
		$$&lt;/li&gt;
	&lt;li&gt;含有不等式的约束条件：

		&lt;p&gt;当约束条件中有不等式时，就需要用到KKT条件。同样地，把所有的不等式约束$g(x)\leq0$、等式约束$h(x)=0$和目标函数$f(x)$全部写为一个式子如下公式。&lt;/p&gt;

		$$
		L(x,\alpha_1, \alpha_2)= f(x) + \alpha_1*g(x)+\alpha_2*h(x)
		$$
		&lt;p&gt;KKT条件是说最优值必须满足以下条件：&lt;/p&gt;

		&lt;ol&gt;
			&lt;li&gt;$L(x, \alpha) = f(x) + \alpha(x)$ 对$x$，$\alpha_1$，$\alpha_2$求导为零。&lt;/li&gt;
			&lt;li&gt;$h(x)=0$ 。&lt;/li&gt;
			&lt;li&gt;$g(x)*\alpha_1=0$。&lt;/li&gt;
		&lt;/ol&gt;

		&lt;p&gt;其中第三个式子非常有趣，因为$g(x)\leq$ 0 ，&lt;strong&gt;如果要满足这个等式，必须有$a = 0$或者$g(x) = 0$&lt;/strong&gt;。这是SVM的很多重要性质的来源。同时$f(x)$也可以写作$max_{\alpha_1,\alpha_2}\ L(x,\alpha_1,\alpha_2)$，这个则是SMO求解中的一个关键性质。详细的论述参考&lt;a href=&quot;https://blog.csdn.net/xianlingmao/article/details/7919597&quot;&gt;这篇博客&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;SMO算法细节&lt;/h3&gt;

&lt;h4&gt;SMO算法综述&lt;/h4&gt;

&lt;p&gt;由于原来直接通过梯度下降进行求解速度太慢，所以1996年，John Platt依靠KKT的特性，将大优化问题变成了多个小优化问题来求解，成为了SVM中最常用的求解思路。&lt;/p&gt;

&lt;p&gt;其思路如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Loop：

		&lt;ol&gt;
			&lt;li&gt;选取一对 $\alpha_i$，$\alpha_j$作为变量，其余看为常数&lt;/li&gt;
			&lt;li&gt;如果这对$\alpha$满足以下两个条件，使用梯度下降算法改变他们的值。

				&lt;ul&gt;
					&lt;li&gt;两者都在间隔边界外&lt;/li&gt;
					&lt;li&gt;两者都没有在进行过区间化处理，或者不在边界上&lt;/li&gt;
				&lt;/ul&gt;&lt;/li&gt;
			&lt;li&gt;当满足了KKT条件，即$\sum_{i=1}^N\alpha_iy_i=0$和$0\leq \alpha_i \leq C$。&lt;/li&gt;
		&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注：这里可以这么理解，$\alpha_i$从之前的公式中我们可以大致理解为是每一个样本的权值，我们这些操作可以理解为通过操作$\alpha$把所有的样本点尽量的放在间隔边界上。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;算法推导&lt;/h4&gt;

&lt;p&gt;我们接下来要做的是，通过类似梯度下降的方式来求的最优的$\alpha$值。正如上一节所说的，SMO的本质是大优化问题画小优化问题。所以从目标函数公式2.9中，随意取出两个$\alpha$，为了表达方便，不妨直接取$\alpha_1$和$\alpha_2$，同时对公式2.9前加负号取反之后，化简如下式4.1，其中$\kappa_{ij}$代表$\langle x_i·x_j\rangle$。&lt;/p&gt;

$$
\begin{split}
min_{\alpha_1, \alpha_2}W(\alpha_1,\alpha_2) &amp;=\frac{1}{2}\kappa_{11}\alpha_1^2+\frac{1}{2}\kappa_{22}\alpha_2^2+y_1y_2\alpha_1\alpha_2\kappa_{12}-(\alpha_1+\alpha_2)\\
&amp;+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_i\kappa_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_i\kappa_{i2}\ \ \ 公式4.1\\
\end{split} 
$$
$$
\begin{split}
st. \ \ &amp;\alpha_1y_1+\alpha_2y_2=-\sum_{i=3}^Ny_i\alpha_i\\
&amp;0\leq\alpha_i\leq C
\end{split}
$$
&lt;p&gt;由于我们已经把不是$\alpha_1$和$\alpha_2$的参数看作常量，所以在进行求偏导进行梯度下降算法的时候就不需要考虑公式4.1中第二行的式子。通过这个式子中约束条件的等式就可以得到仅含$\alpha_j$的式子，对其进行梯度下降算法，得到如下公式4.2：&lt;/p&gt;

$$
g(x)=\sum_{i=1}^Ny_i\alpha_i\kappa(x_i,x)+b  
$$
$$
\eta = \kappa_{11}+\kappa_{22}-2\kappa_{12} = ||x_1-x2||^2
$$
$$
E_i = g(x_i)-y_i = (\sum_{j=1}^Ny_j\alpha_j\kappa_{ji}+b)-y_i
$$
$$
\alpha_i = \frac{\xi-\alpha_j y_j}{y_i}
$$
$$
\alpha_j^{new}=\alpha_j^{old}+\frac{y_j(E_i-E_j)}{\eta}\ \ \ 公式4.2
$$
&lt;p&gt;这时候我们已经找到了两个的$alpha$的新值了，不过我们不能确定这两个新值是否还满足KKT条件。所以我们根据KKT条件中$\alpha$的取值，设置了L和H来防止新值不满足KKT，即$L\leq\alpha_i,\alpha_j \leq H$，其中L，H的公式如下公式4.3和公式4.4得到：&lt;/p&gt;

$$
if\ y_i \neq  y_j\ \ \ L=max(0,\alpha_j-\alpha_i),\ H=min(C,C+\alpha_j-\alpha_i)
$$
$$
if\ y_i = y_j\ \ \ L=max(0,\alpha_j+\alpha_i-C),\ H=min(C,\alpha_j+\alpha_i)
$$
&lt;p&gt;LH的细节推到，在&lt;a href=&quot;https://blog.csdn.net/hjimce/article/details/45421827&quot;&gt;这个博客&lt;/a&gt;中详细的说明了LH是怎么推出来的。&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">支持向量机（Support Vector Machine） SVM的优点和缺点 优点：泛化错误率低，计算开销不大，结果易解释 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。 数据类型：数值型和标称型数据。</summary></entry><entry><title type="html">使用Github创建自己的小博客</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/12/%E4%BD%BF%E7%94%A8Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E5%8D%9A%E5%AE%A2/" rel="alternate" type="text/html" title="使用Github创建自己的小博客" /><published>2018-08-12T00:00:00+08:00</published><updated>2018-08-12T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/12/%E4%BD%BF%E7%94%A8Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E5%8D%9A%E5%AE%A2</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/12/%E4%BD%BF%E7%94%A8Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E5%8D%9A%E5%AE%A2/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;使用Github创建自己的小博客&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;懒人攻略&lt;/h2&gt;
&lt;p&gt;只有四步：&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;找到自己喜欢的别人的博客的Github地址，一般为&lt;code&gt;username.github.io&lt;/code&gt;结尾。&lt;/li&gt;
	&lt;li&gt;Fork一份对方的源码，之后把仓库名改为&lt;code&gt;YourGithubName.github.io&lt;/code&gt;&lt;/li&gt;
	&lt;li&gt;在&lt;code&gt;_config.yaml&lt;/code&gt;中更改个人信息，同时把&lt;code&gt;_posts&lt;/code&gt;中的文章都删了，注意别人的文章格式，之后仿照对方的格式写即可。&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;给你Fork的原作者写封邮件表达感谢！&lt;/strong&gt;说不定就这么勾搭了一个大佬也不一定呢。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;完成了四步后，浏览器输入&lt;code&gt;YourGithubName.github.io&lt;/code&gt;就能在晚上看到自己的博客啦。&lt;/p&gt;

&lt;h2&gt;折腾攻略&lt;/h2&gt;

&lt;p&gt;本这不重新造轮子的原则，附上我参考的大佬们的文章。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;搭建篇：

		&lt;p&gt;简书上chaosinmotion 的&lt;a href=&quot;https://www.jianshu.com/p/7593508666f8&quot;&gt; Github Pages + Jekyll 独立博客一小时快速搭建&amp;amp;上线指南 &lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;添加评论系统：

		&lt;p&gt;Github上knightcai的&lt;a href=&quot;https://knightcai.github.io/2017/12/19/%E4%B8%BA%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0-Gitalk-%E8%AF%84%E8%AE%BA%E6%8F%92%E4%BB%B6/&quot;&gt; 为博客添加 Gitalk 评论插件 &lt;/a&gt;&lt;/p&gt;

		&lt;p&gt;特别一提，如果出现Validation Error是因为博客标题的名字编码后太长了，参考&lt;a href=&quot;https://github.com/gitalk/gitalk/issues/102&quot;&gt;这个Issue&lt;/a&gt;中mr-wind的使用 &lt;code&gt;id: decodeURI(location.pathname)&lt;/code&gt; 解决方案。&lt;/p&gt;

		&lt;p&gt;&lt;strong&gt;注：md5的方案可能更好，偷懒起见我没有用。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;阅读量统计：

		&lt;p&gt;wanghao的&lt;a href=&quot;https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud&quot;&gt; 为NexT主题添加文章阅读量统计功能 &lt;/a&gt;，这个文章用的是leandCloud。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;搜索服务：

		&lt;p&gt;使用Algolia，不过自带的LocalSearch比较简单。&lt;a href=&quot;http://theme-next.iissnan.com/third-party-services.html#algolia-search&quot;&gt;文章&lt;/a&gt;有配置说明。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;主题：

		&lt;p&gt;Next系列。&lt;a href=&quot;http://theme-next.iissnan.com/getting-started.html&quot;&gt;官网&lt;/a&gt;有安装手册。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;CopyRight:

		&lt;p&gt;在目录下搜索copyright，找到那个html文件进行修改就好了。效果是文章下面的红竖杠中的内容。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;小彩蛋：

		&lt;p&gt;史蒂芬小恐龙，他的js文件在&lt;a href=&quot;https://github.com/lmk123/t-rex-runner&quot;&gt;这里&lt;/a&gt;！之后就任君发挥啦，Happy Coding。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最后题外话

		&lt;p&gt;所有的配置基本上都可以在&lt;code&gt;_config.yaml&lt;/code&gt;中设置，同时在博客中&lt;code&gt;\&lt;/code&gt;代表的就是根目录，这样子你自己在配置其他的功能的时候就可以轻松愉悦的配置。值得一提的是css文件和js文件都在&lt;code&gt;assets&lt;/code&gt;文件夹中，自己DIY的时候最好不要打乱目录结构。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="GithubPage" /><category term="博客" /><category term="教程" /><summary type="html">使用Github创建自己的小博客 懒人攻略 只有四步： 找到自己喜欢的别人的博客的Github地址，一般为username.github.io结尾。 Fork一份对方的源码，之后把仓库名改为YourGithubName.github.io 在_config.yaml中更改个人信息，同时把_posts中的文章都删了，注意别人的文章格式，之后仿照对方的格式写即可。 给你Fork的原作者写封邮件表达感谢！说不定就这么勾搭了一个大佬也不一定呢。</summary></entry><entry><title type="html">决策树优化策略</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/10/%E5%86%B3%E7%AD%96%E6%A0%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/" rel="alternate" type="text/html" title="决策树优化策略" /><published>2018-08-10T00:00:00+08:00</published><updated>2018-08-10T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/10/%E5%86%B3%E7%AD%96%E6%A0%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/10/%E5%86%B3%E7%AD%96%E6%A0%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;决策树优化策略&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;剪枝优化&lt;/h2&gt;
&lt;p&gt;决策树的剪枝是决策树算法中最基本、最有用的一种优化方案，分为以下两类：&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;前置剪枝：&lt;/strong&gt;在构建决策树的过程中，提前停止。&lt;strong&gt;这种策略无法得到比较好的结果&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;后置剪枝：&lt;/strong&gt;在决策树构建好后，然后开始剪裁，一般使用两种方案。a）用单一叶子结点代替整个子树，也节点的分类采用子树中最主要的分类。b）将一个子树完全替代另一个子树。后置剪枝的主要问题是存在计算效率问题，存在一定的浪费情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;后置剪枝&lt;/h3&gt;

&lt;p&gt;后置剪枝的核心思想其实就是交叉验证，其通过对完全树进行剪枝，一直剪到只剩下树根，这样子便得到许多树，然后通过使用数据集分别对他们验证，然后根据结果选择最优树。&lt;/p&gt;

&lt;h3&gt;决策树剪枝过程&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; 生成的决策树不为1个节点:
	计算所有内部非叶子节点的剪枝系数;
	选择最小剪枝系数的节点:
		&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; 有多个最小剪枝系数节点:
			选择包含数据项多的节点删除
		&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;else&lt;/span&gt;:
			删除节点
		将剪枝后的树存入之后用的决策树集
&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;for&lt;/span&gt; 决策树 &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;in&lt;/span&gt; 决策树集:
	用数据集验证决策树，得到最优剪枝后的决策树
	&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其最后用于验证决策树的损失函数如下公式1.1：&lt;/p&gt;

$$
loss = \sum_{t=1}^{leaf} \frac{D_t}{D}H(t)\ \ \  公式1.1
$$
&lt;p&gt;剪枝系数的引入：&lt;/p&gt;

&lt;p&gt;剪枝系数的目的为，&lt;strong&gt;平衡准确度和树的节点数量之间的关系&lt;/strong&gt;。所以很自然的想到我们常用的处理手法，在损失函数中引入叶子结点的变量，得到公式1.2。&lt;/p&gt;

$$
loss_{\alpha} = loss + \alpha*leaf\ \ \ \ 公式1.2
$$
&lt;p&gt;设剪枝前的损失函数为$loss(R)$，剪枝后的损失函数为$loss(r)$，由于我们是想让剪枝前后的准确率尽量不变，所以让剪枝前后的损失函数相等，化简得公式1.3，即剪枝系数。&lt;strong&gt;注：剪枝后为根节点，所以$r=1$&lt;/strong&gt;。&lt;/p&gt;

$$
\alpha = \frac{loss(r)-loss(R)}{R_{leaf}-1}\ \ \ \ 	公式1.3
$$
&lt;p&gt;最后，由于我们想尽量减去的叶子结点多点，又同时保持准确度，故剪枝系数越小越好。&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">决策树优化策略 剪枝优化 决策树的剪枝是决策树算法中最基本、最有用的一种优化方案，分为以下两类： 前置剪枝：在构建决策树的过程中，提前停止。这种策略无法得到比较好的结果 后置剪枝：在决策树构建好后，然后开始剪裁，一般使用两种方案。a）用单一叶子结点代替整个子树，也节点的分类采用子树中最主要的分类。b）将一个子树完全替代另一个子树。后置剪枝的主要问题是存在计算效率问题，存在一定的浪费情况。</summary></entry><entry><title type="html">决策树</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/09/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="alternate" type="text/html" title="决策树" /><published>2018-08-09T00:00:00+08:00</published><updated>2018-08-09T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/09/%E5%86%B3%E7%AD%96%E6%A0%91</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/09/%E5%86%B3%E7%AD%96%E6%A0%91/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;决策树&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h3&gt;信息熵&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;信息量:&lt;/strong&gt;指的是一个样本/事件所蕴含的信息，如果一个事件的概率越大，那么就 可以认为该事件所蕴含的信息越少。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;信息熵:&lt;/strong&gt;一个系统越是有序， 信息熵就越低，一个系统越是混乱，信息熵就越高，所以 信息熵被认为是一个系统有序程度的度量。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;信息熵就是用来描述系统信息量的不确定度。&lt;/strong&gt;&lt;/p&gt;

$$
H(x) = - \sum_{i=1}^m p_i log_2(p_i)
$$
&lt;p&gt;&lt;strong&gt;条件熵：&lt;/strong&gt; 给定条件X的情况下，所有不同x值情况下Y的信息熵的平均值叫做条件熵。&lt;/p&gt;

$$
H(Y|X) = \sum_{j=1}P(X = v_j)H(Y|X = v_j)
$$
$$
H(Y|X) = H(X,Y)-H(X)
$$
&lt;h3&gt;决策树定义&lt;/h3&gt;

&lt;p&gt;决策树(Decision Tree)是在已知各种情况发生概率的基础上，通过构建决策树来 进行分析的一种方式，是一种直观应用概率分析的一种图解法。决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系。决策树是一种树形结构， 其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节 点代表一种类别;决策树是一种非常常用的有监督的分类算法。&lt;/p&gt;

&lt;h3&gt;构建过程&lt;/h3&gt;

&lt;p&gt;构建步骤如下:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;将所有的特征看成一个一个的节点;&lt;/li&gt;
	&lt;li&gt;遍历每个特征的每一种分割方式，找到最好的分割点;将数据划分为不同的子节点，eg: N1、 N2....Nm;计算划分之后所有子节点的&amp;#39;纯度&amp;#39;信息;&lt;/li&gt;
	&lt;li&gt;对第二步产生的分割，选择出最优的特征以及最优的划分方式;得出最终的子节点: N1、N2....Nm 4. 对子节点N1、N2....Nm分别继续执行2-3步，直到每个最终的子节点都足够&amp;#39;纯&amp;#39;。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;特征属性类型&lt;/h3&gt;

&lt;p&gt;根据特征属性的类型不同，在构建决策树的时候，采用不同的方式，具体如下: &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt; 属性是离散值，而且不要求生成的是二叉决策树，此时一个属性就是一个分支&lt;/li&gt;
	&lt;li&gt; 属性是离散值，而且要求生成的是二叉决策树，此时使用属性划分的子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支&lt;/li&gt;
	&lt;li&gt;属性是连续值，可以确定一个值作为分裂点split_point，按照&amp;gt;split_point和&amp;lt;=split_point生成两个分支&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;决策树分割属性选择&lt;/h3&gt;

&lt;ul&gt;
	&lt;li&gt;决策树算法是一种“贪心”算法策略，只考虑在当前数据特征情况下的最好分割方式，不能进行回溯操作。&lt;/li&gt;
	&lt;li&gt;对于整体的数据集而言，按照所有的特征属性进行划分操作，对所有划分操作的结果集的“纯度”进行比较，选择“纯度”越高的特征属性作为当前需要分割的数据集进行分割操作，持续迭代，直到得到最终结果。决策树是通过“纯度”来选择分割特征属性点的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;量化纯度&lt;/h3&gt;

&lt;p&gt;基尼系数：$Gini = 1 - \sum_{i=1}^nP(i)^2$&lt;/p&gt;

&lt;p&gt;信息熵：$-\sum_{i=1}^n P(i)log_2(P(i))$&lt;/p&gt;

&lt;p&gt;误差率：$Error = 1 - max_{i=1}^n \{P(i)\}$&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;上面是基础的一些系数，在剪枝和判断的时候需要一个种体现变化的系数。&lt;/p&gt;

$$
Gain = \Delta = H(D)-H(D|A)
$$
&lt;p&gt;可以这么理解，是否决策树以A为节点划分后，信息量增加了。也可以间接体现决策树越往下发生该事件的概率越小信息量越大。&lt;/p&gt;

&lt;h3&gt;停止条件&lt;/h3&gt;

&lt;p&gt;决策树构建的过程是以恶递归的过程，以下是两个停止条件。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;每个字节点只有一种类型时停止条件。（容易过拟合）&lt;/li&gt;
	&lt;li&gt;节点中记录数小于某个阀值的时候，或者迭代次数达到给定值的时候停止构建。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;决策树的评估&lt;/h3&gt;

$$
loss = \sum_{t=1}^{leaf} \frac{|D_t|}{D}H(t)
$$
&lt;p&gt;其中$H(t)$前的参数$\frac{D_t}{d}$主要的目的其实是给信息熵加权值。&lt;/p&gt;

&lt;h3&gt;算法对比&lt;/h3&gt;

&lt;h4&gt;ID3算法优缺点&lt;/h4&gt;

&lt;p&gt;ID3算法是决策树的一个经典的构造算法，内部使用&lt;strong&gt;信息熵&lt;/strong&gt;以及&lt;strong&gt;信息增益&lt;/strong&gt;来进行 构建;每次迭代选择信息增益最大的特征属性作为分割属性。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;决策树构建速度快，实现简单。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优&lt;/li&gt;
	&lt;li&gt;ID3算法不是递增算法 ID3算法是单变量决策树，对于特征属性之间的关系不会考虑 抗噪性差&lt;/li&gt;
	&lt;li&gt;只适合小规模数据集，需要将数据放到内存中&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;C4.5&lt;/h4&gt;

&lt;p&gt;在ID3算法的基础上，进行算法优化提出的一种算法(C4.5)。其使用&lt;strong&gt;信息增益率&lt;/strong&gt;来取代ID3算法中的信息增益，在树的构造过程中&lt;strong&gt;会进行剪枝操作进行优化&lt;/strong&gt;。能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。信息增益率公式如下：&lt;/p&gt;

$$
Gain\_ratio(A) = \frac{Gain(A)}{H(A)}
$$
&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;产生的规则易于理解&lt;/li&gt;
	&lt;li&gt;准确率较高实现简单&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;对数据集需要进行多次顺序扫描和排序，所以效率较低 &lt;/li&gt;
	&lt;li&gt;只适合小规模数据集，需要将数据放到内存中&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;CART&lt;/h4&gt;

&lt;p&gt;使用基尼系数作为数据纯度的量化指标来构建的决策树算法就叫做 CART(Classification And Regression Tree，分类回归树)算法。CART算法使用GINI增益作为分割属性选择的标准，选择GINI增益最大的作为当前数据集的分割属性。可用于分类和回归两类问题。&lt;strong&gt;注意的是：CART构建是二叉树，同时GINI系数的计算不牵扯对数运算比较快&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GINI增益公式如下：&lt;/p&gt;

$$
Gain = \Delta = Gini(D) - Gini(D|A)
$$
&lt;h4&gt;总结&lt;/h4&gt;

&lt;ul&gt;
	&lt;li&gt;ID3和C4.5算法均只适合在小规模数据集上使用&lt;/li&gt;
	&lt;li&gt;ID3和C4.5算法都是单变量决策树&lt;/li&gt;
	&lt;li&gt;当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差&lt;/li&gt;
	&lt;li&gt;决策树分类一般情况只适合小数据量的情况(数据可以放内存)&lt;/li&gt;
	&lt;li&gt;CART算法是三种算法中最常用的一种决策树构建算法。&lt;/li&gt;
	&lt;li&gt;CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;三种算法的区别仅仅只是对于当前树的评价标准不同而已，ID3使用信息增益、 C4.5使用信息增益率、CART使用基尼系数。&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">决策树 信息熵 信息量:指的是一个样本/事件所蕴含的信息，如果一个事件的概率越大，那么就 可以认为该事件所蕴含的信息越少。</summary></entry></feed>