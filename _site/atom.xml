<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-09-16T10:37:35+08:00</updated><id>http://localhost:4000/</id><title type="html">Pre-Demo-Field</title><subtitle>Coding Life Coding Fun</subtitle><author><name>DeamoV</name></author><entry><title type="html">关于设置YouCompleteMe Python3语法支持</title><link href="http://localhost:4000/troubleshoot/2018/09/16/%E5%85%B3%E4%BA%8E%E8%AE%BE%E7%BD%AEYouCompleteMe-Python3%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81/" rel="alternate" type="text/html" title="关于设置YouCompleteMe Python3语法支持" /><published>2018-09-16T00:00:00+08:00</published><updated>2018-09-16T00:00:00+08:00</updated><id>http://localhost:4000/troubleshoot/2018/09/16/%E5%85%B3%E4%BA%8E%E8%AE%BE%E7%BD%AEYouCompleteMe%20Python3%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81</id><content type="html" xml:base="http://localhost:4000/troubleshoot/2018/09/16/%E5%85%B3%E4%BA%8E%E8%AE%BE%E7%BD%AEYouCompleteMe-Python3%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81/">&lt;h1 id=&quot;关于设置youcompleteme-python3语法支持&quot;&gt;关于设置YouCompleteMe Python3语法支持&lt;/h1&gt;
&lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt;
&lt;p&gt;YouCompleteMe 只能支持 Python2 的补全，不支持 Python3 库的补全。&lt;/p&gt;

&lt;h2 id=&quot;解决&quot;&gt;解决&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;重新去 YouCompleteMe 用 git pull 一下。&lt;/li&gt;
  &lt;li&gt;使用 pip 安装 jedi
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; pip3 install jedi
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;重新用 python3 编译（&lt;strong&gt;非常重要&lt;/strong&gt;）
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; python3 ./install.py --all
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;在 &lt;code class=&quot;highlighter-rouge&quot;&gt;vimrc&lt;/code&gt; 中添加支持
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;nb&quot;&gt;let &lt;/span&gt;g:ycm_server_python_interpreter&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'/usr/bin/python3'&lt;/span&gt;
 &lt;span class=&quot;nb&quot;&gt;let &lt;/span&gt;g:ycm_python_binary_path &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'/usr/local/bin/python3'&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;参考&quot;&gt;参考&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/Valloric/YouCompleteMe/issues/2876&quot;&gt;YouCompleteMe Issue 2876&lt;/a&gt;
&lt;a href=&quot;https://vi.stackexchange.com/questions/6692/how-do-i-complete-python3-with-youcompleteme&quot;&gt; How do I complete Python3 with YouCompleteMe? &lt;/a&gt;&lt;/p&gt;</content><author><name>DeamoV</name></author><category term="YouCompleteMe" /><category term="Vim" /><category term="TroubleShoot" /><summary type="html">关于设置YouCompleteMe Python3语法支持 问题描述 YouCompleteMe 只能支持 Python2 的补全，不支持 Python3 库的补全。</summary></entry><entry><title type="html">Python交互模式下自动补全</title><link href="http://localhost:4000/python/2018/09/15/Python%E4%BA%A4%E4%BA%92%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8/" rel="alternate" type="text/html" title="Python交互模式下自动补全" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/python/2018/09/15/Python%E4%BA%A4%E4%BA%92%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8</id><content type="html" xml:base="http://localhost:4000/python/2018/09/15/Python%E4%BA%A4%E4%BA%92%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8/">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;有时候我们需要测试一个小功能，懒人如我完全不愿意新建一个python文件去测试，但是默认的python交互模式下没有代码补全就很恼火，今天就把它解决了。&lt;/p&gt;
&lt;h2 id=&quot;步骤&quot;&gt;步骤&lt;/h2&gt;
&lt;p&gt;首先在 HOME 目录下新建一个叫做 pythonstartup 的文件。&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;touch ~/.pythonstartup
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;接下来在里面输入如下内容：&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pre&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;code&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;python&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;rlcompleter&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;readline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;atexit&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
 
&lt;span class=&quot;c&quot;&gt;# http://stackoverflow.com/questions/7116038/python-tab-completion-mac-osx-10-7-lion&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'libedit'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__doc__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_and_bind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bind ^I rl_complete'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse_and_bind&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tab: complete'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;n&quot;&gt;histfile&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'HOME'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.pyhist'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_history_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;IOError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;atexit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write_history_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rlcompleter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;最后把它添加到环境变量中，zsh 在  &lt;code class=&quot;highlighter-rouge&quot;&gt;zshrc&lt;/code&gt; 中，bash 在 &lt;code class=&quot;highlighter-rouge&quot;&gt;bash_profile&lt;/code&gt; 中。&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'export PYTHONSTARTUP=~/.pythonstartup'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~/.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zshrc&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# for oh my zsh&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'export PYTHONSTARTUP=~/.pythonstartup'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~/.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bash_profile&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# for bash&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;CSDN中 jorrell 博主的&lt;a href=&quot;https://blog.csdn.net/jorrell/article/details/46973521&quot;&gt; 交互模式下python自动补全 &lt;/a&gt;&lt;/p&gt;</content><author><name>DeamoV</name></author><category term="Python" /><category term="技巧" /><summary type="html">前言 有时候我们需要测试一个小功能，懒人如我完全不愿意新建一个python文件去测试，但是默认的python交互模式下没有代码补全就很恼火，今天就把它解决了。 步骤 首先在 HOME 目录下新建一个叫做 pythonstartup 的文件。 touch ~/.pythonstartup 接下来在里面输入如下内容： &amp;lt;pre name=&quot;code&quot; class=&quot;python&quot;&amp;gt;import rlcompleter import readline import atexit import os # http://stackoverflow.com/questions/7116038/python-tab-completion-mac-osx-10-7-lion if 'libedit' in readline.__doc__: readline.parse_and_bind('bind ^I rl_complete') else: readline.parse_and_bind('tab: complete') histfile = os.path.join(os.environ['HOME'], '.pyhist') try: readline.read_history_file(histfile) except IOError: pass atexit.register(readline.write_history_file, histfile) del readline, rlcompleter, histfile, os 最后把它添加到环境变量中，zsh 在 zshrc 中，bash 在 bash_profile 中。 echo 'export PYTHONSTARTUP=~/.pythonstartup' &amp;gt;&amp;gt; ~/.zshrc # for oh my zsh echo 'export PYTHONSTARTUP=~/.pythonstartup' &amp;gt;&amp;gt; ~/.bash_profile # for bash 参考文献 CSDN中 jorrell 博主的 交互模式下python自动补全 </summary></entry><entry><title type="html">EM算法</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/15/EM%E7%AE%97%E6%B3%95/" rel="alternate" type="text/html" title="EM算法" /><published>2018-09-15T00:00:00+08:00</published><updated>2018-09-15T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/15/EM%E7%AE%97%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/15/EM%E7%AE%97%E6%B3%95/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;EM算法&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;1、必要的前置知识&lt;/h2&gt;
&lt;ol&gt;
	&lt;li&gt;最大似然估计（MLE）：找出一组参数(模型中的 参数)，使得模型产出观察数据的概率最大。&lt;/li&gt;
	&lt;li&gt;贝叶斯算法估计：从先验概率和样本分布情况来计算后验概率的一种方式。&lt;/li&gt;
	&lt;li&gt;最大后验概率估计（MAP）：另一种MLE的感觉，求 $\theta$ 使 $P(x|\theta)P(\theta)$ 的值最大，这也就是要求 $\theta$ 值不仅仅是让似然函数最大，同时要求 $\theta$ 本身出现的先验概率也得比较大。&lt;/li&gt;
	&lt;li&gt;Jensen不等式：如果函数 $f$ 为凸函数，那么存在公式$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta) f(y)$，进一步推论得到若 $\theta_1,...\theta_k \geq 0$ 且$\theta_1+\theta_2+...+\theta_k = 1 $，则有 $f(\theta_1 x_1 + ... + \theta_k x_k) \leq \theta_1 f(x_1) + ... + \theta_k f(x_k)$。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;2、EM算法&lt;/h2&gt;
&lt;p&gt;EM算法（Expectation Maximization Algorithm）最大期望算法，是一种较为常用的算法思路，像之前的 SoftMax 算法就算是 EM 算法的一种，这种算法主要分为以下两个循环操作：&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;E步骤：估计隐藏变量的概率分布期望函数&lt;/li&gt;
	&lt;li&gt;M步骤：根据期望函数重新估计分布参数&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;简单的来说就是写出带参数的表达预测正确的概率公式，然后用一种方法使得这个预测正确的概率最大。其整体的流程如下：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：这个使预测概率最大化的过程往往会得到一些关键的参数，有时候这个参数就是我们要求得某个事件发生的概率。&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;初始化分布参数&lt;/li&gt;
	&lt;li&gt;重复下列操作直至收敛

		&lt;ul&gt;
			&lt;li&gt;E步骤：估计隐藏变量的概率分布期望函数&lt;/li&gt;
			&lt;li&gt;M步骤：根据期望函数重新估计分布参数（用的是MAP），即重新调整模型参数$\theta$，公式如下

				&lt;p&gt;&lt;em&gt;设数据集中包含着我们不能观测到的隐含数据$z = {z_1, z_2, ...,z_m}$&lt;/em&gt;&lt;/p&gt;

				$$
				\begin{split}
				\theta &amp;= arg\ max_\theta\sum_{i=1}^m log(P(x_i;\theta)) \\
				&amp;arg\ max_\theta\sum_{i=1}^m log(\sum_{z_i}P(z_i)P(x_i|z_i;\theta)) \\
				&amp;arg\ max_\theta\sum_{i=1}^m log（\sum_{z_i}(P(x_i,z_i;\theta))
				\end{split}
				$$&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;3、EM算法求解原理&lt;/h2&gt;

&lt;p&gt;根据上一章中的EM算法总述看来，现在我们整体要做的就是根据对数似然函数，调整参数 $\theta$ 使得对数似然函数向变大的方向前进。按照以往的惯例，我们直接进行求导就好了。然而在这里则行不通，我们可以看到中间&lt;strong&gt;存在隐藏数据&lt;/strong&gt;。为什么说是隐藏的呢，那是因为这个数据是不可观测的，也就是说我们&lt;strong&gt;不能在每一步中直接测量到这些数据&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;不过好消息是我们可以得到这个隐藏函数的分布。于是进行的曲线救国过程的第一步如下：&lt;/p&gt;

\begin{split}
l(\theta) &amp;= \sum_{i=1}^m log\sum_z p(x,z;\theta) \\
&amp;= \sum_{i=1}^m log \sum_z Q(z;\theta) * \frac{p(x_i,z;\theta)}{Q(z;\theta)}\ \ \ 步骤1 \\
&amp;= \sum_{i=1}^m log(E_Q(\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤2 \\
&amp;\geq \sum_{i=1}^m E_Q(log(\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤3 \\
&amp;= \sum_{i=1}^m \sum_z Q(z;\theta) log (\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤3
\end{split}
&lt;p&gt;首先，我们明确的目标是得到最佳参数 $\theta$ 也就是说，我们企图得到一种可以把似然函数写成纯 $\theta$ 的公式，之后进行通过求偏导的方式来得到我们要求的最佳参数 $\theta$。&lt;/p&gt;

&lt;p&gt;步骤一中，我们引入了隐藏数据的分布 $Q(z;\theta) $ , 根据分布函数的性质$\sum_z Q(z;\theta)=1 $ ，所以我们加入这个参数是不会改变原函数的数值，之后根据数学期望的定义将右边式子写成了数学期望的形式，于是得到了步骤二。&lt;/p&gt;

&lt;p&gt;在步骤三中，根据根据Jensen不等式，我们可以得到$f(E(x)) \leq E(f(x))$，所以我们将$log$塞到期望函数里面，最终得到了步骤四。&lt;/p&gt;

&lt;p&gt;好了到现在为止，我们只是得到了另一个看似依旧不能计算的式子，貌似多此一举的引入了一个叫做 Jensen 不等式的东西，但是这一步是很关键。在未引入Jensen不等式之前，我们只是单纯的添加了一个叫做隐含数据的变量，但是它在式子中并未和别的参数相关，而引入了Jensen不等式之后，这个式子中的隐含数据将和数据集关联就变得可以测量得到了。&lt;/p&gt;

&lt;p&gt;所以曲线救国的第二步就是，根据Jensen不等式的取等条件得到 $\frac{p(x,z;\theta)}{Q(z;\theta)} = c$ 的时候等号成立，于是根据如下推导得到当等号成立的时候，得到的关系公式 3.1 。&lt;/p&gt;

$$
Q(z,\theta) = \frac{p(x,z;\theta)}{c} = \frac{p(x,z;\theta)}{c*\sum_{z_i}p(x,z_i;\theta)} = \frac{p(x,z;\theta)}{p(x;\theta)} = p(z|x;\theta)\ \ \ 公式3.1
$$
&lt;p&gt;之后将公式 3.1 带入之前步骤三的公式中，得到如下公式3.2&lt;/p&gt;

$$
\sum_{i=1}^m \sum_z p(z|x_i;\theta) log (\frac{p(x_i,z;\theta)}{p(z|x_i;\theta)})\ \ 公式3.2
$$
&lt;p&gt;又由于给定数据集之后 $p(z|x_i;\theta)$ 的值是固定的，所以可以去掉，而保留前方的 $p(z|x_i;\theta)$ 的原因类似 MAP 模型，要求先验概率也要大。之后便是传统的求能使公式3.2最大的模型参数 $\theta$ 。&lt;/p&gt;

&lt;p&gt;至此，我们公式已经推完了，简单的整体分析一下，式子中含有两个变量$z$ 和 $\theta$ ，我们先假定了 $\theta$ 的初始值，显然根据求导等于0能得到 $z$ 的值，之后再根据 $z$ 的值可以反求 $\theta$ 的变化方向。如此往复就是 EM 算法的求解过程。&lt;/p&gt;

&lt;h2&gt;4、结语&lt;/h2&gt;

&lt;p&gt;整体而言 EM 算法相比之前的算法引入了一个叫做隐含数据的变量，这个变量我们认为是会对结果发生影响的。整个算法都是在讨论一种，如何将这种我们不清楚的变量和我们预测手法联系在一起，其中用到了 Jensen 不等式来让他们联系在一起，变得可以被计算。以后有时间的话还会补充 GMM 在EM算法中的应用。&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">EM算法 1、必要的前置知识 最大似然估计（MLE）：找出一组参数(模型中的 参数)，使得模型产出观察数据的概率最大。 贝叶斯算法估计：从先验概率和样本分布情况来计算后验概率的一种方式。 最大后验概率估计（MAP）：另一种MLE的感觉，求 $\theta$ 使 $P(x|\theta)P(\theta)$ 的值最大，这也就是要求 $\theta$ 值不仅仅是让似然函数最大，同时要求 $\theta$ 本身出现的先验概率也得比较大。 Jensen不等式：如果函数 $f$ 为凸函数，那么存在公式$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta) f(y)$，进一步推论得到若 $\theta_1,...\theta_k \geq 0$ 且$\theta_1+\theta_2+...+\theta_k = 1 $，则有 $f(\theta_1 x_1 + ... + \theta_k x_k) \leq \theta_1 f(x_1) + ... + \theta_k f(x_k)$。 2、EM算法 EM算法（Expectation Maximization Algorithm）最大期望算法，是一种较为常用的算法思路，像之前的 SoftMax 算法就算是 EM 算法的一种，这种算法主要分为以下两个循环操作： E步骤：估计隐藏变量的概率分布期望函数 M步骤：根据期望函数重新估计分布参数</summary></entry><entry><title type="html">支持向量机的补充之核函数</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E8%A1%A5%E5%85%85%E4%B9%8B%E6%A0%B8%E5%87%BD%E6%95%B0/" rel="alternate" type="text/html" title="支持向量机的补充之核函数" /><published>2018-09-13T00:00:00+08:00</published><updated>2018-09-13T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E8%A1%A5%E5%85%85%E4%B9%8B%E6%A0%B8%E5%87%BD%E6%95%B0</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E8%A1%A5%E5%85%85%E4%B9%8B%E6%A0%B8%E5%87%BD%E6%95%B0/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;支持向量机的补充之核函数&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;支持向量机的补充之核函数&lt;/h1&gt;
&lt;p&gt;在之前的SVM章节中我们介绍了其具体的原理和大致推导过程，但是由于SVM只能应用于线性可分的数据，那么如果出现了线性不可分的情况怎么办呢，这就要引入今天的重点核函数。这种思想将在未来的深度学习中也会出现。&lt;/p&gt;
&lt;h2&gt;为什么要使用核函数&lt;/h2&gt;
&lt;ol&gt;
	&lt;li&gt;之前我们在线性回归算法中讲到的，使用多项式扩展来考虑属性间有相关性的问题。&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;将非线性问题变成线性问题&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;核函数的核心思想与优势&lt;/h2&gt;

&lt;p&gt;什么是核函数呢？核函数就是两个向量在隐式映射过后的高纬空间中的内积的函数。它的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就说它避免了直接在高维空间中的复杂计算。&lt;/p&gt;

&lt;p&gt;简单的来说，就是原本我们将低维的特征值映射到高维度的空间的时候，我们需要先将需要进行内积的两个向量映射到高维度的空间，之后进行内积操作。而使用核函数的价值在于，先对两个向量进行内积操作，之后再进行高维度的映射，而这个结果可以和之前先对向量进行映射的操作的方法大致相同。&lt;/p&gt;

&lt;p&gt;众所周知，在低维度进行内积操作的话，其运算的复杂度显然比高维度好很多，所以采用核函数的思想被广泛的应用。&lt;/p&gt;

&lt;h2&gt;常用的核函数&lt;/h2&gt;

&lt;p&gt;那么常用的核函数有以下三种&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;多项式核：$\kappa(x_1, x_2)=(\langle x_1, x_2 \rangle + R)^d$&lt;/li&gt;
	&lt;li&gt;高斯核函数RBF：$\kappa(x_1, x_2) = exp (-\frac{||x_1 - x_2||}{2\sigma^2})$&lt;/li&gt;
	&lt;li&gt;线性核函数：$\kappa(x_1,x_2) = \langle x_1, x_2 \rangle$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;结语&lt;/h2&gt;

&lt;p&gt;使用核函数的时候其实就把原来的目标函数中的内积换成核函数就好了，不过其实在Sklearn中这些是可以在调用函数的过程中选择的，个人觉得知道这个概念和思想就好了。&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">支持向量机的补充之核函数 支持向量机的补充之核函数 在之前的SVM章节中我们介绍了其具体的原理和大致推导过程，但是由于SVM只能应用于线性可分的数据，那么如果出现了线性不可分的情况怎么办呢，这就要引入今天的重点核函数。这种思想将在未来的深度学习中也会出现。 为什么要使用核函数 之前我们在线性回归算法中讲到的，使用多项式扩展来考虑属性间有相关性的问题。 将非线性问题变成线性问题。</summary></entry><entry><title type="html">Vim作者对高效使用编辑器的建议</title><link href="http://localhost:4000/vim/2018/09/02/Vim%E4%BD%9C%E8%80%85%E5%AF%B9%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E5%BB%BA%E8%AE%AE/" rel="alternate" type="text/html" title="Vim作者对高效使用编辑器的建议" /><published>2018-09-02T00:00:00+08:00</published><updated>2018-09-02T00:00:00+08:00</updated><id>http://localhost:4000/vim/2018/09/02/Vim%E4%BD%9C%E8%80%85%E5%AF%B9%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E5%BB%BA%E8%AE%AE</id><content type="html" xml:base="http://localhost:4000/vim/2018/09/02/Vim%E4%BD%9C%E8%80%85%E5%AF%B9%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E5%BB%BA%E8%AE%AE/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;Vim作者对高效使用编辑器的建议（附原文地址）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;原文链接：&lt;a href=&quot;https://www.moolenaar.net/habits.html&quot;&gt;《Seven Habits of Effective Text Editing》&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;前言&lt;/h2&gt;
&lt;p&gt;本文摘自Vim主要作者Bram Moolennar的2000年11月在其个人网站发布的提高文本编辑效率的7个方法，个人认为从工具作者那里学习如何使用工具是最好的学习方式。本篇文章重点介绍了，达到高效使用编辑器的方法。&lt;/p&gt;
&lt;h2&gt;第一部分、编辑文本&lt;/h2&gt;
&lt;h3&gt;1、快速在文本间移动&lt;/h3&gt;
&lt;ol&gt;
	&lt;li&gt;通过搜索的方式快速定位

		&lt;ul&gt;
			&lt;li&gt;使用&lt;code&gt;\pattern&lt;/code&gt;的方式搜索&lt;/li&gt;
			&lt;li&gt;使用&lt;code&gt;*&lt;/code&gt;直接对所在单词搜索&lt;/li&gt;
			&lt;li&gt;&lt;code&gt;%&lt;/code&gt;在匹配括号间移动，&lt;code&gt;[{&lt;/code&gt;返回之前的&lt;code&gt;{&lt;/code&gt;，&lt;code&gt;gd&lt;/code&gt;本地变量定义的位置。&lt;/li&gt;
		&lt;/ul&gt;

		&lt;p&gt;&lt;em&gt;注：应对搜索结果配置高亮&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;同样的内容不要打2遍

		&lt;ul&gt;
			&lt;li&gt;使用宏来记录你的重复性的操作（之后的文章会介绍）&lt;/li&gt;
			&lt;li&gt;活用&lt;code&gt;.&lt;/code&gt;来重复上一步的文本操作。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;自动纠错

		&lt;p&gt;对于经常犯的拼写错误，我们使用以下几种方式来避免。&lt;/p&gt;

		&lt;ul&gt;
			&lt;li&gt;自动补全：使用自动补全而不是自己手动输入变量名&lt;/li&gt;
			&lt;li&gt;自动纠错：对于自己经常犯的拼写错误可以使用如下配置来让vim自动纠错。

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-bash&quot;&gt;:abbr Lunix Linux
:abbr pn penguin &lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 当然也可以实现快速输入
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;第二部分、多文本操作&lt;/h2&gt;

&lt;ol&gt;
	&lt;li&gt;使用grep，ack，ag来对工程中所有的文件进行搜索

		&lt;p&gt;&lt;em&gt;注：我们使用了ctrlp，CtrlSF来进行辅助&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;在一个终端窗口中进行分割来方便编辑：如&lt;code&gt;:sp&lt;/code&gt;，&lt;code&gt;:vs&lt;/code&gt;&lt;/li&gt;
	&lt;li&gt;让VIM和其他工具整合在一起使用：如&lt;code&gt;:sh&lt;/code&gt;能进入bash模式等。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;第三部分、迭代优化自己的编辑器&lt;/h2&gt;

&lt;p&gt;这部分个人觉得是最重要的，很多都认为使用VIM是先背快捷键然后熟练使用VIM，但是实际上和键盘盲打相似，是在&lt;strong&gt;使用中慢慢的逐渐提高使用VIM技巧&lt;/strong&gt;，这里大佬给出了如下3个步骤：&lt;/p&gt;

&lt;h3&gt;Step 1&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;While you are editing, keep an eye out for actions you repeat and/or spend quite a bit of time on.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;观察自己在哪些步骤进行了很多的重复性输入工作。&lt;/p&gt;

&lt;h3&gt;Step 2&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Find out if there is an editor command that will do this action quicker. Read the documentation, ask a friend, or look at how others do this.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;查找文档，询问朋友看有没有能让这些操作变得更快的方案。（去VIM的Wiki或者看我的专栏（笑））或者自己写一个宏或者脚本来自动化这些输入。&lt;/p&gt;

&lt;h3&gt;Step3&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Train using the command. Do this until your fingers type it without thinking.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;不断的使用快捷命令，直到你的指头形成肌肉记忆。&lt;/p&gt;

&lt;h2&gt;大佬的建议&lt;/h2&gt;

&lt;p&gt;这里大佬说了一个很有意思的事情，就是希望让我们能养成一个习惯。&lt;/p&gt;

&lt;p&gt;首先，我们不需要去记住一个编辑器的所有的命令，这是完全浪费时间的 ( a complete waste of time )，每个人只需要知道其中 20% 左右的命令就够用了。&lt;/p&gt;

&lt;p&gt;其次，不要去优化只用一到两次的操作，把时间花在大量重复的操作上，写一个宏或者去互联网上查看别人的即决方案。&lt;/p&gt;

&lt;p&gt;最后，&lt;strong&gt;也是最重要的&lt;/strong&gt;，将自己的解决方案和查到的命令&lt;strong&gt;记录下来&lt;/strong&gt;。很多指令，在一段时间内经常使用，我们会熟记于心，但是随着一些原因停止了使用，之后再想回忆起来就需要花更多的时间。&lt;strong&gt;简单的来说，就是不做无用功，让我们的每一次努力达到可叠加的效果。&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;最后的话&lt;/h2&gt;

&lt;p&gt;关于为什么使用VIM，大佬给出了如下的话：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Learning to drive a car takes effort. Is that a reason to keep driving your bicycle? No, you realize you need to invest time to learn a skill. Text editing isn&amp;#39;t different. You need to learn new commands and turn them into a habit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;好的编辑器是值得我们花时间学的，最后感谢大家订阅我的专栏。&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="Vim" /><category term="软件使用经验" /><summary type="html">Vim作者对高效使用编辑器的建议（附原文地址） 原文链接：《Seven Habits of Effective Text Editing》 前言 本文摘自Vim主要作者Bram Moolennar的2000年11月在其个人网站发布的提高文本编辑效率的7个方法，个人认为从工具作者那里学习如何使用工具是最好的学习方式。本篇文章重点介绍了，达到高效使用编辑器的方法。 第一部分、编辑文本 1、快速在文本间移动 通过搜索的方式快速定位</summary></entry><entry><title type="html">赫拉利三部曲</title><link href="http://localhost:4000/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/30/%E8%B5%AB%E6%8B%89%E5%88%A9%E4%B8%89%E9%83%A8%E6%9B%B2/" rel="alternate" type="text/html" title="赫拉利三部曲" /><published>2018-08-30T00:00:00+08:00</published><updated>2018-08-30T00:00:00+08:00</updated><id>http://localhost:4000/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/30/%E8%B5%AB%E6%8B%89%E5%88%A9%E4%B8%89%E9%83%A8%E6%9B%B2</id><content type="html" xml:base="http://localhost:4000/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/30/%E8%B5%AB%E6%8B%89%E5%88%A9%E4%B8%89%E9%83%A8%E6%9B%B2/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;赫拉利三部曲&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;1、作者简介&lt;/h2&gt;
&lt;p&gt;尤瓦尔·赫拉利是以色列历史学家，出生于1976年，牛津大学历史学博士，耶路撒冷希伯来大学历史系教授，代表作为《人类简史》和《未来简史》。尤瓦尔·赫拉利擅长世界历史和宏观历史进程研究，是学界公认的“青年怪才”。&lt;/p&gt;
&lt;h2&gt;2、主要内容&lt;/h2&gt;
&lt;p&gt;这三步曲中，赫拉利分别讨论了三个话题，人的过去是什么，现在的人处在发展的什么位置，以及人的未来的发展方向是什么样子的。&lt;/p&gt;
&lt;p&gt;在人类简史中，赫拉利说明我们人类的有三次决定性的“发明”让我们变成现在的样子。第一次是出现了&lt;strong&gt;谈论抽象的能力&lt;/strong&gt;，这使的我们拓展了表达的能力，使人类的合作范围扩大了许多。第二次则是出现了通过想象力来构建一种&lt;strong&gt;虚拟的政治秩序&lt;/strong&gt;，来继续扩大可合作的范围。最终，通过金钱秩序，帝国秩序和信仰秩序这三种秩序，让全球人类进行合作成为了可能。但是这前两种秩序依旧有它的天花板，于是就带来了一种特殊的革命，“科技革命”。在这场革命中，人类首先承认了自己的无知，立根与&lt;strong&gt;观察和数学&lt;/strong&gt;，这种科学是&lt;strong&gt;可以运用已有理论取得新能力&lt;/strong&gt;，即进步是可以叠加的，就好比我们的瓷器曾今是世界最优秀的技术，但是由于仅仅是口口相传，没有留下准确的实验记录导致了烧制瓷器的技术不断的被“再发明”，最终被日本和欧洲超过。&lt;/p&gt;

&lt;p&gt;而在未来简史中，赫拉利讨论的是我们一直以来促进社会变革的到底是什么？他提出了一个非常惊世骇俗的观点：&lt;strong&gt;推动社会变革的不是我们对真实现实的认识，而是我们头脑中虚构的现实，也就是宗教的力量。&lt;/strong&gt;他认为&lt;strong&gt;人文主义也是一种宗教&lt;/strong&gt;，而如今这种宗教遇到了它的瓶颈。人文三件套，自我不可分割，我有自由意志，我最了解自己。如今分别通过“左右脑实验”，“老鼠实验”，“算法预测”得到了否定。他认为，人文这个宗教触碰到了上限急需变革，而接下来他认为的宗教是“数据教”，算法是数据驱动的，它的宗教三定义则是，价值观为&lt;strong&gt;信息要流动&lt;/strong&gt;。需要遵守的戒律为&lt;strong&gt;不断生产和消化信息，最大化自己的信息流&lt;/strong&gt;。当你遵守了戒律之后得到的许诺是，&lt;strong&gt;如果你允许信息自由流动，让万物之网越来越完善，它就能造福每一个人&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;好了我们知道了赫拉利说的我们未来可能的样子，过去我们是怎么发展的，现在再来说下《今日简史》中我们正在进行什么变革。总体有如下几个：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;AI正在蚕食我们的很多岗位，被替代的岗位后，新创造的岗位出现在相对高层级的位置。被替代工作的人很难获得这些新出现的工作，而变成了无用之人。&lt;/li&gt;
	&lt;li&gt;教育方面，上百年来我们社会的教育都是阶段性的教育模式，现在随着社会的快速变革，我们不得不成为终身学习者已不被淘汰。&lt;/li&gt;
	&lt;li&gt;自由和平等将会受到前所未有的冲击。自由和平等本身就是一对相互对立的概念，鱼和熊掌不可兼得，你在追求平等的时候自然会有一部分失去自由，当追求自由的时候也必然会导致部分不平等的现象。而这次的技术革命很有可能进一步的撕裂这种不平等，甚至突破生物的边界，即有钱人可以通过技术改造自己，用技术让自己远超普通人。&lt;/li&gt;
	&lt;li&gt;恐怖主义横行。恐怖主义其实在当今社会造成的伤亡并不多，甚至没有每年车祸去世的人多。但是它造成的恐慌由于互联网的扩大变得具有非常大的恐慌效应。赫拉利做了这样一个比方，恐怖分子就像一只想摧毁瓷器店的苍蝇，但它自身没那么大的力量，于是它就钻进公牛的耳朵里，让公牛发疯，然后冲进瓷器店。&lt;/li&gt;
	&lt;li&gt;警惕战争。虽然目前的世界和平的原因，赫拉利认为是战争带来的收益已经远远小于战争带来的损耗，所以再次爆发大规模战争的可能性不高，不过凡事有例外。因为宗教的冲突带来的发动战争的动机依旧充分，同时由于武器的进步下一次的战争很可能是毁灭性的，所以我们要谨慎对待他。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;3、个人理解&lt;/h2&gt;

&lt;p&gt;这三本书读完之后，和隔壁KK的《必然》这本书相比，有一种头皮发麻的感觉，内容甚至更像带着严密推理逻辑的恐怖片。但是个人整体还是乐观的，首先正如赫拉利所说，整个推进的过程是缓慢的，我们在中途是有大量的机会可以调整历史前进的方向。&lt;/p&gt;

&lt;p&gt;对于三本书中提到的大部分观点我是持赞同的态度，对于《人类简史》，我认同科技革命特殊的原因在于它的可叠加的特性，也认同人类相比其他物种，包括其他人科物种优秀的部分在于能够合作完成个体不能完成的大型工程这种看法。甚至部分赞同，每次革命都会有部分个体陷入更加悲惨的境地这种结论。对于《未来简史》，我也接受人文主义，带有一定的宗教的色彩，是科学和宗教互相作用的结果。也认同我们发展的未来是“数据教”这种方向。&lt;/p&gt;

&lt;p&gt;但是这种未来真的是一个“危机”么？我是不认同的，首先在三本书中多次提到的，革命之后带来的个体的苦难。这个逻辑看似很合理，像农业革命之后，人的脑容量没有之前采集时期的人类的脑容量大。法老奴役着个体去修建毫无意义的金字塔，现在我们看来这些都是一些少数人奴役集体的暴行。但是这种偏见是否是因为我们站在上帝视角才会这么觉得呢，在当时的历史条件下，这种社会结构完全是一种创举，而且也因为有这种社会结构他们才能在文明的碰撞中幸存下来。而雅典这种超前的民主体制，却因为自身的局限性，（沟通技术手段不够发达）无法扩张，并被罗马取代。也许我们穿越回古代，去询问这些古人，可能他们的幸福指数并不比现代低，因为每个人都有着信仰，有着自己人生的意义。同时，如果一个文明获得了进步，比如获得了人文主义，人类的生活质量得到了提高，你再希望它再退回到之前相对落后的制度中是不会成功的，这是一种必然的规矩，像楚门的世界中，走出了那个精心设计的小天地之后，再想回去就不那么容易了，这是一种不可逆的变化。而我们现在正在经历这种不可逆的变化，即人工智能的发展。&lt;/p&gt;

&lt;p&gt;历史上，人类的进步均是，朝着能让更多的人进行合作的方向进展的。尤其是互联网的出现，地球上的所有人都能轻易的进行沟通，交流，科技的思想从欧洲开始遍布全球。而人类也开始意识到自身的局限性，好比人对大数不敏感，在很多机械性的劳动中容易出错，人由于生理的原因不能进行24小时连轴的工作等等。这些是可以交给人工智能的，就像当年工业革命一样，&lt;strong&gt;人应该做自己擅长的事情，而不是全部都想雨露均沾，我们应该要做减法，而这是趋势&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;是的，不可否认，它会带来很多负面的事情，很多人失业不得不继续学习，也许会造成更大的贫富差距，造成许多不平等。但是整个过程是缓慢的，当今时代下的我们有充足的时间去学习新的知识，以让我们赶上新的科技发展的浪潮。而对于人工智能替代人类的这件事上，我的看法是，人工智能会成为我们的好帮手，但不会替代我们，至少短期内不会。&lt;/p&gt;

&lt;p&gt;总结一下，我个人是不太追求绝对的平等或者绝对的自由的，只要社会是像着规范化的发展，不同的阶级间仍保留充足的上升通道，我觉得便是一个非常nice的未来。至于人工智能，物联网Iot等新技术的出现，个人觉得是一种非常棒的未来，机会和风险并存，相比我们的父辈生活在这个时代是非常幸运的。至于特别的危机，就个体而言，我们只需做好自己手头的事情，顺浪潮而行，少一些抱怨和怨天忧人，多一份好奇的探索。&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="Reading" /><summary type="html">赫拉利三部曲 1、作者简介 尤瓦尔·赫拉利是以色列历史学家，出生于1976年，牛津大学历史学博士，耶路撒冷希伯来大学历史系教授，代表作为《人类简史》和《未来简史》。尤瓦尔·赫拉利擅长世界历史和宏观历史进程研究，是学界公认的“青年怪才”。 2、主要内容 这三步曲中，赫拉利分别讨论了三个话题，人的过去是什么，现在的人处在发展的什么位置，以及人的未来的发展方向是什么样子的。 在人类简史中，赫拉利说明我们人类的有三次决定性的“发明”让我们变成现在的样子。第一次是出现了谈论抽象的能力，这使的我们拓展了表达的能力，使人类的合作范围扩大了许多。第二次则是出现了通过想象力来构建一种虚拟的政治秩序，来继续扩大可合作的范围。最终，通过金钱秩序，帝国秩序和信仰秩序这三种秩序，让全球人类进行合作成为了可能。但是这前两种秩序依旧有它的天花板，于是就带来了一种特殊的革命，“科技革命”。在这场革命中，人类首先承认了自己的无知，立根与观察和数学，这种科学是可以运用已有理论取得新能力，即进步是可以叠加的，就好比我们的瓷器曾今是世界最优秀的技术，但是由于仅仅是口口相传，没有留下准确的实验记录导致了烧制瓷器的技术不断的被“再发明”，最终被日本和欧洲超过。</summary></entry><entry><title type="html">LaunchBar小插件</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/26/LaunchBar%E5%B0%8F%E6%8F%92%E4%BB%B6/" rel="alternate" type="text/html" title="LaunchBar小插件" /><published>2018-08-26T00:00:00+08:00</published><updated>2018-08-26T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/26/LaunchBar%E5%B0%8F%E6%8F%92%E4%BB%B6</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/26/LaunchBar%E5%B0%8F%E6%8F%92%E4%BB%B6/">&lt;h1 id=&quot;wechathelper&quot;&gt;WeChatHelper&lt;/h1&gt;
&lt;p&gt;简介
—-
WeCharHelper是基于Tkkk-iOSer的WeChatPlugin-MacOS微信插件的LaunchBar第三方插件支持。
主要功能有以下两个：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;在不打开微信的情况下&lt;/strong&gt;，通过搜索快速定位到要聊天的对象，并&lt;strong&gt;打开相应的聊天窗口&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;通过搜索快速定位到聊天对象，并发送信息。&lt;strong&gt;全程不打开微信窗口&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;补充说明：支持使用拼音进行汉字的模糊搜索。&lt;/em&gt;
&lt;img src=&quot;https://ws2.sinaimg.cn/large/006tNbRwly1funj39y5ucj30sg0oo764.jpg&quot; alt=&quot;&quot; /&gt;
依赖库
—–&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;python &amp;gt;= 3.6&lt;/li&gt;
  &lt;li&gt;requests&lt;/li&gt;
  &lt;li&gt;TKkk-iOSer/WeChatPlugin-MacOS（支持防撤回，微信免扫码认证，微信多开，自动回复）&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;安装指南&quot;&gt;安装指南&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;HomeBrew安装
 在终端中执行如下指令即可：
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  /usr/bin/ruby -e &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;$(&lt;/span&gt;curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install&lt;span class=&quot;k&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;通过HomeBrew安装python3
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  brew install python3
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;安装python3的Requests库
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  pip3 install requests
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;安装WeChatPlugin-MacOS
 WeChatPlugin-MacOS的&lt;a href=&quot;https://github.com/TKkk-iOSer/WeChatPlugin-MacOS&quot;&gt;Github地址&lt;/a&gt;，这里有详细的下载说明。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/VDeamoV/WeChatHelper&quot;&gt;下载目录&lt;/a&gt;中的WeChatHelper.lbaction，双击即可安装&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;使用说明&quot;&gt;使用说明&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;呼出WeChatHelper
通过LaunchBar搜索WeChatHelper叫出WeChatHelper之后键入空格进入输入模式&lt;/li&gt;
  &lt;li&gt;发送模式 
输入内容格式为“要搜索的微信名/发送的内容”时，下方会出现下拉菜单，选择你要发送的对象发送信息。&lt;/li&gt;
  &lt;li&gt;打开聊天窗口模式
输入内容格式为“要搜索的微信名”，在下拉菜单中选择要打开窗口的对象即可&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;注：由于launchBar的自身原因，在发送内容的时候，以下拉菜单中显示的消息为准，有时会延迟大约2ms左右。&lt;/em&gt;&lt;/p&gt;</content><author><name>DeamoV</name></author><category term="LaunchBar" /><category term="Mac插件开发" /><category term="python" /><summary type="html">WeChatHelper 简介 —- WeCharHelper是基于Tkkk-iOSer的WeChatPlugin-MacOS微信插件的LaunchBar第三方插件支持。 主要功能有以下两个： 在不打开微信的情况下，通过搜索快速定位到要聊天的对象，并打开相应的聊天窗口 通过搜索快速定位到聊天对象，并发送信息。全程不打开微信窗口。</summary></entry><entry><title type="html">集成学习</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" rel="alternate" type="text/html" title="集成学习" /><published>2018-08-24T00:00:00+08:00</published><updated>2018-08-24T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;集成学习&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;集成学习一句话版本&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;集成学习的思想是将若干个学习器（分类器&amp;amp;回归器）组合之后产生新的学习器。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在学习这一章节中，老师提到了这个说法，我觉得非常言简意赅就直接引用了过来。集成学习算法的成功在于保证若分类器（错误率略小于0.5，即勉强比瞎猜好一点）的多样性，且集成不稳定的算法也能得到一种比较明显的提升。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;注：深度学习其实也可以看作是一种集成学习&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;集成学习的作用&lt;/h2&gt;

&lt;p&gt;采用集成学习的原因有以下四点：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;分类器间存在一定的差异性，这会导致分类的边界不同，也就是说分类器是一个比较专精的专家，它有它自己一定的适用范围和特长。那么通过一定的策略将多个弱分类器合并后，就可以拓展模型的适用范围，减少整体的错误率，实现更好的效果。

		&lt;p&gt;&lt;em&gt;注：不严谨的类比的话，就像弹性网络模型就可以看作是由LASSO回归和Ridge回归组成的集成学习。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;对于数据集过大或者过小，过大会导致训练一个模型太慢，过小则会导致训练不充分，在这种情况下可以分别对数据集进行划分和有放回的操作产生不同的数据子集，然后使用数据子集训练不同的分类器，最终再将不同的分类器合并成为一个大的分类器。

		&lt;p&gt;&lt;em&gt;注：这种方案的优势就在于，提高了准确度和训练速度，使得之前很难利用的数据得到了充分的利用&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，然后再进行模型的融合。

		&lt;p&gt;&lt;em&gt;注：这种特性就好比当初素描老师教我们画圆一样，画一个正方形，再用一堆小直线一点一点切成圆形。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;对于多个异构的特征集的时候，很难进行融合，那么可以考虑每个数据集构建一个分类模型，然后将多个模型融合。

		&lt;p&gt;&lt;em&gt;注：简单的来说就是公司有两个人都很厉害，但是偏偏不凑巧两个人打架，就不能把他们放一个部门里，得放不同部门一样。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2&gt;集成学习的三种思想&lt;/h2&gt;

&lt;h3&gt;Bagging&lt;/h3&gt;

&lt;h4&gt;Bagging算法思想&lt;/h4&gt;

&lt;p&gt;Bagging，这个名字就是从袋子里取的意思，本身便很形象的说明了这个算法的核心思想，即在原始数据集上通过&lt;strong&gt;有放回的抽样&lt;/strong&gt;的方式，重新选择出S个新数据集来分别训练S个分类器，随后在预测的时候采用&lt;strong&gt;多数投票&lt;/strong&gt;或者&lt;strong&gt;求均值&lt;/strong&gt;的方式来判断预测结果。&lt;/p&gt;

&lt;h4&gt;Bagging适用弱学习器的范围&lt;/h4&gt;

&lt;p&gt;基本的弱学习器都能用，如Linear、Ridge、Lasso、 Logistic、Softmax、ID3、C4.5、CART、SVM、KNN。&lt;/p&gt;

&lt;h3&gt;Boosting&lt;/h3&gt;

&lt;h4&gt;Boosting算法思想&lt;/h4&gt;

&lt;p&gt;提升学习（Boosting），这个名字也很形象，在赛车游戏中氮气加速有时候界面就描述是boost，也就是越加越快，每次都比上一次更快，也就是说同Bagging是不一样的，Boosting是会根据其他的弱分类器的结果来&lt;strong&gt;更改数据集&lt;/strong&gt;再喂给下一个弱分类器。准确的描述为，Boosting算法每一步产生&lt;strong&gt;弱预测模型&lt;/strong&gt;(如决策树)，并&lt;strong&gt;加权累加&lt;/strong&gt;到总模型中。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;它的意义在于如果一个问题存在弱预测模型，那么可以通过提升技术的办法得到一个强预测模型。&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1: 如果每一步的弱预测模型的生成都是依据损失函数的梯度方式的，那么就称为梯度提升(Gradient boosting)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2：Boosting这个集成学习的思想就有点深度网络的意思了。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;Boosting适用范围&lt;/h4&gt;

&lt;p&gt;提升学习适用于&lt;strong&gt;回归&lt;/strong&gt;和&lt;strong&gt;分类&lt;/strong&gt;的问题。&lt;/p&gt;

&lt;h3&gt;Stacking&lt;/h3&gt;

&lt;p&gt;之前提到了Bagging是把训练集拆成不同的子集训练多个学习器投票，而Boosting是根据学习器学习的结果来改动数据集，经过多层改动后试图获得一个更好的预测效果。Bagging和Boosting这两个集成学习其实并没有通过训练结果来改变弱分类器的参数。相对比而言，Stacking就激进许多，当然也复杂和困难许多，它首先训练出多个不同的模型，然后再以之前&lt;strong&gt;训练的各个模型的输出作为输入来新训练一个新的模型&lt;/strong&gt;，换句话说，Stacking算法根据模型的输出是允许改其他分类器的参数甚至结构的，也正是因为这点&lt;code&gt;sklearn&lt;/code&gt;中很少有stacking的内置的算法。&lt;/p&gt;

&lt;h2&gt;1、Bagging算法&lt;/h2&gt;

&lt;h3&gt;随机森林(Random Forest)&lt;/h3&gt;

&lt;p&gt;随机森林的思路很简单如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;从样本集中用Bootstrap采样选出n个样本;&lt;/li&gt;
	&lt;li&gt;从所有属性中随机选择K个属性，选择出最佳分割属性作为节点创建决策树&lt;/li&gt;
	&lt;li&gt;重复以上两步m次，即建立m棵决策树&lt;/li&gt;
	&lt;li&gt;这m个决策树形成随机森林，通过投票表决结果决定数据属于那一类&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注：RF算法在实际应用中具有比较好的特性，应用也比较广泛，主要应用在&lt;strong&gt;分类、 回归、特征转换、异常点检测&lt;/strong&gt;等。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RF算法分析&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RF的主要优点：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;训练可以并行化，对于大规模样本的训练具有速度的优势。&lt;/li&gt;
	&lt;li&gt;由于进行随机选择决策树划分特征列表，这样在样本维度比较高的时候，仍然具有比较高的训练性能。&lt;/li&gt;
	&lt;li&gt;给以给出各个特征的重要性列表。&lt;/li&gt;
	&lt;li&gt;由于存在随机抽样，训练出来的模型方差小，泛化能力强;。&lt;/li&gt;
	&lt;li&gt;RF实现简单。&lt;/li&gt;
	&lt;li&gt;对于部分特征的缺失不敏感。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;RF的主要缺点：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;在某些噪音比较大的特征上，RF模型容易陷入过拟合。&lt;/li&gt;
	&lt;li&gt;取值比较多的划分特征对RF的决策会产生更大的影响，从而有可能影响模型的效果。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;RF的变种&lt;/h3&gt;

&lt;h4&gt;Extra Tree&lt;/h4&gt;

&lt;p&gt;Extra Tree是RF的一个相当激进的变种，原理基本和RF一样，区别如下:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;RF会随机采样来作为子决策树的训练集，而Extra Tree每个子决策树&lt;strong&gt;采用原始数据&lt;/strong&gt;集训练;&lt;/li&gt;
	&lt;li&gt;RF在选择划分特征点的时候会和传统决策树一样，会基于信息增益、信息增益率、 基尼系数、均方差等原则来选择最优特征值。而Extra Tree会&lt;strong&gt;随机的选择一个特征值&lt;/strong&gt;来划分决策树。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Extra Tree因为是随机选择特征值的划分点，这样会导致决策树的规模一般大于RF所生成的决策树。也就是说Extra Tree模型的方差相对于RF进一步减少。在某些情况下，Extra Tree的泛化能力比RF的强。&lt;/p&gt;

&lt;h4&gt;Totally Random Trees Embedding&lt;/h4&gt;

&lt;p&gt;TRTE算法主要进行了两部分操作，第一部分是对数据进行操作，第二部分是对生成的决策树的位置信息转换成向量信息以供之后构建特征编码使用。抛开数据集上的操作，TRTE算法对RF的变种在于如何参考最终生成的多个决策树来给出预测结果。&lt;/p&gt;

&lt;p&gt;RF是采用投票的方式，而TRTE算法中，每个决策树会&lt;strong&gt;生成一个编码来对应叶子结点的位置信息&lt;/strong&gt;，那么把所有的决策树对应&lt;strong&gt;相同的分类的编码合并&lt;/strong&gt;起来，就可以用这一合并后的编码来代表它的特征了，预测时待预测样本经过这些决策树的预测也会得到这样一个合并后的编码，通过同训练好的类别的编码之间的差距的大小来预测这个样本应该属于哪一个类别。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;详细的说明说下：&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;TRTE是一种非监督的数据转化方式。将&lt;strong&gt;低维的数据集映射到高维&lt;/strong&gt;，从而让映射 到高维的数据更好的应用于分类回归模型。&lt;/li&gt;
	&lt;li&gt;TRTE算法的转换过程类似RF算法的方法，建立T个决策树来拟合数据。当决策树构建完成后，数据集里的每个数据在T个决策树中叶子节点的位置就定下来了， 将位置信息转换为向量就完成了特征转换操作，这个转换过程有点像霍夫曼编码的过程。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;Isolation Forest&lt;/h4&gt;

&lt;p&gt;这个算法是用来异常点检测的，正如isolation这个名字，是找出非正常的点，而这些非正常的点显然是特征比较明确的，故不需要太多的数据，也不需要太大规模的决策树。&lt;/p&gt;

&lt;p&gt;它和RF算法有以下几个差别：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;在随机采样的过程中，一般只需要少量数据即可。&lt;/li&gt;
	&lt;li&gt;在进行决策树构建过程中，IForest算法会随机选择一个划分特征，并对划分特征随机选择一个划分阈值。&lt;/li&gt;
	&lt;li&gt;IForest算法构建的决策树一般深度max_depth是比较小的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;算法思路如下：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于异常点的判断，则是将测试样本x拟合到T棵决策树上。计算在每棵树上该样本的叶子节点的深度$h_t(x)$ 。从而计算出&lt;strong&gt;平均深度&lt;/strong&gt; $h(x) $ 。然后就可以使用下列公式计算样本点x的异常概率值，$p(x,m)$的取值范围为$[0,1]$ ，越接近于1，则是异常点的概率越大。&lt;/p&gt;

$$
p(x,m) = 2^{-\frac{h(x)}{c(m)}}  
$$
$$
c(m) = 2\ln(m-1)+\xi - 2\frac{m-1}{m}\ \ \ \ m为样本个数，\xi为欧拉常数
$$
&lt;p&gt;&lt;em&gt;注：这个公式可以简单的理解为越是出现在越深的层数，这个事件越不可能发生，足够深的情况基本上就可以判断为不可能发生是异常点&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;2、Boosting算法&lt;/h2&gt;

&lt;h3&gt;Adaboost&lt;/h3&gt;

&lt;h4&gt;总览&lt;/h4&gt;

&lt;p&gt;Adaboost全名为Adaptive Boosting，每轮迭代中会在训练集上产生一个新的学习器，然后使用该学习器对所有样本进行预测，以评估每个样本的重要性 (Informative)。换句话来讲就是，算法会为每个样本赋予一个权重，每次用训练好的学习器标注/预测各个样本，如果某个样本点&lt;strong&gt;被预测的越正确，则将其权重降低&lt;/strong&gt;，否则提高样本的权重。权重越高的样本在下一个迭代训练中所占的比重就越大，也就是说越难区分的样本在训练过程中会变得越重要。&lt;/p&gt;

&lt;p&gt;整个算法的迭代的结束条件就是错误率足够小或者达到一定的迭代次数为止。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：整体的过程很像，分豆子，先把我们直接能看出来的区别的豆子分开，留下不太能区分开来的豆子，然后交给母上大人帮忙再分这种感觉。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;细节描述&lt;/h4&gt;

&lt;p&gt;首先再重新强调下，从线性回归开始的两种思想，第一种是，&lt;strong&gt;设计出一个损失函数来代表预测结果，之后根据其应该为极小值和凸函数的特性，求原公式中的参数&lt;/strong&gt;，一般是用导数等于0这种方式。第二种思想则是，当有多个变量共同作用结果的时候，我们给每个变量前加参数，&lt;strong&gt;也就是权值来控制变量的影响结果的能力&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;这两种贯穿了几乎所有机器学习的思想，当然在Adaboost中也不会例外，整体的步骤分两部分：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;每次迭代都为新的弱学习器加权重，并根据损失函数计算得到这个权重。&lt;/li&gt;
	&lt;li&gt;根据这个新的学习器的预测结果，对每个样本特征的权重进行调整。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;算法构建之权重系数&lt;/h4&gt;

&lt;p&gt;假设我们打算用的最终分类器为$G(x)$，第m次迭代用的弱分类器为$G_m(x)$，并给分类器前加权重$\alpha_m$已保证分类准的分类器得到足够的重视。于是得到下面公式3.1，公式3.2。&lt;/p&gt;

$$
f(x) = \sum_{m=1}^M \alpha_m G_m(x)\ \ \ 公式3.1 
$$
$$
G(x) = sign(f(x)) = sign[\sum_{m=1}^M \alpha_m G_m(x)]\ \ \ 公式3.2
$$
&lt;p&gt;有了一个对算法整体的数学表达以后，我们就可以根据它写出AdaBoost的损失函数如下公式3.3：&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：到现在为止大家应该对$e^{(-y_i f(x))}$这种公式的函数图不陌生了，就不赘述了&lt;/em&gt;&lt;/p&gt;

$$
loss = \frac{1}{n}\sum_{i=1}^n I(G(x_i) \neq y_i) \leq \frac{1}{n}\sum_{i=1}^n e^{(-y_i f(x_i))}\ \ \ 公式3.3 
$$
&lt;p&gt;有了损失函数了，那么$\alpha$在哪里呢，是要像SVM一样找个算法一起求么，显然不是了，如果那样子的话估计就不是集成算法了，它是一步一步求的，它&lt;strong&gt;只关心当前最好结果&lt;/strong&gt;，类比算法中的贪心算法。于是，我们讨论第k-1轮和第k论迭代的关系：&lt;/p&gt;

$$
f_{k-1}(x) = \ sum_{j=1}^{k-1}\alpha_j G_j(x) \ \ \ 第k-1轮的函数  
$$
$$
f_k(x) = \sum_{j=1}^k \alpha_j G_j(x) = f_{k-1}(x) + \alpha_k G_k(x) \ \ \ 第k轮函数
$$
&lt;p&gt;根据loss函数的构成方法，我们很容易写出第k轮含有$\alpha_k$的公式如下公式3.4，之后对其进行求导就可以得到$\alpha_k$的公式3.5：&lt;/p&gt;

$$
loss(\alpha_k,G_k(x)) = \frac{1}{n} \sum_{i=1}^n e^{(-y_i(f_{m-1}(x)+\alpha_k G_k(x)))} \ \ \ 公式3.4
$$
$$
\begin{split}
&amp;\alpha_k^* = \frac{1}{x}\ln(\frac{1-\varepsilon_k}{\varepsilon_k})\ \ \ 公式3.5 \\
&amp;\overline w_{ki} = e^{(-y_i f_{k-1}(x))} \\
&amp;\varepsilon_k = \frac{1}{n}\sum_{i=1}^n \overline w_{ki}I(y_i \neq G_m(x_i))
\end{split}
$$
&lt;p&gt;于是至此，我们就将弱分类器简单的连接在一起了，做好了下一步对数据样本特征值的权重调整的准备。&lt;/p&gt;

&lt;h4&gt;算法构建之样本特征权重调整&lt;/h4&gt;

&lt;p&gt;首先我们设定第k轮的数据集的权重分布为$D_k = (w_{k,1}, w_{k,2},... ,w_{k,n},)$。同时每次的$D_k$都是由$D_{k-1}$通过某种规律计算得到的，这种计算公式如下公式3.6：&lt;/p&gt;

$$
\begin{split} 
&amp;w_k = \frac{w_{k-1,i}}{Z_{k-1}}e^{-\alpha_{k-1}y_i G_{k-1}(x_i)}\ \ \ 公式3.6 \\
&amp;Z_k = \sum_{i=1}^n w_{k,i}e^{-\alpha_k y_i G_k(x_i)} 
\end{split}
$$
&lt;p&gt;可以看到这种&lt;strong&gt;第k次迭代开始前&lt;/strong&gt;的数据的权重的调整是在&lt;strong&gt;根据第k-1次迭代中预测结果&lt;/strong&gt;来进行调整的，换句话说，第k次迭代的数据集被第k-1次的训练修正了。&lt;/p&gt;

&lt;h4&gt;算法构建之总览&lt;/h4&gt;

&lt;p&gt;我们现在知道了每个弱分类器的权值是怎么够建的，也知道了Boosting算法中调整数据的部分是怎么调整的，算法的零件已经齐全，现在拼接起来如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;初始化数据集权重为$\frac{1}{n}$，n为特征的个数。&lt;/li&gt;
	&lt;li&gt;加入弱分类器，并根据数据集feed进模型确定这个新加入的弱分类的权重。&lt;/li&gt;
	&lt;li&gt;根据最终训练的结果，调整数据集中不同特征的权值&lt;/li&gt;
	&lt;li&gt;重复2，3步骤直到符合结束条件，一般为达到预计准确度，或者为达到规定迭代次数。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;Gradient Boosting&lt;/h3&gt;

&lt;p&gt;首先值得注意的是，GBDT算法，它有很多&lt;strong&gt;别名如GBT，GTB，BGRT，GBDT，MART&lt;/strong&gt;，初学者很容易把它们当作是多个算法，比如我（笑。&lt;/p&gt;

&lt;p&gt;言归正传GBDT全名为Gradient Boosting Decision Tree。它也是Boosting算法的一种，它的算法推导相比之前算法的较为复杂，详细公式推导参考&lt;a href=&quot;https://blog.csdn.net/yangxudong/article/details/53872141&quot;&gt;这篇文章&lt;/a&gt;，这里就不赘述了。&lt;/p&gt;

&lt;p&gt;算法大体的步骤如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;算法每次迭代生成一颗新的决策树 

		&lt;p&gt;&lt;em&gt;注：GBDT的核心其实是找出一堆决策树，然后让他们的结果累加得到最终的预测值&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数和二阶导数&lt;/li&gt;
	&lt;li&gt;通过贪心策略生成新的决策树，通过等式计算每个叶节点对应的预测值

		&lt;p&gt;&lt;em&gt;注：这步是通过目标函数求导得到的，需要利用第二步中的二阶导数和一阶导数，同时等式的推导中用到了泰勒公式&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;把新生成的决策树 $f_t(x) $ 添加到模型中：$\widehat{y}_i^{t} = \widehat{y}_i^{t-1}+f_t(x_i)$

		&lt;p&gt;&lt;em&gt;注：这里我们会将模型替换为$\widehat{y}_i^{t} = \widehat{y}_i^{t-1}+\xi f_t(x_i)$，这里的$\xi$ 称之为步长或者学习率。增加ϵ因子的目的是为了避免模型过拟合。*
		&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;GBDT总结&lt;/h4&gt;

&lt;p&gt;GBDT优点如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;可以处理连续值和离散值&lt;/li&gt;
	&lt;li&gt;在相对少的调参情况下，模型的预测效果也会不错&lt;/li&gt;
	&lt;li&gt;模型的鲁棒性比较强。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;由于弱学习器之间存在关联关系，难以并行训练模型&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Bagging和Boosting的总结&lt;/h2&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;strong&gt;样本选择：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging算法是有放回的随机采样&lt;/li&gt;
			&lt;li&gt;Boosting算法是每一轮训练集不变，只是训练集中的每个样例在分类器中的权重发生变化，而权重根据上一轮的分类结果进行调整&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;样例权重：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging使用随机抽样，样例的权重&lt;/li&gt;
			&lt;li&gt;Boosting根据错误率不断的调整样例的权重值， 错误率越大则权重越大&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;预测函数：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging所有预测模型的权重相等&lt;/li&gt;
			&lt;li&gt;Boosting算法对于误差小的分类器具有更大的权重&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;并行计算：&lt;/strong&gt;

		&lt;ul&gt;
			&lt;li&gt;Bagging算法可以并行生成各个基模型&lt;/li&gt;
			&lt;li&gt;Boosting理论上只能顺序生产，因为后一个模型需要前一个模型的结果&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;Bagging是减少模型的&lt;strong&gt;variance(方差)&lt;/strong&gt;，Boosting是减少模型的&lt;strong&gt;Bias(偏度)&lt;/strong&gt;。&lt;/li&gt;
	&lt;li&gt;Bagging里每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合。Boosting里每个分类模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合。&lt;/li&gt;
&lt;/ol&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">集成学习 集成学习一句话版本 集成学习的思想是将若干个学习器（分类器&amp;amp;回归器）组合之后产生新的学习器。 在学习这一章节中，老师提到了这个说法，我觉得非常言简意赅就直接引用了过来。集成学习算法的成功在于保证若分类器（错误率略小于0.5，即勉强比瞎猜好一点）的多样性，且集成不稳定的算法也能得到一种比较明显的提升。 注：深度学习其实也可以看作是一种集成学习</summary></entry><entry><title type="html">贝叶斯算法</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/" rel="alternate" type="text/html" title="贝叶斯算法" /><published>2018-08-19T00:00:00+08:00</published><updated>2018-08-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;贝叶斯算法&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;知识前置&lt;/h2&gt;
&lt;p&gt;这个章节的机器学习，其实更像是一种概率论的学习，同时这也是机器学习和数据分析中非常重要的一环。如果学习遇到了困难非常推荐参考张宇考研概率论部分的内容。同时这一章的算法，也是在文本分类中使用的比较多的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;名词解释：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;先验概率：$P(A)$&lt;/li&gt;
	&lt;li&gt;条件概率：$P(A|B)$&lt;/li&gt;
	&lt;li&gt;后验概率：$P(B|A)$&lt;/li&gt;
	&lt;li&gt;全概率：$P(B) = \sum_{i=1}^n P(A_i)*P(B|A_i)$&lt;/li&gt;
	&lt;li&gt;贝叶斯公式：$P(A|B) = \frac{P(A)*P(B|A)}{\sum_{i=1}^n P(B|A_i)*P(A_i)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;概率分布：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;高斯分布：简单的来说它的分布呈现的是正态分布的样子。&lt;a href=&quot;https://blog.csdn.net/renwudao24/article/details/44463489&quot;&gt;参考链接&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;伯努利分布：伯努利分布是0-1分布，简单的来说就是那种仍硬币的概率分布。&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83&quot;&gt;参考链接&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;多项式分布：是伯努利分布的推广，不再是只有两种情况，有多种情况的概率分布。&lt;a href=&quot;https://baike.baidu.com/item/%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83&quot;&gt;参考链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;贝叶斯核心思想：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;找出在特征出现时，各个标签出现的概率，选择概率最大的作为其分类。&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;朴素贝叶斯&lt;/h2&gt;

&lt;p&gt;我们来“望文生义”的理解这个算法，贝叶斯指的就是上面的贝叶斯公式，而朴素则指的是“&lt;strong&gt;特征之间是独立的&lt;/strong&gt;”这个朴素假设。&lt;/p&gt;

&lt;p&gt;假设有给定样本X，其特征向量为$(x_1,x_2,...,x_m)$，同时类别为$y$。算法中使用公式2.1表达在当前特征下将类别y预测正确的概率。由于特征属性之间是假定独立的，所以$P(x_1,x_2,...x_m)$是可以直接拆开的，故根据这个特性优化，得到公式2.2。由于样本给定的情况下，$P(x_1,x_2,...,x_m)$的值不变，故研究概率最大的问题只需要研究公式2.2等号右侧上面的部分，最终写出预测函数公式2.3。&lt;/p&gt;

$$
P(y|x_1,x_2,...,x_m) = \frac{P(y)P(x_1,x_2,...,x_m|y)}{P(x_1,x_2,...,x_m)}\ \ \ 公式2.1  
$$
$$
P(y|x_1,x_2,...,x_m) = \frac{P(y)\prod_{i=1}^m P(x_i|y)}{P(x_1,x_2,...,x_m)}\ \ \ 公式2.2
$$
$$
\hat{y} = arg\ max_y P(y) \prod_{i=1}^m P(x_i|y) \ \ \ 公式2.3
$$
&lt;p&gt;到这里，算法的流程就很显而易见了，和softmax算法类似，让预测正确的概率最大即可，具体计算流程如下：&lt;/p&gt;

&lt;p&gt;设$x = {a_1,a_2,...a_m}$为带分类项，其中a为x的一个特征属性，类别集合$C={y_1,y_2,...y_n}$&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;分别计算所有的$P(y_i|x)$，使用上述公式2.3&lt;/li&gt;
	&lt;li&gt;选择$P(y_i|x)$最大的$y_i$作为x的类型&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;其他朴素贝叶斯&lt;/h2&gt;

&lt;h3&gt;高斯朴素贝叶斯&lt;/h3&gt;

&lt;p&gt;在上述贝叶斯算法中的特征是离散的，那么考虑特征属虚连续值时，且分布服从高斯分布的情况下。用高斯公式（公式3.1）代替原来计算概率的公式。那么根据训练集中，对应的类别下的属性的均值和标准差，对比待分类数据中的特征项划分的各个均值和标准差，即可得到预测类型。&lt;/p&gt;

$$
p(x_k|y_k) = g(x_k,\eta_{y_k},\sigma_{y_k}) = \frac{1}{\sqrt{2 \pi}\sigma}e^{-\frac{(x-\eta_{y_k})^2}{2\sigma_{y_k}^2}}\ \ \ 公式3.1
$$
&lt;h3&gt;伯努利朴素贝叶斯&lt;/h3&gt;

&lt;p&gt;特征值的取值是布尔型的，是有true和false，符合伯努利分布，那么其$P（x_i|y_k）$的表达式如下公式3.3。&lt;/p&gt;

$$
P（x_i|y_k）= P(x_i = 1 | y_k)*x_i + (1-P(x_i=1|y_k))(1-x_k)\ \ \ 公式3.2
$$
&lt;p&gt;&lt;em&gt;注：这意味着没有某个特征也可以是一个特征，其中公式3.2其实是把两个不同条件的概率公式融合在一起了，这种方法也在逻辑回归中使用过&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;多项式朴素贝叶斯&lt;/h3&gt;

&lt;p&gt;特征属性分布服从多项分布时，得到如下公式3.3，公式的来源简单的来说就是已知盒子中红球和所有球的总个数，求从盒中摸到红球的概率差不多。&lt;/p&gt;

&lt;p&gt;其中$N_{y_k x_i} $为类别$y_k$下，特征$x_i$出现的次数，$N_{y_k}$ 指的是类别 $y_k$ 下，所有特征出现的次数。&lt;/p&gt;

$$
P(x_i|y_k) = \frac{N_{y_k x_i} + \alpha}{N_{y_k} + \alpha n}  
$$
&lt;p&gt;&lt;em&gt;注：待预测样本中的特征xi在训练时可能没有出现，如果没有出现，则$N_{y_k x_i} $ 值为0，如果直接拿来计算该样本属于某个分类的概率，结果都将是0。所以在分子中加入α，在分母中加入αn可以解决这个问题。&lt;/em&gt;&lt;/p&gt;

&lt;h2&gt;贝叶斯网络&lt;/h2&gt;

&lt;p&gt;由于之前朴素贝叶斯，前提条件是假定特征值之间没有关系，这显然是不现实的而贝叶斯网络正是解决这个问题的。其&lt;strong&gt;关键方法是图模型&lt;/strong&gt;，我们构建一个图模型，把具有因果联系的各个变量联系在一起。贝叶斯网络的有向无换图中的节点表示随机变量，连接节点的箭头表示因果关系。&lt;/p&gt;

&lt;p&gt;简单的来说贝叶斯网络就是模拟人的认知思维推理模式的，用一组条件概率以及有向无换图对不确定关系推理关系建模。&lt;/p&gt;

&lt;p&gt;而这种方式在深度学习之前是很受欢迎的，它和之后的隐马尔可夫被使用作为提取特征的工具，而现在渐渐的过度到了深度学习。&lt;/p&gt;

&lt;h3&gt;贝叶斯网络工作原理&lt;/h3&gt;

&lt;p&gt;首先贝叶斯网络的实质就是建立一个有向无环图，其中方向代表因果关系。仔细思考一下，为什么是有向无环图，是因为如果是有环的话，就会有节点是自己依赖于自己，显然这样是有问题的。&lt;/p&gt;

&lt;p&gt;具体贝叶斯工作的核心原理可以理解为，根据人已知的经验或者其他手段，规定一些完全没有依赖于其他事件的事件发生的概率，随后根据制作的贝叶斯网络（因果关系图）推算出不同事件发生的概率。这个过程有点像是在做一个概率论的期末考试题，已知A，B，C的概率和ABCD之间转换的关系，问在发生了BC条件下，发生D的概率。大体就是这样一种感觉。&lt;/p&gt;

&lt;p&gt;事例如下图：&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fuiulz6amfj30iy0ge74k.jpg&quot;/&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;其中$x_1,x_2,x_3$独立，则$x_6,x_7$独立&lt;/strong&gt;，$x_1,x_2,x_3,...,x_7$的联合概率分布如下：&lt;/p&gt;

$$
p(x_1,x_2,...,x_7) = p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)p(x_5|x_1,x_3)p(x_6|x_4)p(x_7|x_4,X_5)
$$
&lt;p&gt;实际上这部分的概率计算，其实就是根据初始条件和转移方式，求的目标的概率这样的过程。和之前常用的最大似然估计算法对比，贝叶斯的这一系列算法考虑了先验概率，而最大似然估计算法没有，在最大似然估计算法中其实相当于默认了先验概率是相同的。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：最大后验概率MAP其实可以看作是贝叶斯算法和最大似然估计算法结合的应用&lt;/em&gt;&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">贝叶斯算法 知识前置 这个章节的机器学习，其实更像是一种概率论的学习，同时这也是机器学习和数据分析中非常重要的一环。如果学习遇到了困难非常推荐参考张宇考研概率论部分的内容。同时这一章的算法，也是在文本分类中使用的比较多的。 名词解释：</summary></entry><entry><title type="html">聚类算法（下）</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8B/" rel="alternate" type="text/html" title="聚类算法（下）" /><published>2018-08-19T00:00:00+08:00</published><updated>2018-08-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95(%E4%B8%8B)</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8B/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;聚类算法（下）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;聚类算法上中讲了大名鼎鼎的K-Means算法及其优化变种，在这篇中几种讲述两位两种不同思路的聚类算法。&lt;/p&gt;
&lt;h2&gt;层聚类算法&lt;/h2&gt;
&lt;h3&gt;传统层聚类算法—AGNES和DIANA算法&lt;/h3&gt;
&lt;p&gt;层次聚类和K-Means的思路不太一样，它的思路有点像是决策树，按照层次进行分解，知道满足某种条件为止，传统的层次聚类分为自底而上，和自上而下两类：&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;凝聚的层次聚类:

		&lt;p&gt;这类算法是采用&lt;strong&gt;采用自底向上&lt;/strong&gt;的策略，其中的代表便是&lt;strong&gt;AGNES算法&lt;/strong&gt;(AGglomerative Nesting)，它的核心思想是：最初将每个对象作为一个簇，然后这些簇根据某些准则被一步一步合并，两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定。聚类的合并过程反复进行直到所有的对象满足簇数目。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;分裂的层次聚类：

		&lt;p&gt;和凝聚的层次聚类相反，这种是采用&lt;strong&gt;自顶向下&lt;/strong&gt;的策略，代表算法为&lt;strong&gt;DIANA算法&lt;/strong&gt;(DIvisive Analysis)。其核心思想是：首先将所有对象置于一个簇中，然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式 距离)，直到达到某个终结条件(簇数目或者簇距离达到阈值)。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在AGNES算法中都提到了，簇是根据某些原则进行分裂或者合并的，而这个原则就是&lt;strong&gt;簇间距离&lt;/strong&gt;。计算簇间距离的方法有最小距离（SL聚类），最大距离（CL聚类）以及平均距离（AL聚类），具体的说明如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;最小距离（SL聚类）

		&lt;p&gt;选择两个聚簇中最近的两个样本之间的距离（&lt;strong&gt;S&lt;/strong&gt;ingle/Word-&lt;strong&gt;L&lt;/strong&gt;inkage）&lt;/p&gt;

		&lt;p&gt;&lt;em&gt;注：得到的模型容易形成链式结构&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最大距离（CL聚类）

		&lt;p&gt;选择两个聚簇中最圆的两个眼本的距离（&lt;strong&gt;C&lt;/strong&gt;omplete-&lt;strong&gt;L&lt;/strong&gt;inkage）&lt;/p&gt;

		&lt;p&gt;&lt;em&gt;注：如果出现了异常值的话，那他们的构建很容易受这个异常值的影响。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;平均距离（AL聚类）

		&lt;p&gt;选择两个聚类中的平均值（Average-Linkage聚类算法）或者中值（Median-Linkage聚类法）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;AGNES和DIANA算法优缺点如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;简单，理解容易。&lt;/li&gt;
	&lt;li&gt;合并点/分裂点选择不太容易。&lt;/li&gt;
	&lt;li&gt;合并/分类的操作不能进行撤销。&lt;/li&gt;
	&lt;li&gt;由于执行效率较低$O(t*n^2)$，$t$为迭代次数，$n$为样本点数。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;层次聚类优化算法&lt;/h3&gt;

&lt;p&gt;之前我们看到了传统的层次聚类算法，由于其执行效率太低，且不能动构建的的特点，显然不适合大数据集。于是我们在此基础上引入了&lt;strong&gt;BIRCH算法&lt;/strong&gt;和&lt;strong&gt;CURE算法&lt;/strong&gt;。&lt;/p&gt;

&lt;h4&gt;BIRCH算法&lt;/h4&gt;

&lt;p&gt;BIRCH (balanced iterative reducing and clustering using hierarchies) 算法，英文的全称翻译过来以后是平衡迭代削减聚类算法，其构成和我们考研数据结构中学过的B+树非常的类似，甚至很多特性都是相同的，具体的说它构建的树叫做CF（Cluster Feature）-Tree。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;节点，即簇的结构：

		&lt;p&gt;既然是树，那么就不得不提它的节点的结构了。在BIRCH构建CF树的过程中，每个节点等于说是存放了它之下所有节点的特征，于是他在节点中存放了如下的三部分数据。&lt;/p&gt;

		&lt;ul&gt;
			&lt;li&gt;N，指在这个节点中有多少个样本点。&lt;/li&gt;
			&lt;li&gt;LS，指的是这个节点中的样本相应特征的和。&lt;/li&gt;
			&lt;li&gt;LS，指的是这个节点中的样本相应特征的特征的平方和。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;节点之间，节点和子节点，以及叶子结点之间的关系

		&lt;p&gt;节点和其子节点是包含的关系，也就是父节点中的N，LS以及SS是其所有子节点的和。而相应的样本点的具体信息指包含在底层节点中（叶子结点的子节点），同时叶子结点构成一个单项链表，同时有一个指针指向其表头。这点的特性是&lt;strong&gt;同B+树高度一致的&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最多子女个数，以及分裂判定

		&lt;p&gt;和B+树一样，对于树构建中的分叉个数是有限制的，这个限制需要提前给出，即&lt;strong&gt;分支因子&lt;/strong&gt;。同时，&lt;strong&gt;值得注意的是&lt;/strong&gt;，一般而言在构建节点簇的中心点的时候，一般选用第一个进入这个节点的样本点作为中心点，然后根据指定的该簇和中心点限定的距离，即&lt;strong&gt;类直径&lt;/strong&gt;，其往往通过LS和SS算出。判断新入的点是否可以划入该簇，而分裂节点的时候，往往以这个初始点进行分割。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;综上我们可以看出，BIRCH算法的本质&lt;strong&gt;其实就是动态的插入样本点，然后动态的根据规则构建一个类B+树。&lt;/strong&gt;它的优点是动态建树且效率高是线性效率，即每个样本点都是一次性插入的，同时也节省内存，所以非常适合大数据集。不过遗憾的是它也是采用距离作为分类标准，故&lt;strong&gt;只适合分布呈凸形或者球形的数据集&lt;/strong&gt;、且需要给定聚类个数和簇之间的相关参数，而&lt;strong&gt;这些对节点CF的限制可能导致簇类结果和真实不太一致&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1：BIRCH不依赖给定的待分类簇数量K，但是给定了K值最好，若不一定K值，最终CF-Tree的叶子结点树木就是最终分类的簇的数目。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2:BIRCH算法在训练大规模数据集的时候，和mini-batch K-Means相比，BIRCH算法更加适合类别数量K比较多的情况。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注3：由于类直径是通过LS和SS算出的，所以当特征维度超过20～30左右的时候，不建议使用该算法。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;CURE算法（使用代表点的聚类法）&lt;/h4&gt;

&lt;p&gt;CURE（Clustering Using REpresentatives），该算法先把每个数据点看成一类，然后合并距离最近的类直至类个数为所要求的个数为止。但是和AGNES算法的区别是：取消了使用所有点，或用中心点+距离来表示一个类，而是从每个类中抽取固定数量、 分布较好的点作为此类的代表点，并将这些代表点乘以一个适当的收缩因子，使它们更加靠近类中心点。代表点的收缩特性可以调整模型可以匹配那些非球形的场景，而且&lt;strong&gt;收缩因子的使用可以减少噪音对聚类的影响&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;CURE算法的优点是能够处理&lt;strong&gt;非球形分布&lt;/strong&gt;的应用场景，同时彩娱乐随机抽样和分区的方式可以提高算法的执行效率。&lt;/p&gt;

&lt;h2&gt;密度聚类算法&lt;/h2&gt;

&lt;p&gt;密度聚类方法的指导思想是：只要样本点的密度大于某个阀值，则将该样本添加到最近的簇中。这类算法可以克服基于距离的算法只能发现凸聚类的缺点，可以发现任意形状的聚类，而且对噪声数据不敏感。不过这种计算的复杂度高，计算量大。&lt;/p&gt;

&lt;p&gt;密度聚类算法的常用算法有&lt;strong&gt;DBSCAN&lt;/strong&gt;和&lt;strong&gt;密度最大值算法&lt;/strong&gt;。&lt;/p&gt;

&lt;h3&gt;DBSCAN算法&lt;/h3&gt;

&lt;p&gt;DBSCAN（Density-Based Spatial Clustering of Applications with Noise），将簇定义为密度相连的点的最大集合，能够将足够高密度的区域划分为簇，并且在具有噪声的空间数据上能够发现任意形状的簇。其核心思路&lt;strong&gt;是用一个点的ε邻域内的邻居点数衡量该点所在空间的密度&lt;/strong&gt;，该算法可以找出形状不规则的cluster，而且聚类的时候事先不需要给定cluster的数量。&lt;/p&gt;

&lt;h4&gt;DBSCAN算法流程&lt;/h4&gt;

&lt;p&gt;它的算法流程如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;如果一个点$x$的$\varepsilon$领域内包含m个对象，则创建一个x作为&lt;strong&gt;核心对象&lt;/strong&gt;的新簇。&lt;/li&gt;
	&lt;li&gt;寻找并合并核心对象&lt;strong&gt;直接密度可达&lt;/strong&gt;的对象&lt;/li&gt;
	&lt;li&gt;没有新点可以更新簇的时候，算法结束&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1. 每个簇至少包含一个&lt;strong&gt;核心对象&lt;/strong&gt;；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2. 非核心对象可以是簇的一部分，构成簇的边缘；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;3. 包含过少对象的簇被认为是噪声；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;4. 最大的密度相连对象的集合C为密度聚类中的一个簇，它满足两个属性，Maximality和Connectivity，Maximality指的是若$x$属于C，$y$从$x$&lt;strong&gt;密度可达&lt;/strong&gt;，那么$y$也属于C，Connectivity指的是，若$x$和$y$都属于C，那么$x$和$y$是&lt;strong&gt;密度相连&lt;/strong&gt;的。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;DBSCAN相关名词解释&lt;/h4&gt;

&lt;p&gt;其中提到的定义有$\varepsilon$领域，密度，MinPts，核心点，边界点，噪音点，直接密度可达，密度可达，密度相连。他们的解释如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;$\varepsilon$邻域($\varepsilon$ neighborhood）：给定对象在半径$\varepsilon$的区域。&lt;/li&gt;
	&lt;li&gt;密度(density)：在$\varepsilon$领域中的$x$的密度，是一个整数依赖于半径$\varepsilon$，$N_{\varepsilon}(X) $指的是半径内的点的个数。

		$$
		p(x) = |N_{\varepsilon}(X)|  
		$$&lt;/li&gt;
	&lt;li&gt;MinPts：指得是判定该点是不是核心点的时候使用的阀值，记为M&lt;/li&gt;
	&lt;li&gt;核心点（core point）：如果$p(x) \geq M$ ,那么称$x$为$X$的核心点，记由$X$中所有核心点构成的集合为$X_c$，并记$X_nc$ 表示由$X$中所有非核心点构成的集合。通俗的来说， 核心点是密度达到一定阀值的的点。&lt;/li&gt;
	&lt;li&gt;边界点（border point）：如果非核心点$x$的$\varepsilon$邻域中存在核心点，那么认为$x$为$X$的边界点。通俗来讲就是密度特别稠密的边缘地带，也就是簇的边缘部分。&lt;/li&gt;
	&lt;li&gt;噪音点（noise point）：集合中除了边界点和核心点之外的点都是噪音点，所有噪音点组成的集合叫做$X_noi$，显然这些点就是对应稀疏区域的点。&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;直接密度可达&lt;/strong&gt;：这个是密度聚类中最重要的概念，它指的是给定一个对象集合 $X$，如果$y$是在$x$的$\varepsilon$邻域内，而且$x$是一个核心对象，可以说对象y从对象$x$出发是直接密度可达的&lt;/li&gt;
	&lt;li&gt;密度可达：如果存在一个对象链$p_1, p_2,...,p_m $ ，如果满足$p_{i+1}$是从$p_i$&lt;strong&gt;直接密度可达&lt;/strong&gt;的，那么称$p_m$是从$p1$密度可达的，简单的来说就像铁链环环相扣差不多。&lt;/li&gt;
	&lt;li&gt;密度相连：在集合$X$中，如果存在一个对象$o$，使得对象$x$和$y$是从$o$关于$\varepsilon$和$m$密度可达的，那么对象$x$和$y$是关于$\varepsilon$和$m$密度相连的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;DBSCAN算法优缺点&lt;/h4&gt;

&lt;p&gt;优点: &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;不需要事先给定cluster的数目&lt;/li&gt;
	&lt;li&gt;可以发现&lt;strong&gt;任意形状的cluster&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;能够&lt;strong&gt;找出数据中的噪音，且对噪音不敏感&lt;/strong&gt; &lt;/li&gt;
	&lt;li&gt;算法只需要&lt;strong&gt;两个输入参数&lt;/strong&gt; &lt;/li&gt;
	&lt;li&gt;聚类结果几乎不依赖节点的遍历顺序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;DBSCAN算法聚类效果依赖距离公式的选取，最常用的距离公式为欧几里得距离。但是对于高维数据，由于维数太多，距离的度量已变得不是那么重要&lt;/li&gt;
	&lt;li&gt;DBSCAN算法不适合数据集中密度差异很小的情况&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;MDCA密度最大值聚类算法&lt;/h3&gt;

&lt;p&gt;MDCA(Maximum Density Clustering Application)算法基于密度的思想引入划分聚类中，能够自动确定簇数量并发现任意形状的簇。另外MDCA一般不保留噪声，因此也避免了阈值选择不当情况下造成的对象丢弃情况。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：MDCA的算法和AGNES非常相像，不同的是最初的初始簇确定是通过密度来确定的。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;MDCA算法思路&lt;/h4&gt;

&lt;p&gt;MDCA算法核心一共分三步，划分、合并簇以及处理剩余节点三部分。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;将数据集划分为基本簇：

		&lt;ul&gt;
			&lt;li&gt;对数据集X选取最大密度点$P_{max}$ ，形成以最大密度点为核心的新簇$C_i$，按照距离排序计算出序列$S_{p_max}$,对序列的前M个样本数据进行循环判断，如果节点的密度大于等于$density_0$ ，那么将当前节点添加$C_i$中。&lt;/li&gt;
			&lt;li&gt;循环处理剩下的数据集X，选择最大密度点$P_{max}$，并构建基本簇$C_{i+1}$，直到X中剩余的样本数据的密度均小于$deansity_0$。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;使用凝聚层次聚类的思想，合并较近的基本簇，得到最终的簇划分：

		&lt;ul&gt;
			&lt;li&gt;在所有簇中选择距离最近的两个簇进行合并，合并要求是：簇间距小于等于$dist_0$，如果所有簇中没有簇间距小于$dist_0$的时候，结束合并操作&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;处理剩余节点，归入最近的簇

		&lt;ul&gt;
			&lt;li&gt;最常用、最简单的方式是：将剩余样本对象归入到最近的簇。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;MDCA算法名词解释&lt;/h4&gt;

&lt;p&gt;最大密度点：如字面意思，就是密度最大的点，密度计算公式一般取DBSCAN算法中的密度计算公式。&lt;/p&gt;

&lt;p&gt;有序序列$S_{p_{max}}$：根据所有对象与最大密度点的距离进行排序。&lt;/p&gt;

&lt;p&gt;密度阈值$density_0$：当节点的密度值大于密度阈值的时候，认为该节点属于一个 比较固定的簇，在第一次构建基本簇的时候，就将这些节点添加到对应簇中，如果小于这个值的时候，暂时认为该节点为噪声节点。&lt;/p&gt;

&lt;p&gt;簇间距离：对于两个簇C1和C2之间的距离，采用两个簇中最近两个节点之间的距离作为簇间距离。&lt;/p&gt;

&lt;p&gt;M值：初始簇中最多数据样本个数&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">聚类算法（下） 聚类算法上中讲了大名鼎鼎的K-Means算法及其优化变种，在这篇中几种讲述两位两种不同思路的聚类算法。 层聚类算法 传统层聚类算法—AGNES和DIANA算法 层次聚类和K-Means的思路不太一样，它的思路有点像是决策树，按照层次进行分解，知道满足某种条件为止，传统的层次聚类分为自底而上，和自上而下两类： 凝聚的层次聚类:</summary></entry></feed>