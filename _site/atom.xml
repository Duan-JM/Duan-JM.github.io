<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-08-19T21:48:14+08:00</updated><id>http://localhost:4000/</id><title type="html">Pre-Demo-Field</title><subtitle>Coding Life Coding Fun</subtitle><author><name>DeamoV</name></author><entry><title type="html">聚类算法（下）</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8B/" rel="alternate" type="text/html" title="聚类算法（下）" /><published>2018-08-19T00:00:00+08:00</published><updated>2018-08-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95(%E4%B8%8B)</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8B/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;聚类算法（下）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;聚类算法上中讲了大名鼎鼎的K-Means算法及其优化变种，在这篇中几种讲述两位两种不同思路的聚类算法。&lt;/p&gt;
&lt;h2&gt;层聚类算法&lt;/h2&gt;
&lt;h3&gt;传统层聚类算法—AGNES和DIANA算法&lt;/h3&gt;
&lt;p&gt;层次聚类和K-Means的思路不太一样，它的思路有点像是决策树，按照层次进行分解，知道满足某种条件为止，传统的层次聚类分为自底而上，和自上而下两类：&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;凝聚的层次聚类:

		&lt;p&gt;这类算法是采用&lt;strong&gt;采用自底向上&lt;/strong&gt;的策略，其中的代表便是&lt;strong&gt;AGNES算法&lt;/strong&gt;(AGglomerative Nesting)，它的核心思想是：最初将每个对象作为一个簇，然后这些簇根据某些准则被一步一步合并，两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定。聚类的合并过程反复进行直到所有的对象满足簇数目。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;分裂的层次聚类：

		&lt;p&gt;和凝聚的层次聚类相反，这种是采用&lt;strong&gt;自顶向下&lt;/strong&gt;的策略，代表算法为&lt;strong&gt;DIANA算法&lt;/strong&gt;(DIvisive Analysis)。其核心思想是：首先将所有对象置于一个簇中，然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式 距离)，直到达到某个终结条件(簇数目或者簇距离达到阈值)。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在AGNES算法中都提到了，簇是根据某些原则进行分裂或者合并的，而这个原则就是&lt;strong&gt;簇间距离&lt;/strong&gt;。计算簇间距离的方法有最小距离（SL聚类），最大距离（CL聚类）以及平均距离（AL聚类），具体的说明如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;最小距离（SL聚类）

		&lt;p&gt;选择两个聚簇中最近的两个样本之间的距离（&lt;strong&gt;S&lt;/strong&gt;ingle/Word-&lt;strong&gt;L&lt;/strong&gt;inkage）&lt;/p&gt;

		&lt;p&gt;&lt;em&gt;注：得到的模型容易形成链式结构&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最大距离（CL聚类）

		&lt;p&gt;选择两个聚簇中最圆的两个眼本的距离（&lt;strong&gt;C&lt;/strong&gt;omplete-&lt;strong&gt;L&lt;/strong&gt;inkage）&lt;/p&gt;

		&lt;p&gt;&lt;em&gt;注：如果出现了异常值的话，那他们的构建很容易受这个异常值的影响。&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;平均距离（AL聚类）

		&lt;p&gt;选择两个聚类中的平均值（Average-Linkage聚类算法）或者中值（Median-Linkage聚类法）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;AGNES和DIANA算法优缺点如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;简单，理解容易。&lt;/li&gt;
	&lt;li&gt;合并点/分裂点选择不太容易。&lt;/li&gt;
	&lt;li&gt;合并/分类的操作不能进行撤销。&lt;/li&gt;
	&lt;li&gt;由于执行效率较低$O(t*n^2)$，$t$为迭代次数，$n$为样本点数。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;层次聚类优化算法&lt;/h3&gt;

&lt;p&gt;之前我们看到了传统的层次聚类算法，由于其执行效率太低，且不能动构建的的特点，显然不适合大数据集。于是我们在此基础上引入了&lt;strong&gt;BIRCH算法&lt;/strong&gt;和&lt;strong&gt;CURE算法&lt;/strong&gt;。&lt;/p&gt;

&lt;h4&gt;BIRCH算法&lt;/h4&gt;

&lt;p&gt;BIRCH (balanced iterative reducing and clustering using hierarchies) 算法，英文的全称翻译过来以后是平衡迭代削减聚类算法，其构成和我们考研数据结构中学过的B+树非常的类似，甚至很多特性都是相同的，具体的说它构建的树叫做CF（Cluster Feature）-Tree。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;节点，即簇的结构：

		&lt;p&gt;既然是树，那么就不得不提它的节点的结构了。在BIRCH构建CF树的过程中，每个节点等于说是存放了它之下所有节点的特征，于是他在节点中存放了如下的三部分数据。&lt;/p&gt;

		&lt;ul&gt;
			&lt;li&gt;N，指在这个节点中有多少个样本点。&lt;/li&gt;
			&lt;li&gt;LS，指的是这个节点中的样本相应特征的和。&lt;/li&gt;
			&lt;li&gt;LS，指的是这个节点中的样本相应特征的特征的平方和。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;节点之间，节点和子节点，以及叶子结点之间的关系

		&lt;p&gt;节点和其子节点是包含的关系，也就是父节点中的N，LS以及SS是其所有子节点的和。而相应的样本点的具体信息指包含在底层节点中（叶子结点的子节点），同时叶子结点构成一个单项链表，同时有一个指针指向其表头。这点的特性是&lt;strong&gt;同B+树高度一致的&lt;/strong&gt;。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最多子女个数，以及分裂判定

		&lt;p&gt;和B+树一样，对于树构建中的分叉个数是有限制的，这个限制需要提前给出，即&lt;strong&gt;分支因子&lt;/strong&gt;。同时，&lt;strong&gt;值得注意的是&lt;/strong&gt;，一般而言在构建节点簇的中心点的时候，一般选用第一个进入这个节点的样本点作为中心点，然后根据指定的该簇和中心点限定的距离，即&lt;strong&gt;类直径&lt;/strong&gt;，其往往通过LS和SS算出。判断新入的点是否可以划入该簇，而分裂节点的时候，往往以这个初始点进行分割。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;综上我们可以看出，BIRCH算法的本质&lt;strong&gt;其实就是动态的插入样本点，然后动态的根据规则构建一个类B+树。&lt;/strong&gt;它的优点是动态建树且效率高是线性效率，即每个样本点都是一次性插入的，同时也节省内存，所以非常适合大数据集。不过遗憾的是它也是采用距离作为分类标准，故&lt;strong&gt;只适合分布呈凸形或者球形的数据集&lt;/strong&gt;、且需要给定聚类个数和簇之间的相关参数，而&lt;strong&gt;这些对节点CF的限制可能导致簇类结果和真实不太一致&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1：BIRCH不依赖给定的待分类簇数量K，但是给定了K值最好，若不一定K值，最终CF-Tree的叶子结点树木就是最终分类的簇的数目。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2:BIRCH算法在训练大规模数据集的时候，和mini-batch K-Means相比，BIRCH算法更加适合类别数量K比较多的情况。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注3：由于类直径是通过LS和SS算出的，所以当特征维度超过20～30左右的时候，不建议使用该算法。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;CURE算法（使用代表点的聚类法）&lt;/h4&gt;

&lt;p&gt;CURE（Clustering Using REpresentatives），该算法先把每个数据点看成一类，然后合并距离最近的类直至类个数为所要求的个数为止。但是和AGNES算法的区别是：取消了使用所有点，或用中心点+距离来表示一个类，而是从每个类中抽取固定数量、 分布较好的点作为此类的代表点，并将这些代表点乘以一个适当的收缩因子，使它们更加靠近类中心点。代表点的收缩特性可以调整模型可以匹配那些非球形的场景，而且&lt;strong&gt;收缩因子的使用可以减少噪音对聚类的影响&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;CURE算法的优点是能够处理&lt;strong&gt;非球形分布&lt;/strong&gt;的应用场景，同时彩娱乐随机抽样和分区的方式可以提高算法的执行效率。&lt;/p&gt;

&lt;h2&gt;密度聚类算法&lt;/h2&gt;

&lt;p&gt;密度聚类方法的指导思想是：只要样本点的密度大于某个阀值，则将该样本添加到最近的簇中。这类算法可以克服基于距离的算法只能发现凸聚类的缺点，可以发现任意形状的聚类，而且对噪声数据不敏感。不过这种计算的复杂度高，计算量大。&lt;/p&gt;

&lt;p&gt;密度聚类算法的常用算法有&lt;strong&gt;DBSCAN&lt;/strong&gt;和&lt;strong&gt;密度最大值算法&lt;/strong&gt;。&lt;/p&gt;

&lt;h3&gt;DBSCAN算法&lt;/h3&gt;

&lt;p&gt;DBSCAN（Density-Based Spatial Clustering of Applications with Noise），将簇定义为密度相连的点的最大集合，能够将足够高密度的区域划分为簇，并且在具有噪声的空间数据上能够发现任意形状的簇。其核心思路&lt;strong&gt;是用一个点的ε邻域内的邻居点数衡量该点所在空间的密度&lt;/strong&gt;，该算法可以找出形状不规则的cluster，而且聚类的时候事先不需要给定cluster的数量。&lt;/p&gt;

&lt;h4&gt;DBSCAN算法流程&lt;/h4&gt;

&lt;p&gt;它的算法流程如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;如果一个点$x$的$\varepsilon$领域内包含m个对象，则创建一个x作为&lt;strong&gt;核心对象&lt;/strong&gt;的新簇。&lt;/li&gt;
	&lt;li&gt;寻找并合并核心对象&lt;strong&gt;直接密度可达&lt;/strong&gt;的对象&lt;/li&gt;
	&lt;li&gt;没有新点可以更新簇的时候，算法结束&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注：&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1. 每个簇至少包含一个&lt;strong&gt;核心对象&lt;/strong&gt;；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;2. 非核心对象可以是簇的一部分，构成簇的边缘；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;3. 包含过少对象的簇被认为是噪声；&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;4. 最大的密度相连对象的集合C为密度聚类中的一个簇，它满足两个属性，Maximality和Connectivity，Maximality指的是若$x$属于C，$y$从$x$&lt;strong&gt;密度可达&lt;/strong&gt;，那么$y$也属于C，Connectivity指的是，若$x$和$y$都属于C，那么$x$和$y$是&lt;strong&gt;密度相连&lt;/strong&gt;的。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;DBSCAN相关名词解释&lt;/h4&gt;

&lt;p&gt;其中提到的定义有$\varepsilon$领域，密度，MinPts，核心点，边界点，噪音点，直接密度可达，密度可达，密度相连。他们的解释如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;$\varepsilon$邻域($\varepsilon$ neighborhood）：给定对象在半径$\varepsilon$的区域。&lt;/li&gt;
	&lt;li&gt;密度(density)：在$\varepsilon$领域中的$x$的密度，是一个整数依赖于半径$\varepsilon$，$N_{\varepsilon}(X) $指的是半径内的点的个数。

		$$
		p(x) = |N_{\varepsilon}(X)|  
		$$&lt;/li&gt;
	&lt;li&gt;MinPts：指得是判定该点是不是核心点的时候使用的阀值，记为M&lt;/li&gt;
	&lt;li&gt;核心点（core point）：如果$p(x) \geq M$ ,那么称$x$为$X$的核心点，记由$X$中所有核心点构成的集合为$X_c$，并记$X_nc$ 表示由$X$中所有非核心点构成的集合。通俗的来说， 核心点是密度达到一定阀值的的点。&lt;/li&gt;
	&lt;li&gt;边界点（border point）：如果非核心点$x$的$\varepsilon$邻域中存在核心点，那么认为$x$为$X$的边界点。通俗来讲就是密度特别稠密的边缘地带，也就是簇的边缘部分。&lt;/li&gt;
	&lt;li&gt;噪音点（noise point）：集合中除了边界点和核心点之外的点都是噪音点，所有噪音点组成的集合叫做$X_noi$，显然这些点就是对应稀疏区域的点。&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;直接密度可达&lt;/strong&gt;：这个是密度聚类中最重要的概念，它指的是给定一个对象集合 $X$，如果$y$是在$x$的$\varepsilon$邻域内，而且$x$是一个核心对象，可以说对象y从对象$x$出发是直接密度可达的&lt;/li&gt;
	&lt;li&gt;密度可达：如果存在一个对象链$p_1, p_2,...,p_m $ ，如果满足$p_{i+1}$是从$p_i$&lt;strong&gt;直接密度可达&lt;/strong&gt;的，那么称$p_m$是从$p1$密度可达的，简单的来说就像铁链环环相扣差不多。&lt;/li&gt;
	&lt;li&gt;密度相连：在集合$X$中，如果存在一个对象$o$，使得对象$x$和$y$是从$o$关于$\varepsilon$和$m$密度可达的，那么对象$x$和$y$是关于$\varepsilon$和$m$密度相连的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;DBSCAN算法优缺点&lt;/h4&gt;

&lt;p&gt;优点: &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;不需要事先给定cluster的数目&lt;/li&gt;
	&lt;li&gt;可以发现&lt;strong&gt;任意形状的cluster&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;能够&lt;strong&gt;找出数据中的噪音，且对噪音不敏感&lt;/strong&gt; &lt;/li&gt;
	&lt;li&gt;算法只需要&lt;strong&gt;两个输入参数&lt;/strong&gt; &lt;/li&gt;
	&lt;li&gt;聚类结果几乎不依赖节点的遍历顺序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;DBSCAN算法聚类效果依赖距离公式的选取，最常用的距离公式为欧几里得距离。但是对于高维数据，由于维数太多，距离的度量已变得不是那么重要&lt;/li&gt;
	&lt;li&gt;DBSCAN算法不适合数据集中密度差异很小的情况&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;MDCA密度最大值聚类算法&lt;/h3&gt;

&lt;p&gt;MDCA(Maximum Density Clustering Application)算法基于密度的思想引入划分聚类中，能够自动确定簇数量并发现任意形状的簇。另外MDCA一般不保留噪声，因此也避免了阈值选择不当情况下造成的对象丢弃情况。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：MDCA的算法和AGNES非常相像，不同的是最初的初始簇确定是通过密度来确定的。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;MDCA算法思路&lt;/h4&gt;

&lt;p&gt;MDCA算法核心一共分三步，划分、合并簇以及处理剩余节点三部分。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;将数据集划分为基本簇：

		&lt;ul&gt;
			&lt;li&gt;对数据集X选取最大密度点$P_{max}$ ，形成以最大密度点为核心的新簇$C_i$，按照距离排序计算出序列$S_{p_max}$,对序列的前M个样本数据进行循环判断，如果节点的密度大于等于$density_0$ ，那么将当前节点添加$C_i$中。&lt;/li&gt;
			&lt;li&gt;循环处理剩下的数据集X，选择最大密度点$P_{max}$，并构建基本簇$C_{i+1}$，直到X中剩余的样本数据的密度均小于$deansity_0$。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;使用凝聚层次聚类的思想，合并较近的基本簇，得到最终的簇划分：

		&lt;ul&gt;
			&lt;li&gt;在所有簇中选择距离最近的两个簇进行合并，合并要求是：簇间距小于等于$dist_0$，如果所有簇中没有簇间距小于$dist_0$的时候，结束合并操作&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;处理剩余节点，归入最近的簇

		&lt;ul&gt;
			&lt;li&gt;最常用、最简单的方式是：将剩余样本对象归入到最近的簇。&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;MDCA算法名词解释&lt;/h4&gt;

&lt;p&gt;最大密度点：如字面意思，就是密度最大的点，密度计算公式一般取DBSCAN算法中的密度计算公式。&lt;/p&gt;

&lt;p&gt;有序序列$S_{p_{max}}$：根据所有对象与最大密度点的距离进行排序。&lt;/p&gt;

&lt;p&gt;密度阈值$density_0$：当节点的密度值大于密度阈值的时候，认为该节点属于一个 比较固定的簇，在第一次构建基本簇的时候，就将这些节点添加到对应簇中，如果小于这个值的时候，暂时认为该节点为噪声节点。&lt;/p&gt;

&lt;p&gt;簇间距离：对于两个簇C1和C2之间的距离，采用两个簇中最近两个节点之间的距离作为簇间距离。&lt;/p&gt;

&lt;p&gt;M值：初始簇中最多数据样本个数&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">聚类算法（下） 聚类算法上中讲了大名鼎鼎的K-Means算法及其优化变种，在这篇中几种讲述两位两种不同思路的聚类算法。 层聚类算法 传统层聚类算法—AGNES和DIANA算法 层次聚类和K-Means的思路不太一样，它的思路有点像是决策树，按照层次进行分解，知道满足某种条件为止，传统的层次聚类分为自底而上，和自上而下两类： 凝聚的层次聚类:</summary></entry><entry><title type="html">聚类算法（上）</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8A/" rel="alternate" type="text/html" title="聚类算法（上）" /><published>2018-08-19T00:00:00+08:00</published><updated>2018-08-19T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95(%E4%B8%8A)</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8A/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;聚类算法（上）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;p&gt;聚类算法很多，所以和讲回归算法一样，分成了上下，上中主要讲了传统的K-Means算法以及其相应的优化算法入K-Means++，K-Means||和Canopy等。下中主要讲了另外两种的思路的聚类算法，即层次聚类和密度聚类。&lt;/p&gt;
&lt;h2&gt;什么是聚类&lt;/h2&gt;
&lt;p&gt;聚类算就是怼大量未知标注的数据集，按照数据&lt;strong&gt;内部存在的数据特征&lt;/strong&gt;将数据集&lt;strong&gt;划分为多个不同的类别&lt;/strong&gt;，使类别内的数据比较相似，类别之间的数据相似度比较小，属于&lt;strong&gt;无监督学习&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;从定义就可以看出，聚类算法的关键在于计算样本之间的&lt;strong&gt;相似度&lt;/strong&gt;，也称为&lt;strong&gt;样本间的距离&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;相似度/距离计算公式&lt;/h2&gt;
&lt;p&gt;说到聚类算法，那肯定核心就是计算距离的公式了，目前常用的有以下几种。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;闵可夫斯基距离（Minkowski）&lt;/strong&gt;：公式2.1&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;当p为1的时候是曼哈顿距离（Manhattan）：公式2.2&lt;/li&gt;
	&lt;li&gt;当p为2的时候是欧式距离（Euclidean）：公式2.3

		&lt;ul&gt;
			&lt;li&gt;标准化欧式距离：

				&lt;p&gt;这个距离的计算方式如同其字面意思，标准化欧式距离就是对欧式距离的标准化。标准化的正常定义为，$X^* = \frac{X - \overline X}{s}$，这个$s$指的就是方差，而方差的计算公式为$s = \sqrt{\frac{\sum_{i=1}^n(x_i - \overline X)^2}{n}}$，所以其标准化公式如下公式2.5。&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;&lt;/li&gt;
	&lt;li&gt;当p为无穷大的以后是切比雪夫距离（Chebyshev）：公式2.4&lt;/li&gt;
&lt;/ul&gt;

$$
dist(X,Y)= \sqrt[p]{\sum_{i=1}^{n} |x_i - y_i|^p}\ \ \ 公式2.1
$$
$$
M\_dist=\sum_{i=1}^n|x_i-y_i| \ \ \ 公式2.2
$$
$$
E\_dist = \sqrt{\sum_{i=1}^n|x_i-y_i|^2} \ \ \ 公式2.3
$$
$$
C\_dist = max_i(|x_i-y_i|)\ \ \ 公式2.4
$$
$$
S\_E\_D = \sqrt{\sum_{i=1}^n(\frac{x_i-y_i}{s_i})^2}\ \ \ 公式2.5 
$$
&lt;p&gt;&lt;strong&gt;夹角余弦相似度（Cosine）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;使用这个公式的时候，需要注意的是，这里的相似之的是&lt;strong&gt;同一个方向上&lt;/strong&gt;的，而同一个方向上的两个点可能距离是非常远的。比如一个吻张灏总分别出现单词A 10次，单词B 20次，另一个文章中出现单词A 100次，单词B 200次，这时候如果使用欧几里得距离的话，这两个文章是不相似的，然而显然这两个单词的比例相似很能说明这两个文章其实是有关系的，所以在文章的相似度的判别中使用夹角余弦相似度比较合适，公式如下2.6。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;个人理解为，其是从距离以外的衡量相似度的另一个维度的指标&lt;/strong&gt;。&lt;/p&gt;

$$
\cos(\theta)  = \frac{\sum_{k=1}^n x_{1k}x_{2k}}{\sqrt{\sum_{k=1}^n x_{1k}^2} * \sqrt{\sum_{k=1}^n x_{2k}^2}} = \frac{a^T · b}{|a||b|}\ \ \ 公式2.6
$$
&lt;p&gt;&lt;strong&gt;KL距离（相对熵）&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;思考下条件熵的定义，简单的来说就是在放生一件事情的时候，发生另一件事的概率。公式如下公式2.7.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：这里书的概率不是实指概率，而是熵表达的含义。这个公式其实就是条件熵的公式。&lt;/em&gt;&lt;/p&gt;

$$
D(P|Q)=\sum_x P(x)\log(\frac{P(x)}{Q(x)})\ \ \ 公式2.7
$$
&lt;p&gt;&lt;strong&gt;杰卡德相似系数(Jaccard)&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;这个很好理解，它的核心就是使用两个集合的交集和并集的比率来代表两者的相似度，也就是说重合的越多越相似。公式如下，公式2.8.&lt;/p&gt;

$$
J(A,B) = \frac{|A\bigcap B|}{|A \bigcup B|}  
$$
$$
dist(A,B) = 1-J(A,B) \ \ \ 公式2.8 
$$

&lt;p&gt;&lt;strong&gt;Pearson相关系数&lt;/strong&gt;：&lt;/p&gt;

&lt;p&gt;这个就是考研数学中的相关系数，表达就是两者之间的想关系，所以直接拿来用就好了，公式如下公式2.9。&lt;/p&gt;

$$
\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{E[(X-E(X))(Y-E(Y))]}{\sqrt{D(X)}\sqrt{D(Y)}} = \frac{\sum_{i=1}^n(X_i - \mu_x)(Y_i - \mu_Y)}{\sqrt{\sum_{i=1}^n(X_i - \mu_X)^2}*\sqrt{\sum_{i=1}^n(Y_i - \mu_Y)^2}}  
$$
$$
dist(X,Y) = 1 - \rho_{XY}\ \ \ 公式2.9
$$

&lt;h2&gt;聚类的思想&lt;/h2&gt;

&lt;p&gt;给定一个有M个对象的数据集，构建一个具有k个簇的模型，其中k&amp;lt;=M。满足 以下条件:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;每个簇至少包含一个对象&lt;/li&gt;
	&lt;li&gt;每个对象属于且仅属于一个簇 &lt;/li&gt;
	&lt;li&gt;将满足上述条件的k个簇成为一个合理的聚类划分&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基本思想:&lt;/p&gt;

&lt;p&gt;对于给定的类别数目k，首先给定初始划分，通过迭代改变样本和簇的隶属关系，使的每次处理后得到的划分方式比上一次的好，即&lt;strong&gt;总的数据集之间的距离和变小了&lt;/strong&gt;&lt;/p&gt;

&lt;h2&gt;K-Means 系列&lt;/h2&gt;

&lt;h3&gt;K-Means算法&lt;/h3&gt;

&lt;p&gt;K-means的核心算法如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 假设输入样本T为x1,x2,x3,...,xm
&lt;/span&gt;
初始化k个类别的中心点a1,a2,a3,&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;...&lt;/span&gt;,ak
&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;not&lt;/span&gt; EndCondition :
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;1&lt;/span&gt;.根据每个样本和中心点的欧几里得距离，选择最近的中心点作为自己的类别
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;2&lt;/span&gt;.更新每个类别的中心点aj，为隶属该类别的所有的样本的均值

&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# EndCondition有迭代次数，最小平方误差MSE，簇中心点变化率。
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;再循环中的第二步，我们移动了中心点的位置，把中心点移到了隶属于该中心点类别的所有样本的中间，并使用样本的均值作为位置。这样子看似是拍脑袋想的移动策略，其实是可以推导出来的。正如聚类算法思想所指出的，我们要让所有的点到自己的分类的中心点的欧几里得距离最小，所以我们设置目标放称为公式4.1，公式中的1/2是为了之后求导运算方便。我们为了让目标函数尽可能的小，所以使用了之前一直在使用的思考方式，对其使用梯度下降算法，求导后得到公式4.2，之后令其等于0，就得到了公式4.3。&lt;/p&gt;

$$
J(a_1,a_2,a_3,...,a_k) = \frac{1}{2}\sum_{j=1}^K \sum_{i=1}^n (x_i - a_j)^2 \ \ \ 公式4.1
$$
$$
\frac{\partial J}{\partial a_j} = \sum_{i=1}^{N_j}(x_i-a_j)\ \ \ 公式4.2
$$

$$
a_j = \frac{1}{N}\sum_{i=1}^{N_j} x_i \ \ \ 公式4.3 
$$
&lt;p&gt;最后这个看似不错的算法，其实有着不小的缺点，那就是&lt;strong&gt;初值敏感&lt;/strong&gt;。我们来仔细想一想，如果两个不小心随机生成的初值落到了一个类别中，两者的距离还特别近，这中情况下就很难正确分类了。除此之外，由于移动策略中使用的是均值，也就是说如果集合中含有非常大的误差点的话，这样子会是中心点的设置偏离正确点很远，所以很多时候我们改用&lt;strong&gt;中值来更新中心点&lt;/strong&gt;，这就是我们说的K-Mediods聚类，即K中值聚类。&lt;/p&gt;

&lt;p&gt;总结下K-means算法&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;优点：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;理解容易，聚类效果不错&lt;/li&gt;
	&lt;li&gt;处理大数据集的时候，该算法可以保证较好的伸缩性和高效率&lt;/li&gt;
	&lt;li&gt;当簇近似高斯分布的时候，效果非常不错&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;缺点：&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt; K值是用户给定的，在进行数据处理前，K值是未知的，不同的K值得到的结果也不一样&lt;/li&gt;
	&lt;li&gt;对初始簇中心点是敏感的 &lt;/li&gt;
	&lt;li&gt;不适合发现非凸形状的簇或者大小差别较大的簇 &lt;/li&gt;
	&lt;li&gt;特殊值(离群值)对模型的影响比较大&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;二分K-Means算法&lt;/h3&gt;

&lt;p&gt;由于K-Means对初始中心点非常敏感，我们这里就尝试着通过二分法弱化初始中心点。这种算法的具体步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 把所有样本数据作为一个簇放到队列中
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;not&lt;/span&gt; EndCondition:
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;1&lt;/span&gt;.从队列中选择一个簇，使用K&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;-&lt;/span&gt;means划分为两个簇
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;2&lt;/span&gt;.将划分好的两个簇放回队列
&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# EndCondition 为簇的数量，最小平方误差，迭代次数等
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 选择簇的手段有两种1.使用SSE 2.选择数据量最多的簇
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们在这个算法中提到了SSE，这个可以是簇内所有样本点，到其中心点的距离的总和，代表着簇内的点是不是高度相关。计算公式如下公式4.4。&lt;/p&gt;

$$
SSE = \sum_{i=1}^n w_i(y_i - \hat y_i)^2\ \ \ 公式4.4
$$
&lt;p&gt;可以看出在这种算法下，很好的避开了，两个中心点都在一起的情况。&lt;/p&gt;

&lt;h3&gt;K-Means++和K-Means||&lt;/h3&gt;

&lt;p&gt;K-Means++做的改善，是直接对初始点的生成位置的选择进行优化的，他的初始点生成策略如下：&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;从数据集中任选一个节点作为第一个聚类中心&lt;/li&gt;
	&lt;li&gt;对数据集中的每个点x，计算x到所有已有聚类中心点的距离和D(X)，基于D(X)采用线性概 率选择出下一个聚类中心点(距离较远的一个点成为新增的一个聚类中心点)&lt;/li&gt;
	&lt;li&gt;重复步骤2直到找到k个聚类中心点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;可以看出，K-Means++企图生成相聚距离较远的几个中心点。但是缺点也是显而易见的，由于聚类中心点选择过程中的内在有序性，在扩展方面存在着性能方面的问题，即&lt;strong&gt;第k个聚类中心点的选择依赖前k-1个聚类中心点的值&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;而K-Means||就是针对K-Means++缺点作出了的优化，主要思路是改变每次遍历时候的取样规则，并非按照K-Means++算法每次遍历只获取一个样本，而是每次获取 K个样本，重复该取样操作$O(\log{n}w	z)$ 次，然后再将这些抽样出来的样本聚类出K个点，最后使用这K个点作为K-Means算法的初始聚簇中心点。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：一般5次重复采用就可以保证一个比较好的聚簇中心点。&lt;/em&gt;&lt;/p&gt;

&lt;h3&gt;Canopy算法&lt;/h3&gt;

&lt;p&gt;Canopy属于一种“粗略地”聚类算法，简单的来说就是，不那么追求自动获得最优解，而是引入了一种人为规定的先验值进行聚类，具体步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 给定样本列表L=x1,x,2...,xm以及先验值r1和r2(r1 &amp;gt; r2)
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;P&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;L&lt;/span&gt;:
	计算P到所有聚簇中心点的距离(如果不存在聚簇中心，那么此时点P形成一个新的聚簇)，并选择出最小距离D(&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;P&lt;/span&gt;,aj)
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;&amp;lt;&lt;/span&gt; r1:
		&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 表示该节点属于该聚簇
&lt;/span&gt;		添加到该聚簇列表中
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;&amp;lt;&lt;/span&gt; r2:
		&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 表示该节点不仅仅属于该聚簇，还表示和当前聚簇中心点非常近，
&lt;/span&gt;		将该聚簇的中心点设置为P，并将P从列表L中删除
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-constant&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;&amp;gt;&lt;/span&gt; r1:
		节点P形成一个新的聚簇
	&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; EndCondition:
		&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 结束条件为直到列表L中的元素数据不再有变化或者元素数量为0的时候
&lt;/span&gt;		&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;break&lt;/span&gt;		&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;注：Canopy算法得到的最终结果的值，聚簇之间是可能存在重叠的，但是不会存在 某个对象不属于任何聚簇的情况&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;显然，这种算法虽然快，但是很难生成满足我们应用的模型，所以通常我们将它作为解决K-Means初值敏感的方案，他们合在一起就是Canopy+K-Means算法。&lt;/p&gt;

&lt;p&gt;顺序就是先使用Canopy算法获得K个聚类中心，然后用这K个聚类中心作为K-Means算法。这样子就很好的解决了K-Means初值敏感的问题。&lt;/p&gt;

&lt;h3&gt;Mini Batch K-Means算法&lt;/h3&gt;

&lt;p&gt;Mini Batch K-Means算法是K-Means算法的一种优化变种，采用小规模的数据子集，来减少计算时间。其中采用小规模的数据子集指的是每次训练使用的数据集是在训练算法的时候随机抽取的数据子集。Mini Batch K-Means算法可以减少K-Means算法的收敛时间，而且产生的结果效果只是略差于标准K-Means算法。&lt;/p&gt;

&lt;p&gt;它的算法步骤如下：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# 首先抽取部分数据集，使用K-Means算法构建出K个聚簇点的模型
&lt;/span&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;not&lt;/span&gt; EndCondition:
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;1&lt;/span&gt;.抽取训练数据集中的部分数据集样本数据，并将其添加到模型中，分配给距离最近的聚簇中心点
	&lt;span class=&quot;syntax-all syntax-constant&quot;&gt;2&lt;/span&gt;.更新聚簇的中心点值
&lt;span class=&quot;syntax-all syntax-comment&quot;&gt;# EndCondtion同K-Means一样，可以理解为不停的进行K-Means算法。
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;聚类算法衡量标准&lt;/h2&gt;

&lt;p&gt;聚类算法的衡量标准有很多，包括均一性、完整性、V-measure、调整兰德系数（ARI ，Adjusted Rnd Index）、调整互信息(AMI，Adjusted Mutual Information)以及轮廓系数等等。&lt;/p&gt;

&lt;h3&gt;均一性、完整性以及V-measure&lt;/h3&gt;

&lt;p&gt;均一性：一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率，即每个聚簇中正确分类的样本数占该聚簇总样本数的比例和。其公式如下公式5.1。&lt;/p&gt;

$$
p = \frac{1}{k}\sum_{i=1}^k \frac{N(C_i == K_i)}{N(K_i)}\ \ \ 公式5.1  
$$
&lt;p&gt;完整性：同类别样本被归类到相同簇中，则满足完整性。每个聚簇中正确分类的样本数占该类型的总样本数比例的和，通俗的来说就是，我们已分类类别中，分类正确的个数。&lt;/p&gt;

&lt;p&gt;其公式如下，公式5.2：&lt;/p&gt;

$$
r = \frac{1}{k}\sum_{i=1}^k \frac{N(C_i == K_i)}{N(C_i)}\ \ \ 公式5.2 
$$
&lt;p&gt;在实际的情况中，均一性和完整性是往往不能兼得的，就好像抓特务时的矛盾一样，到底是保证每个抓的人都是特务，还是宁可错抓也不放过一个特务，之间的取舍很难把握。所以再一次贯彻，鱼和熊掌不可兼得，我们就加权，于是得到的就是V-measure，其公式如下公式5.3：&lt;/p&gt;

$$
V_\beta = \frac{(1+\beta^2)·pr}{\beta^2·p + r}\ \ \ 公式5.3  
$$
&lt;h3&gt;调整蓝德系数ARI&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;兰德系数（RI，Rand index）&lt;/strong&gt;，我用中文看了不少讲兰德系数的博客，其中的文字说明几乎都是相同的，对个人的理解帮助不是特别大，于是用英文查的。最终理解了这个系数的参数的意思，想看英文说明的，个人觉得还挺好懂的参考&lt;a href=&quot;https://en.wikipedia.org/wiki/Rand_index&quot;&gt;这里&lt;/a&gt;。以下是我个人的讲解。&lt;/p&gt;

&lt;p&gt;首先，将原数据集中的元素进行两两配对形成一个新的数据集，我们称之为S数据集。这时候，我们将原数据集，根据两种不同的策略分别划分成r份和s份，并对这两个数据集命名为X和Y。在这里我们可以看出，X和Y的元素是相同的，只是他们的划分方式不同。&lt;/p&gt;

&lt;p&gt;接下来我们来思考，S数据集中，每个元素中的两个样本，在X和Y中只有两种可能，就是两个样本都在一个子集中，或者不在一个子集中，那么对于S中的一个元素，只有四种可能性。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;两个样本都在X的一个子集中，也同时在Y的一个子集中，这些元素的个数是a&lt;/li&gt;
	&lt;li&gt;两个样本横跨X的不同子集，也同时在Y中横跨Y的不同子集，这些元素的个数是b&lt;/li&gt;
	&lt;li&gt;两个样本都在X的一个子集中，但在Y中横跨Y的不同子集，同理数量为c&lt;/li&gt;
	&lt;li&gt;两个样本横跨X的不同子集，但在Y的的一个子集中，同理数量为d&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;有了上述的理解，我们再看蓝得系数公式，公式5.4，我们就不难理解了。RI的取值在$[0,1]$之间，越靠近1代表越相似。&lt;/p&gt;

$$
RI = \frac{a+b}{a+b+c+d} = \frac{a+b}{C_2^n} \ \ \ 公式5.4  
$$
&lt;p&gt;接下来引入，&lt;strong&gt;调整兰德系数(ARI，Adjusted Rnd Index)&lt;/strong&gt;，ARI取值范围$[-1,1]$，值越大，表示聚类结果和真实情况越吻合。从广义的角度来将，ARI是衡量两个数据分布的吻合程度的，公式5.5如下：&lt;/p&gt;

$$
ARI = \frac{RI - E[RI]}{max(RI) - E[RI]}\ \ \ 公式5.5
$$
&lt;h3&gt;调整互信息(AMI，Adjusted Mutual Information)&lt;/h3&gt;

&lt;p&gt;调整互信息，整体的流程很像ARI，AMI则是对MI进行调整。而MI是使用信息熵来描述的。那么互信息表示了什么呢，首先先看下&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF&quot;&gt;维基百科的定义&lt;/a&gt;：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;独立的(H(X),H(Y)), 联合的(H(X,Y)), 以及一对带有互信息 I(X; Y) 的相互关联的子系统 X,Y 的条件熵。在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息是点间互信息（PMI）的期望值。互信息最常用的单位是bit。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;简单的来说，这个公式代表着两个子系统X和Y的相似度，但是这里的相似度是从信息熵的角度出发的，&lt;strong&gt;它越大代表着两者的差异越大&lt;/strong&gt;，其计算公式以及相关的公式如下公式5.6，公式5.7所示。&lt;/p&gt;

$$
MI(X;Y) = \sum_{y \in Y}\sum_{x \in X}p(x,y)\log{\frac{p(x,y)}{p(x)p(y)}}\ \ \ 公式5.6
$$
$$
\begin{split}
MI(X;Y) &amp;= H(X) - H(X|Y)\\
&amp;=H(Y) - H(Y|X)\\
&amp;=H(X) + H(Y) - H(X,Y)\\
&amp;=H(X,Y) - H(X|Y) - H(Y|X) \ \ \ 公式5.7
\end{split}
$$
&lt;h3&gt;轮廓系数&lt;/h3&gt;

&lt;p&gt;之前我们说到的衡量指标都是有标签的，这里的轮廓系数则是不包含标签的评价指标。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;簇内不相似度：&lt;/strong&gt;

		&lt;p&gt;计算样本i到同簇其它样本的平均距离为$a_i$ 。$a_i$越小，表示样本$i$越应该被聚类到该簇，而簇C中的所有样本的$a_i$的均值被称为簇C的簇不相似度。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;簇间不相似度：&lt;/strong&gt;

		&lt;p&gt;计算样本i到其它簇$C_j$ 的所有样本的平均距离bij， $b_i=min{bi1,bi2,...,bik}$ ，$b_i$ 越大，表示该样本$i$越不属于其它簇。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;轮廓系数：&lt;/strong&gt;

		&lt;p&gt;$s_i$值越接近1表示样本i聚类越合理，越接近-1，表示样本i应该分类到另外的簇中，近似为0，表示样本i应该在边界上。所有样本的si的均值被成为聚类结果的轮廓系数。&lt;/p&gt;

		$$
		s_i = \frac{b_i - a_i}{max\{a_i,b_i\}}
		$$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">聚类算法（上） 聚类算法很多，所以和讲回归算法一样，分成了上下，上中主要讲了传统的K-Means算法以及其相应的优化算法入K-Means++，K-Means||和Canopy等。下中主要讲了另外两种的思路的聚类算法，即层次聚类和密度聚类。 什么是聚类 聚类算就是怼大量未知标注的数据集，按照数据内部存在的数据特征将数据集划分为多个不同的类别，使类别内的数据比较相似，类别之间的数据相似度比较小，属于无监督学习。 从定义就可以看出，聚类算法的关键在于计算样本之间的相似度，也称为样本间的距离。 相似度/距离计算公式 说到聚类算法，那肯定核心就是计算距离的公式了，目前常用的有以下几种。 闵可夫斯基距离（Minkowski）：公式2.1</summary></entry><entry><title type="html">BaiduPCS-Go的使用</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/17/BaiduPCS-Go%E7%9A%84%E4%BD%BF%E7%94%A8/" rel="alternate" type="text/html" title="BaiduPCS-Go的使用" /><published>2018-08-17T00:00:00+08:00</published><updated>2018-08-17T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/17/BaiduPCS-Go%E7%9A%84%E4%BD%BF%E7%94%A8</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/17/BaiduPCS-Go%E7%9A%84%E4%BD%BF%E7%94%A8/">&lt;h2 id=&quot;why-baidupcs-go&quot;&gt;Why BaiduPCS-Go&lt;/h2&gt;
&lt;p&gt;BaiduPCS-Go是一个用Go语言编的命令行版的百度网盘，我们可以类比mas和Appstore的关系。那么为什么要用这样一个安装比较麻烦，还要记命令行的百度网盘的替代品，直接用百度网盘客户端不好么？
这还真的是不好，百度网盘在mac下是一个十足的阉割版，最常用的功能中，Mac版缺失了以下几种功能：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;没有分享功能&lt;/strong&gt;：mac下的客户端的分享功能居然是需要通过浏览器打开，太不优雅了。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;没有离线下载任务&lt;/strong&gt;：直接导致不能下载磁力链接。
如果你和我一样平时一样习惯终端操作，这个工具的学习成本超级低，同时它还有一定的提升下载速度的功效。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;使用指南&quot;&gt;使用指南&lt;/h2&gt;
&lt;h3 id=&quot;安装&quot;&gt;安装&lt;/h3&gt;
&lt;p&gt;Mac一般是预装了go的，如果没有的话，使用&lt;code class=&quot;highlighter-rouge&quot;&gt;brew install go&lt;/code&gt;来安装。除了go我们还需要安装git，同样使用&lt;code class=&quot;highlighter-rouge&quot;&gt;brew install git&lt;/code&gt;。在拥有了git和go以后，执行下面的指令即可。&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;go get -u -v github.com/iikira/BaiduPCS-Go
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;注：在安装途中，有提示说其安装到了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;~/go/bin&lt;/code&gt;的目录，也就是说这个工具的执行文件在&lt;code class=&quot;highlighter-rouge&quot;&gt;~/go/bin&lt;/code&gt;这个目录。
为了之后我们能够全局使用这个指令，于是我们将&lt;code class=&quot;highlighter-rouge&quot;&gt;export PATH=&quot;/Users/deamov/go/bin:$PATH&quot;&lt;/code&gt;添加到配置环境变量的文件中，如果没有使用zsh的话在&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;中，如果用的是zsh的话在&lt;code class=&quot;highlighter-rouge&quot;&gt;~/.zshrc&lt;/code&gt;中。
&lt;em&gt;注：deamov是我的电脑的用户名，至此安装便结束了。&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;常用操作说明&quot;&gt;常用操作说明&lt;/h3&gt;
&lt;h4 id=&quot;登陆&quot;&gt;登陆&lt;/h4&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;BaiduPCS-Go
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;简单一行指令就可以登录了，如果之前已经登陆过账号的话，现在就已经可以开始进行下载等操作了，如下效果图。
 &lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fucuk0k8hnj310c0lojst.jpg&quot; alt=&quot;&quot; /&gt;
第一次使用需要有登陆的操作，输入&lt;code class=&quot;highlighter-rouge&quot;&gt;login&lt;/code&gt;即可登陆，尊许提示依次输入账户和密码即可，如果需要验证码，则会输出一个链接，打开就可以看到验证码了。&lt;/p&gt;
&lt;h4 id=&quot;基本操作&quot;&gt;基本操作&lt;/h4&gt;
&lt;p&gt;基本的移动目录的方式和linux的操作一样，&lt;code class=&quot;highlighter-rouge&quot;&gt;ls&lt;/code&gt;是现实当前目录的文件，&lt;code class=&quot;highlighter-rouge&quot;&gt;rm&lt;/code&gt;是删除命令，&lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt;是切换目录，创建目录是&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir&lt;/code&gt;，拷贝是&lt;code class=&quot;highlighter-rouge&quot;&gt;cp&lt;/code&gt;，值得一提的是它支持Tab补全。和平时使用的终端命令不同的有如下几个指令。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;搜索：&lt;/strong&gt;平时我们使用的&lt;code class=&quot;highlighter-rouge&quot;&gt;grep&lt;/code&gt;在这里是不能使用的，我们用&lt;code class=&quot;highlighter-rouge&quot;&gt;search&lt;/code&gt;关键词来搜索。
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  search 关键字 &lt;span class=&quot;c&quot;&gt;# 搜索当前工作目录的文件&lt;/span&gt;
  search -path&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/ 关键字 &lt;span class=&quot;c&quot;&gt;# 搜索根目录的文件&lt;/span&gt;
  search -r 关键字	&lt;span class=&quot;c&quot;&gt;# 递归搜索当前工作目录的文件 &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;下载：&lt;/strong&gt;记住是download就好啦
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  BaiduPCS-Go download &amp;lt;网盘文件或目录的路径1&amp;gt; 
  BaiduPCS-Go d &amp;lt;网盘文件或目录的路径1&amp;gt; &amp;lt;文件或目录2&amp;gt; &amp;lt;文件或目录3&amp;gt; ...
  &lt;span class=&quot;c&quot;&gt;# 当然支持多文件下载咯，下载目录默认在~/Download文件夹中&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;离线下载: &lt;/strong&gt;支持http/https/ftp/电驴/磁力链协议
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;c&quot;&gt;# 将百度和腾讯主页, 离线下载到根目录 /&lt;/span&gt;
  offlinedl add -path&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/ http://baidu.com http://qq.com

  &lt;span class=&quot;c&quot;&gt;# 添加磁力链接任务&lt;/span&gt;
  offlinedl add magnet:?xt&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;urn:btih:xxx

  &lt;span class=&quot;c&quot;&gt;# 查询任务ID为 12345 的离线下载任务状态&lt;/span&gt;
  offlinedl query 12345

  &lt;span class=&quot;c&quot;&gt;# 取消任务ID为 12345 的离线下载任务&lt;/span&gt;
  offlinedl cancel 12345 
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;h4 id=&quot;分享share&quot;&gt;分享share&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;查看分享内容&lt;/strong&gt;
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  share list
  share l
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;取消分享&lt;/strong&gt;
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  share cancel &amp;lt;shareid_1&amp;gt;
  share c &amp;lt;shareid_1&amp;gt;
  &lt;span class=&quot;c&quot;&gt;# 遗憾的是只能支持通过shareid来取消分享&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;h4 id=&quot;上传同名文件会被覆盖&quot;&gt;上传：同名文件会被覆盖&lt;/h4&gt;
    &lt;p&gt;&lt;em&gt;注：需要退出BaiduPCS-Go使用，否则本地文件目录不能自动补全&lt;/em&gt;&lt;/p&gt;
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$BaiduPCS&lt;/span&gt;-Go upload &amp;lt;本地文件/目录的路径1&amp;gt; &amp;lt;文件/目录2&amp;gt; &amp;lt;文件/目录3&amp;gt; ... &amp;lt;目标目录&amp;gt;
&lt;span class=&quot;nv&quot;&gt;$BaiduPCS&lt;/span&gt;-Go u &amp;lt;本地文件/目录的路径1&amp;gt; &amp;lt;文件/目录2&amp;gt; &amp;lt;文件/目录3&amp;gt; ... &amp;lt;目标目录&amp;gt;
&lt;span class=&quot;c&quot;&gt;# Example&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$BaiduPCS&lt;/span&gt;-Go upload ~/Downloads/1.mp4 /Video
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;h3 id=&quot;其他&quot;&gt;其他&lt;/h3&gt;
    &lt;p&gt;这个工具很强大，还可以通过设置下载线程数等等操作来提升下载速度，更多详细的操作请参考它的&lt;a href=&quot;https://github.com/iikira/BaiduPCS-Go&quot;&gt;官网&lt;/a&gt;。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>DeamoV</name></author><category term="百度网盘" /><category term="下载" /><category term="教程" /><summary type="html">Why BaiduPCS-Go BaiduPCS-Go是一个用Go语言编的命令行版的百度网盘，我们可以类比mas和Appstore的关系。那么为什么要用这样一个安装比较麻烦，还要记命令行的百度网盘的替代品，直接用百度网盘客户端不好么？ 这还真的是不好，百度网盘在mac下是一个十足的阉割版，最常用的功能中，Mac版缺失了以下几种功能： 没有分享功能：mac下的客户端的分享功能居然是需要通过浏览器打开，太不优雅了。 没有离线下载任务：直接导致不能下载磁力链接。 如果你和我一样平时一样习惯终端操作，这个工具的学习成本超级低，同时它还有一定的提升下载速度的功效。</summary></entry><entry><title type="html">Tobias的小粉丝在此</title><link href="http://localhost:4000/%E6%9D%82%E9%A1%B9/2018/08/16/Tobias%E5%B0%8F%E8%BF%B7%E5%BC%9F%E5%9C%A8%E6%AD%A4/" rel="alternate" type="text/html" title="Tobias的小粉丝在此" /><published>2018-08-16T00:00:00+08:00</published><updated>2018-08-16T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9D%82%E9%A1%B9/2018/08/16/Tobias%E5%B0%8F%E8%BF%B7%E5%BC%9F%E5%9C%A8%E6%AD%A4</id><content type="html" xml:base="http://localhost:4000/%E6%9D%82%E9%A1%B9/2018/08/16/Tobias%E5%B0%8F%E8%BF%B7%E5%BC%9F%E5%9C%A8%E6%AD%A4/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;Tobias的小粉丝在此&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;序言&lt;/h2&gt;
&lt;p&gt;最近迷上了吉他，当然不是指那种一周速成的把妹弹唱啦。为了防止大家对吉他有一种特别简单，把妹专用道具的奇怪印象。特别提一个小知识，古典吉他在世界公认的十大难学的乐器中排第三，顺便一提，钢琴排第五。&lt;/p&gt;

&lt;h2&gt;正文&lt;/h2&gt;

&lt;p&gt;不过，非常遗憾最近喜欢上的是并不是尼龙弦的古典吉他，而是它的另一个兄弟指弹吉他中的Percussive Fingerstyle。Percussive FingerStyle起源于80到90年代，它通过快速击打琴弦，琴身，以及琴的边缘制造装饰音。现在比较有名的几个这个风格的吉他手有Andy McKee和Tommy Emmanuel，而Tobias Rauscher也是这个领域的新秀，也是目前我最喜欢的吉他手之一。值得一提的是，他是14岁开始自学的吉他，开始的时候主要弹奏摇滚和重金属，知道2010年终于开始了Solo Acoustic Guitar的道路。和相似的同时用多个吉他，甚至奇怪形状吉他的Luca Stricagnoli不同，他只使用一把吉他，视觉效果上没有了那份笨重，同时Tobias的笑容也能让人感受到他对于吉他的热爱。有兴趣的小伙伴可以在网易云搜他的曲子。惭愧的说，第一次听他的曲子也是在网易云里面听到了，结果整整单曲循环了好几天（笑。&lt;/p&gt;

&lt;h2&gt;其他&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;www.tobiasguitar.com&quot;&gt;Tobias的官网在此&lt;/a&gt;&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="FingerStyle" /><category term="Guitar" /><summary type="html">Tobias的小粉丝在此 序言 最近迷上了吉他，当然不是指那种一周速成的把妹弹唱啦。为了防止大家对吉他有一种特别简单，把妹专用道具的奇怪印象。特别提一个小知识，古典吉他在世界公认的十大难学的乐器中排第三，顺便一提，钢琴排第五。</summary></entry><entry><title type="html">支持向量机SVM</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/" rel="alternate" type="text/html" title="支持向量机SVM" /><published>2018-08-13T00:00:00+08:00</published><updated>2018-08-13T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;css/style.css&quot; /&gt;
		&lt;title&gt;支持向量机（Support Vector Machine）&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;SVM的优点和缺点&lt;/h2&gt;
&lt;ul&gt;
	&lt;li&gt;优点：泛化错误率低，计算开销不大，结果易解释&lt;/li&gt;
	&lt;li&gt;缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。&lt;/li&gt;
	&lt;li&gt;数据类型：数值型和标称型数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;SVM算法原理&lt;/h2&gt;

&lt;p&gt;首先我们都知道，为了划分二维的数据需要一根线，划分三维数据需要一个面。这里线是一维，面是二维，同理继续推出对N维的数据需要使用N-1维的对象进行分割，线性回归和SVM本质都是通过找这个超平面来达到分类的效果。&lt;/p&gt;

&lt;p&gt;具体的来说SVM是在优化线性回归中的$kx+b$模型。在线性回归中只需要考虑有一个分割超平面能进行分类即可，而SVM则想找出所有能分类的分割超平面中最优的超平面，即&lt;strong&gt;所有点都到分割超平面的距离最大&lt;/strong&gt;，而支持向量指的就是离超平面最近的那些点。&lt;/p&gt;

&lt;p&gt;超平面的公式为公式2.1。所以点A到分割超平面的距离为公式2.2。这里我们为了方便计算引入类别标签为-1和+1。所以保证所有的最小间隔的点最大化的公式为公式2.3。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1：-1和+1是为了保证预测正确的时候，$y(x_i)*label_i$都是一个很大的正值。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2：$arg\ max_{w,b}$的含义是，得到w和b使得后面式子取最大值&lt;/em&gt;&lt;/p&gt;

$$
y(x) = w^TX+b\ \ \ 公式2.1
$$

$$
\frac{|w^TX+b|}{||w||}\ \ \ 公式2.2
$$

$$
arg\ max_{w,b}\{min_i(label_i*(w^Tx_i+b)*\frac{1}{||w||})\}\ \ \ 公式2.3
$$
&lt;p&gt;显然我们不能直接求解上面的式子。需要化简下它。由于公式2.1在正确预测时，同$label$的乘积大于1。所以我们可以拆分公式2.3为公式2.4，约束条件为公式2.5。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：这里的约束条件公式2.5中，要对每一个式子前都要加系数，即拉格朗日数乘子$\alpha_i$&lt;/em&gt;。&lt;/p&gt;

$$
arg\ min_{w,b}\ ||w||\ \ \ 公式2.4
$$
$$
st.\ \ label_i*(w^Tx_i+b) \geq 1 \ \ \ 公式2.5
$$
&lt;p&gt;对为了方便求导计算在公式2.4前加上$\frac{1}{2}$这个系数。之后使用拉格朗日乘子法得到公式2.6，并进行计算。根据KKT条件中的偏导数等于0得到公式2.7和公式2.8。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;祝：这里需要注意的是拉格朗日数乘子的正负号，这个同不等式的符号有关&lt;/em&gt;&lt;/p&gt;

$$
L(w,b,\alpha)= \frac{1}{2}||w||^2-\sum_{i=1}^n\alpha_i*[label_i*(w^Tx_i+b)-1]\ \ \ 公式2.6
$$
$$
\sum_{i=1}^{n}\alpha_i label_i x_i = w\ \ \ 公式2.7
$$
$$
\sum_{i=1}^{n}\alpha_i label_i = 0\ \ \ 公式2.8
$$
&lt;p&gt;将公式2.7，公式2.8代入公式2.6化简，再根据对偶问题得到最终公式2.9，根据KKT，其约束条件为公式2.10。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注1：KKT条件在SMO算法中统一进行讲解。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注2：b是由公式2.8消掉的。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注3：在拉格朗日乘子法应用在这里，我们可以把$||w||$，写作$max_\alpha\ L(w,b,\alpha)$，所以原式可以写作$min_{w,b}\ max_\alpha\ L(w,b,\alpha)$，根据对偶问题，就可以变成$max_\alpha\ min_{w,b}\ L(w,b,\alpha)$，这也是能把公式2.7和公式2.8代入公式2.6的原因，也是公式2.9种是$max_\alpha$的原因。具体证明在KKT中的附上的博客中。&lt;/em&gt;&lt;/p&gt;

$$
max_\alpha\ \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{m}label_i*label_j*a_i*a_j\langle x_i·x_j\rangle\ \ \ 公式2.9
$$

$$
\alpha_i \geq 0\ \  且\ \sum_{i=1}^{m}\alpha_i*label_i = 0\ \ \ 公式2.10
$$
&lt;p&gt;&lt;em&gt;注：这里$\langle x_i·x_j\rangle$是两者向量积的运算，是从$x_i^T*x_j$得到的。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;这么看来我们算出了$\alpha$就能算出超平面，所以SVM的工作就是算出这些$\alpha$，而SMO算法就是求$\alpha$的典型算法。&lt;/p&gt;

&lt;h2&gt;对SVM引入线性不可分&lt;/h2&gt;

&lt;p&gt;由于数据都不那么干净，所以我们不应该假设数据能够100%的线性可分。我们通过对判定的公式，公式2.5，引入松弛变量$\xi_i\geq 0$，得到其引入了松弛因子的形式，如下公式3.1。&lt;/p&gt;

$$
y_i(w*x_i+b)\geq1-\xi_i\ \ \ 公式3.1
$$
&lt;p&gt;同时对于目标函数公式2.4也需要调整，我们将$\xi$引入目标函数并对其设置权值，得到公式3.2，也因此其约束条件变为公式3.1，公式3.2。&lt;/p&gt;

$$
min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i\ \ \ 公式3.2
$$
$$
\begin{split}
st.\ \ \ &amp;y_i(w*x_i+b)\geq 1 - \xi_i\\
&amp;\xi \geq 0
\end{split}
$$
&lt;p&gt;故拉格朗日函数$L(w,b,\xi,\alpha,\mu)$为如下公式3.3，其中$\alpha$，$\mu$为拉格朗日数乘子。&lt;/p&gt;

$$
L(w,b,\xi,\alpha,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^n\alpha_i*[label_i*(w^Tx_i+b)-1+\xi_i]-\sum_{i=1}^n\mu_i\xi_i\ \ \ 公式3.3
$$
&lt;p&gt;和之前的操作一样，对其进行求偏导操作后，类似的得到了相同的公式2.7，公式2.8，不同的是这里对$\xi$的求到后对$\alpha$有了限制，得到了公式3.4，由于$\mu\geq0$所以有$\alpha_i$的取值范围$0 \leq \alpha_i \leq C$。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;注：注意这里的$\alpha$取值，之后SMO会用&lt;/em&gt;&lt;/p&gt;

$$
C-\alpha_i-\mu_i = 0\ \ \ 公式3.4  
$$
&lt;p&gt;最终目标函数还是同之前推到的相同，即公式2.9，约束条件中只有$\alpha$的取值变为了，$0 \leq \alpha_i \leq C$。这样有了目标函数了以后，之后可以根据梯度下降算法求得最终的$\alpha$&lt;/p&gt;

&lt;h2&gt;SMO（Sequential Minimal Optimization）&lt;/h2&gt;

&lt;h3&gt;KKT条件 &lt;/h3&gt;

&lt;p&gt;求$f(x)$ 极值的时候我们这里讨论三种情况。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;没有约束条件：&lt;/li&gt;
	&lt;li&gt;有一个等式$h(x)$的约束条件：

		&lt;p&gt;使用拉格朗日乘子法（Lagrange Multiplier），也就是我们在高数中求极值常用的。设置一个拉格朗日系数$\alpha_1$，得到如下公式，之后对$x$和$\alpha_1$用求导的方式求极值即可。&lt;/p&gt;

		$$
		L(x, \alpha) = f(x) + \alpha*h(x)
		$$&lt;/li&gt;
	&lt;li&gt;含有不等式的约束条件：

		&lt;p&gt;当约束条件中有不等式时，就需要用到KKT条件。同样地，把所有的不等式约束$g(x)\leq0$、等式约束$h(x)=0$和目标函数$f(x)$全部写为一个式子如下公式。&lt;/p&gt;

		$$
		L(x,\alpha_1, \alpha_2)= f(x) + \alpha_1*g(x)+\alpha_2*h(x)
		$$
		&lt;p&gt;KKT条件是说最优值必须满足以下条件：&lt;/p&gt;

		&lt;ol&gt;
			&lt;li&gt;$L(x, \alpha) = f(x) + \alpha(x)$ 对$x$，$\alpha_1$，$\alpha_2$求导为零。&lt;/li&gt;
			&lt;li&gt;$h(x)=0$ 。&lt;/li&gt;
			&lt;li&gt;$g(x)*\alpha_1=0$。&lt;/li&gt;
		&lt;/ol&gt;

		&lt;p&gt;其中第三个式子非常有趣，因为$g(x)\leq$ 0 ，&lt;strong&gt;如果要满足这个等式，必须有$a = 0$或者$g(x) = 0$&lt;/strong&gt;。这是SVM的很多重要性质的来源。同时$f(x)$也可以写作$max_{\alpha_1,\alpha_2}\ L(x,\alpha_1,\alpha_2)$，这个则是SMO求解中的一个关键性质。详细的论述参考&lt;a href=&quot;https://blog.csdn.net/xianlingmao/article/details/7919597&quot;&gt;这篇博客&lt;/a&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;SMO算法细节&lt;/h3&gt;

&lt;h4&gt;SMO算法综述&lt;/h4&gt;

&lt;p&gt;由于原来直接通过梯度下降进行求解速度太慢，所以1996年，John Platt依靠KKT的特性，将大优化问题变成了多个小优化问题来求解，成为了SVM中最常用的求解思路。&lt;/p&gt;

&lt;p&gt;其思路如下：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Loop：

		&lt;ol&gt;
			&lt;li&gt;选取一对 $\alpha_i$，$\alpha_j$作为变量，其余看为常数&lt;/li&gt;
			&lt;li&gt;如果这对$\alpha$满足以下两个条件，使用梯度下降算法改变他们的值。

				&lt;ul&gt;
					&lt;li&gt;两者都在间隔边界外&lt;/li&gt;
					&lt;li&gt;两者都没有在进行过区间化处理，或者不在边界上&lt;/li&gt;
				&lt;/ul&gt;&lt;/li&gt;
			&lt;li&gt;当满足了KKT条件，即$\sum_{i=1}^N\alpha_iy_i=0$和$0\leq \alpha_i \leq C$。&lt;/li&gt;
		&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;注：这里可以这么理解，$\alpha_i$从之前的公式中我们可以大致理解为是每一个样本的权值，我们这些操作可以理解为通过操作$\alpha$把所有的样本点尽量的放在间隔边界上。&lt;/em&gt;&lt;/p&gt;

&lt;h4&gt;算法推导&lt;/h4&gt;

&lt;p&gt;我们接下来要做的是，通过类似梯度下降的方式来求的最优的$\alpha$值。正如上一节所说的，SMO的本质是大优化问题画小优化问题。所以从目标函数公式2.9中，随意取出两个$\alpha$，为了表达方便，不妨直接取$\alpha_1$和$\alpha_2$，同时对公式2.9前加负号取反之后，化简如下式4.1，其中$\kappa_{ij}$代表$\langle x_i·x_j\rangle$。&lt;/p&gt;

$$
\begin{split}
min_{\alpha_1, \alpha_2}W(\alpha_1,\alpha_2) &amp;=\frac{1}{2}\kappa_{11}\alpha_1^2+\frac{1}{2}\kappa_{22}\alpha_2^2+y_1y_2\alpha_1\alpha_2\kappa_{12}-(\alpha_1+\alpha_2)\\
&amp;+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_i\kappa_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_i\kappa_{i2}\ \ \ 公式4.1\\
\end{split} 
$$
$$
\begin{split}
st. \ \ &amp;\alpha_1y_1+\alpha_2y_2=-\sum_{i=3}^Ny_i\alpha_i\\
&amp;0\leq\alpha_i\leq C
\end{split}
$$
&lt;p&gt;由于我们已经把不是$\alpha_1$和$\alpha_2$的参数看作常量，所以在进行求偏导进行梯度下降算法的时候就不需要考虑公式4.1中第二行的式子。通过这个式子中约束条件的等式就可以得到仅含$\alpha_j$的式子，对其进行梯度下降算法，得到如下公式4.2：&lt;/p&gt;

$$
g(x)=\sum_{i=1}^Ny_i\alpha_i\kappa(x_i,x)+b  
$$
$$
\eta = \kappa_{11}+\kappa_{22}-2\kappa_{12} = ||x_1-x2||^2
$$
$$
E_i = g(x_i)-y_i = (\sum_{j=1}^Ny_j\alpha_j\kappa_{ji}+b)-y_i
$$
$$
\alpha_i = \frac{\xi-\alpha_j y_j}{y_i}
$$
$$
\alpha_j^{new}=\alpha_j^{old}+\frac{y_j(E_i-E_j)}{\eta}\ \ \ 公式4.2
$$
&lt;p&gt;这时候我们已经找到了两个的$alpha$的新值了，不过我们不能确定这两个新值是否还满足KKT条件。所以我们根据KKT条件中$\alpha$的取值，设置了L和H来防止新值不满足KKT，即$L\leq\alpha_i,\alpha_j \leq H$，其中L，H的公式如下公式4.3和公式4.4得到：&lt;/p&gt;

$$
if\ y_i \neq  y_j\ \ \ L=max(0,\alpha_j-\alpha_i),\ H=min(C,C+\alpha_j-\alpha_i)
$$
$$
if\ y_i = y_j\ \ \ L=max(0,\alpha_j+\alpha_i-C),\ H=min(C,\alpha_j+\alpha_i)
$$
&lt;p&gt;LH的细节推到，在&lt;a href=&quot;https://blog.csdn.net/hjimce/article/details/45421827&quot;&gt;这个博客&lt;/a&gt;中详细的说明了LH是怎么推出来的。&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">支持向量机（Support Vector Machine） SVM的优点和缺点 优点：泛化错误率低，计算开销不大，结果易解释 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。 数据类型：数值型和标称型数据。</summary></entry><entry><title type="html">使用Github创建自己的小博客</title><link href="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/12/%E4%BD%BF%E7%94%A8Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E5%8D%9A%E5%AE%A2/" rel="alternate" type="text/html" title="使用Github创建自己的小博客" /><published>2018-08-12T00:00:00+08:00</published><updated>2018-08-12T00:00:00+08:00</updated><id>http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/12/%E4%BD%BF%E7%94%A8Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E5%8D%9A%E5%AE%A2</id><content type="html" xml:base="http://localhost:4000/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/12/%E4%BD%BF%E7%94%A8Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E5%8D%9A%E5%AE%A2/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;使用Github创建自己的小博客&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;懒人攻略&lt;/h2&gt;
&lt;p&gt;只有四步：&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;找到自己喜欢的别人的博客的Github地址，一般为&lt;code&gt;username.github.io&lt;/code&gt;结尾。&lt;/li&gt;
	&lt;li&gt;Fork一份对方的源码，之后把仓库名改为&lt;code&gt;YourGithubName.github.io&lt;/code&gt;&lt;/li&gt;
	&lt;li&gt;在&lt;code&gt;_config.yaml&lt;/code&gt;中更改个人信息，同时把&lt;code&gt;_posts&lt;/code&gt;中的文章都删了，注意别人的文章格式，之后仿照对方的格式写即可。&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;给你Fork的原作者写封邮件表达感谢！&lt;/strong&gt;说不定就这么勾搭了一个大佬也不一定呢。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;完成了四步后，浏览器输入&lt;code&gt;YourGithubName.github.io&lt;/code&gt;就能在晚上看到自己的博客啦。&lt;/p&gt;

&lt;h2&gt;折腾攻略&lt;/h2&gt;

&lt;p&gt;本这不重新造轮子的原则，附上我参考的大佬们的文章。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;搭建篇：

		&lt;p&gt;简书上chaosinmotion 的&lt;a href=&quot;https://www.jianshu.com/p/7593508666f8&quot;&gt; Github Pages + Jekyll 独立博客一小时快速搭建&amp;amp;上线指南 &lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;添加评论系统：

		&lt;p&gt;Github上knightcai的&lt;a href=&quot;https://knightcai.github.io/2017/12/19/%E4%B8%BA%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0-Gitalk-%E8%AF%84%E8%AE%BA%E6%8F%92%E4%BB%B6/&quot;&gt; 为博客添加 Gitalk 评论插件 &lt;/a&gt;&lt;/p&gt;

		&lt;p&gt;特别一提，如果出现Validation Error是因为博客标题的名字编码后太长了，参考&lt;a href=&quot;https://github.com/gitalk/gitalk/issues/102&quot;&gt;这个Issue&lt;/a&gt;中mr-wind的使用 &lt;code&gt;id: decodeURI(location.pathname)&lt;/code&gt; 解决方案。&lt;/p&gt;

		&lt;p&gt;&lt;strong&gt;注：md5的方案可能更好，偷懒起见我没有用。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;阅读量统计：

		&lt;p&gt;wanghao的&lt;a href=&quot;https://notes.wanghao.work/2015-10-21-%E4%B8%BANexT%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%96%87%E7%AB%A0%E9%98%85%E8%AF%BB%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%8A%9F%E8%83%BD.html#%E9%85%8D%E7%BD%AELeanCloud&quot;&gt; 为NexT主题添加文章阅读量统计功能 &lt;/a&gt;，这个文章用的是leandCloud。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;搜索服务：

		&lt;p&gt;使用Algolia，不过自带的LocalSearch比较简单。&lt;a href=&quot;http://theme-next.iissnan.com/third-party-services.html#algolia-search&quot;&gt;文章&lt;/a&gt;有配置说明。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;主题：

		&lt;p&gt;Next系列。&lt;a href=&quot;http://theme-next.iissnan.com/getting-started.html&quot;&gt;官网&lt;/a&gt;有安装手册。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;CopyRight:

		&lt;p&gt;在目录下搜索copyright，找到那个html文件进行修改就好了。效果是文章下面的红竖杠中的内容。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;小彩蛋：

		&lt;p&gt;史蒂芬小恐龙，他的js文件在&lt;a href=&quot;https://github.com/lmk123/t-rex-runner&quot;&gt;这里&lt;/a&gt;！之后就任君发挥啦，Happy Coding。&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;最后题外话

		&lt;p&gt;所有的配置基本上都可以在&lt;code&gt;_config.yaml&lt;/code&gt;中设置，同时在博客中&lt;code&gt;\&lt;/code&gt;代表的就是根目录，这样子你自己在配置其他的功能的时候就可以轻松愉悦的配置。值得一提的是css文件和js文件都在&lt;code&gt;assets&lt;/code&gt;文件夹中，自己DIY的时候最好不要打乱目录结构。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="GithubPage" /><category term="博客" /><category term="教程" /><summary type="html">使用Github创建自己的小博客 懒人攻略 只有四步： 找到自己喜欢的别人的博客的Github地址，一般为username.github.io结尾。 Fork一份对方的源码，之后把仓库名改为YourGithubName.github.io 在_config.yaml中更改个人信息，同时把_posts中的文章都删了，注意别人的文章格式，之后仿照对方的格式写即可。 给你Fork的原作者写封邮件表达感谢！说不定就这么勾搭了一个大佬也不一定呢。</summary></entry><entry><title type="html">决策树优化策略</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/10/%E5%86%B3%E7%AD%96%E6%A0%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/" rel="alternate" type="text/html" title="决策树优化策略" /><published>2018-08-10T00:00:00+08:00</published><updated>2018-08-10T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/10/%E5%86%B3%E7%AD%96%E6%A0%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/10/%E5%86%B3%E7%AD%96%E6%A0%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;决策树优化策略&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;剪枝优化&lt;/h2&gt;
&lt;p&gt;决策树的剪枝是决策树算法中最基本、最有用的一种优化方案，分为以下两类：&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;strong&gt;前置剪枝：&lt;/strong&gt;在构建决策树的过程中，提前停止。&lt;strong&gt;这种策略无法得到比较好的结果&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;后置剪枝：&lt;/strong&gt;在决策树构建好后，然后开始剪裁，一般使用两种方案。a）用单一叶子结点代替整个子树，也节点的分类采用子树中最主要的分类。b）将一个子树完全替代另一个子树。后置剪枝的主要问题是存在计算效率问题，存在一定的浪费情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;后置剪枝&lt;/h3&gt;

&lt;p&gt;后置剪枝的核心思想其实就是交叉验证，其通过对完全树进行剪枝，一直剪到只剩下树根，这样子便得到许多树，然后通过使用数据集分别对他们验证，然后根据结果选择最优树。&lt;/p&gt;

&lt;h3&gt;决策树剪枝过程&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;code-highlighted code-python&quot;&gt;&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;while&lt;/span&gt; 生成的决策树不为1个节点:
	计算所有内部非叶子节点的剪枝系数;
	选择最小剪枝系数的节点:
		&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;if&lt;/span&gt; 有多个最小剪枝系数节点:
			选择包含数据项多的节点删除
		&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;else&lt;/span&gt;:
			删除节点
		将剪枝后的树存入之后用的决策树集
&lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;for&lt;/span&gt; 决策树 &lt;span class=&quot;syntax-all syntax-keyword&quot;&gt;in&lt;/span&gt; 决策树集:
	用数据集验证决策树，得到最优剪枝后的决策树
	&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其最后用于验证决策树的损失函数如下公式1.1：&lt;/p&gt;

$$
loss = \sum_{t=1}^{leaf} \frac{D_t}{D}H(t)\ \ \  公式1.1
$$
&lt;p&gt;剪枝系数的引入：&lt;/p&gt;

&lt;p&gt;剪枝系数的目的为，&lt;strong&gt;平衡准确度和树的节点数量之间的关系&lt;/strong&gt;。所以很自然的想到我们常用的处理手法，在损失函数中引入叶子结点的变量，得到公式1.2。&lt;/p&gt;

$$
loss_{\alpha} = loss + \alpha*leaf\ \ \ \ 公式1.2
$$
&lt;p&gt;设剪枝前的损失函数为$loss(R)$，剪枝后的损失函数为$loss(r)$，由于我们是想让剪枝前后的准确率尽量不变，所以让剪枝前后的损失函数相等，化简得公式1.3，即剪枝系数。&lt;strong&gt;注：剪枝后为根节点，所以$r=1$&lt;/strong&gt;。&lt;/p&gt;

$$
\alpha = \frac{loss(r)-loss(R)}{R_{leaf}-1}\ \ \ \ 	公式1.3
$$
&lt;p&gt;最后，由于我们想尽量减去的叶子结点多点，又同时保持准确度，故剪枝系数越小越好。&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">决策树优化策略 剪枝优化 决策树的剪枝是决策树算法中最基本、最有用的一种优化方案，分为以下两类： 前置剪枝：在构建决策树的过程中，提前停止。这种策略无法得到比较好的结果 后置剪枝：在决策树构建好后，然后开始剪裁，一般使用两种方案。a）用单一叶子结点代替整个子树，也节点的分类采用子树中最主要的分类。b）将一个子树完全替代另一个子树。后置剪枝的主要问题是存在计算效率问题，存在一定的浪费情况。</summary></entry><entry><title type="html">决策树</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/09/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="alternate" type="text/html" title="决策树" /><published>2018-08-09T00:00:00+08:00</published><updated>2018-08-09T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/09/%E5%86%B3%E7%AD%96%E6%A0%91</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/09/%E5%86%B3%E7%AD%96%E6%A0%91/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;决策树&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h3&gt;信息熵&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;信息量:&lt;/strong&gt;指的是一个样本/事件所蕴含的信息，如果一个事件的概率越大，那么就 可以认为该事件所蕴含的信息越少。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;信息熵:&lt;/strong&gt;一个系统越是有序， 信息熵就越低，一个系统越是混乱，信息熵就越高，所以 信息熵被认为是一个系统有序程度的度量。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;信息熵就是用来描述系统信息量的不确定度。&lt;/strong&gt;&lt;/p&gt;

$$
H(x) = - \sum_{i=1}^m p_i log_2(p_i)
$$
&lt;p&gt;&lt;strong&gt;条件熵：&lt;/strong&gt; 给定条件X的情况下，所有不同x值情况下Y的信息熵的平均值叫做条件熵。&lt;/p&gt;

$$
H(Y|X) = \sum_{j=1}P(X = v_j)H(Y|X = v_j)
$$
$$
H(Y|X) = H(X,Y)-H(X)
$$
&lt;h3&gt;决策树定义&lt;/h3&gt;

&lt;p&gt;决策树(Decision Tree)是在已知各种情况发生概率的基础上，通过构建决策树来 进行分析的一种方式，是一种直观应用概率分析的一种图解法。决策树是一种预测模型，代表的是对象属性与对象值之间的映射关系。决策树是一种树形结构， 其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节 点代表一种类别;决策树是一种非常常用的有监督的分类算法。&lt;/p&gt;

&lt;h3&gt;构建过程&lt;/h3&gt;

&lt;p&gt;构建步骤如下:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;将所有的特征看成一个一个的节点;&lt;/li&gt;
	&lt;li&gt;遍历每个特征的每一种分割方式，找到最好的分割点;将数据划分为不同的子节点，eg: N1、 N2....Nm;计算划分之后所有子节点的&amp;#39;纯度&amp;#39;信息;&lt;/li&gt;
	&lt;li&gt;对第二步产生的分割，选择出最优的特征以及最优的划分方式;得出最终的子节点: N1、N2....Nm 4. 对子节点N1、N2....Nm分别继续执行2-3步，直到每个最终的子节点都足够&amp;#39;纯&amp;#39;。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;特征属性类型&lt;/h3&gt;

&lt;p&gt;根据特征属性的类型不同，在构建决策树的时候，采用不同的方式，具体如下: &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt; 属性是离散值，而且不要求生成的是二叉决策树，此时一个属性就是一个分支&lt;/li&gt;
	&lt;li&gt; 属性是离散值，而且要求生成的是二叉决策树，此时使用属性划分的子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支&lt;/li&gt;
	&lt;li&gt;属性是连续值，可以确定一个值作为分裂点split_point，按照&amp;gt;split_point和&amp;lt;=split_point生成两个分支&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;决策树分割属性选择&lt;/h3&gt;

&lt;ul&gt;
	&lt;li&gt;决策树算法是一种“贪心”算法策略，只考虑在当前数据特征情况下的最好分割方式，不能进行回溯操作。&lt;/li&gt;
	&lt;li&gt;对于整体的数据集而言，按照所有的特征属性进行划分操作，对所有划分操作的结果集的“纯度”进行比较，选择“纯度”越高的特征属性作为当前需要分割的数据集进行分割操作，持续迭代，直到得到最终结果。决策树是通过“纯度”来选择分割特征属性点的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;量化纯度&lt;/h3&gt;

&lt;p&gt;基尼系数：$Gini = 1 - \sum_{i=1}^nP(i)^2$&lt;/p&gt;

&lt;p&gt;信息熵：$-\sum_{i=1}^n P(i)log_2(P(i))$&lt;/p&gt;

&lt;p&gt;误差率：$Error = 1 - max_{i=1}^n \{P(i)\}$&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;p&gt;上面是基础的一些系数，在剪枝和判断的时候需要一个种体现变化的系数。&lt;/p&gt;

$$
Gain = \Delta = H(D)-H(D|A)
$$
&lt;p&gt;可以这么理解，是否决策树以A为节点划分后，信息量增加了。也可以间接体现决策树越往下发生该事件的概率越小信息量越大。&lt;/p&gt;

&lt;h3&gt;停止条件&lt;/h3&gt;

&lt;p&gt;决策树构建的过程是以恶递归的过程，以下是两个停止条件。&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;每个字节点只有一种类型时停止条件。（容易过拟合）&lt;/li&gt;
	&lt;li&gt;节点中记录数小于某个阀值的时候，或者迭代次数达到给定值的时候停止构建。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;决策树的评估&lt;/h3&gt;

$$
loss = \sum_{t=1}^{leaf} \frac{|D_t|}{D}H(t)
$$
&lt;p&gt;其中$H(t)$前的参数$\frac{D_t}{d}$主要的目的其实是给信息熵加权值。&lt;/p&gt;

&lt;h3&gt;算法对比&lt;/h3&gt;

&lt;h4&gt;ID3算法优缺点&lt;/h4&gt;

&lt;p&gt;ID3算法是决策树的一个经典的构造算法，内部使用&lt;strong&gt;信息熵&lt;/strong&gt;以及&lt;strong&gt;信息增益&lt;/strong&gt;来进行 构建;每次迭代选择信息增益最大的特征属性作为分割属性。&lt;/p&gt;

&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;决策树构建速度快，实现简单。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优&lt;/li&gt;
	&lt;li&gt;ID3算法不是递增算法 ID3算法是单变量决策树，对于特征属性之间的关系不会考虑 抗噪性差&lt;/li&gt;
	&lt;li&gt;只适合小规模数据集，需要将数据放到内存中&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;C4.5&lt;/h4&gt;

&lt;p&gt;在ID3算法的基础上，进行算法优化提出的一种算法(C4.5)。其使用&lt;strong&gt;信息增益率&lt;/strong&gt;来取代ID3算法中的信息增益，在树的构造过程中&lt;strong&gt;会进行剪枝操作进行优化&lt;/strong&gt;。能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。信息增益率公式如下：&lt;/p&gt;

$$
Gain\_ratio(A) = \frac{Gain(A)}{H(A)}
$$
&lt;p&gt;优点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;产生的规则易于理解&lt;/li&gt;
	&lt;li&gt;准确率较高实现简单&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;缺点：&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;对数据集需要进行多次顺序扫描和排序，所以效率较低 &lt;/li&gt;
	&lt;li&gt;只适合小规模数据集，需要将数据放到内存中&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;CART&lt;/h4&gt;

&lt;p&gt;使用基尼系数作为数据纯度的量化指标来构建的决策树算法就叫做 CART(Classification And Regression Tree，分类回归树)算法。CART算法使用GINI增益作为分割属性选择的标准，选择GINI增益最大的作为当前数据集的分割属性。可用于分类和回归两类问题。&lt;strong&gt;注意的是：CART构建是二叉树，同时GINI系数的计算不牵扯对数运算比较快&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GINI增益公式如下：&lt;/p&gt;

$$
Gain = \Delta = Gini(D) - Gini(D|A)
$$
&lt;h4&gt;总结&lt;/h4&gt;

&lt;ul&gt;
	&lt;li&gt;ID3和C4.5算法均只适合在小规模数据集上使用&lt;/li&gt;
	&lt;li&gt;ID3和C4.5算法都是单变量决策树&lt;/li&gt;
	&lt;li&gt;当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差&lt;/li&gt;
	&lt;li&gt;决策树分类一般情况只适合小数据量的情况(数据可以放内存)&lt;/li&gt;
	&lt;li&gt;CART算法是三种算法中最常用的一种决策树构建算法。&lt;/li&gt;
	&lt;li&gt;CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;三种算法的区别仅仅只是对于当前树的评价标准不同而已，ID3使用信息增益、 C4.5使用信息增益率、CART使用基尼系数。&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">决策树 信息熵 信息量:指的是一个样本/事件所蕴含的信息，如果一个事件的概率越大，那么就 可以认为该事件所蕴含的信息越少。</summary></entry><entry><title type="html">《滚雪球》- 爱丽丝·查理芒格</title><link href="http://localhost:4000/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/08/%E6%BB%9A%E9%9B%AA%E7%90%83-%E7%88%B1%E4%B8%BD%E4%B8%9D-%E6%9F%A5%E7%90%86%E8%8A%92%E6%A0%BC/" rel="alternate" type="text/html" title="《滚雪球》- 爱丽丝·查理芒格" /><published>2018-08-08T00:00:00+08:00</published><updated>2018-08-08T00:00:00+08:00</updated><id>http://localhost:4000/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/08/%E3%80%8A%E6%BB%9A%E9%9B%AA%E7%90%83-%E7%88%B1%E4%B8%BD%E4%B8%9D%C2%B7%E6%9F%A5%E7%90%86%E8%8A%92%E6%A0%BC%E3%80%8B</id><content type="html" xml:base="http://localhost:4000/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/08/%E6%BB%9A%E9%9B%AA%E7%90%83-%E7%88%B1%E4%B8%BD%E4%B8%9D-%E6%9F%A5%E7%90%86%E8%8A%92%E6%A0%BC/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;《滚雪球》 - 爱丽丝·查理芒格&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;读书笔记&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;在巴菲特看来，真正富有的人生应该是这样的：做一份自己喜欢的工作，找到兴趣相投的朋友。只要能做到这些，你的人生也一样是成功的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;正如巴菲特的总结，做一个自己能感到自己价值的工作，找到能互相认可的人也许才是人生的最大意义。在努力的做自己的喜欢的工作的时候才能更容因进入”专注“的境界。而兴趣相投的朋友则会让你的生活变得丰富，人说到底还是群居的动物，社交是必要的也是维持正常心理所必须要的要素。&lt;/p&gt;

&lt;p&gt;同时他们之间也是互通的，一个好的社交保持了好的积极的心态，保持了良好的且多元的信息社区，促进了工作的进步，工作的进步获得了回报，在财力和心理上形成正向激励，达成”飞轮效应“，进步越来越开实现复利增长。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;巴菲特有三宝，“内部积分卡”，“专注”，“复利”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;内部积分卡指的是，自己的行为有自己的评价标准，自己来定义自己是什么样的人，而不是由外部来定义自己，这样做到之后就会达到一种非常好的内心自洽。正如熊太行老师讲的那样，其实有时候内心自洽是一个强大者的开端，能让我们不去盲目的做一些违背自我的决定。&lt;/p&gt;

&lt;p&gt;“异于常人”的专注，其实很多时候是别人不能复制你成功的最大的壁垒。有着异于常人的专注的巴菲特能够通过专注找到“地上的烟蒂”，而缺了专注的行业其他人即便知道了方法也不能发现“烟蒂”。&lt;/p&gt;

&lt;p&gt;复利，在滚雪球中，复利的魅力用于体现在投资上，每次都能在之前的基础上以一个固定的比例进行增长，之后会越涨越快一发不可收拾。这点在吴军的《Google方法论》和《硅谷来信》，以及吴伯凡老师也多次提及，不过名词略有不同，吴军老师称之为指数增长，吴伯凡老师称之为飞轮效应。都在强调可叠加的魅力。如果放在生活中，我们应该常常反思，我们现在做的工作是不是可以叠加的，是不是在以大利益行动，是不是能成为未来的自己工作学习的基础。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;人生就像滚雪球，最重要的就是发现湿雪和长长的山坡&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这句话特别经典，也被很多人视为人生格言。我的感悟是，湿雪不难找到，但是有耐心和勇气去用人生这一个有限的时间，去选择一个长而不急的山坡不是每个人都能做到的。很多人看到别人在急坡中一时的成功，便忍不住放弃自己的计划，跟随别人，最终摔得人仰马翻。随后重复上述过程，直到到达山底最终一无所获。最佳的策略也许是认准一个“湿雪”较多的路径，随着不陡不缓的山坡，不急不躁，把自己的雪球越滚越大。&lt;/p&gt;

&lt;h2&gt;个人总结&lt;/h2&gt;

&lt;p&gt;今年开始，随着读的书越来越多，发现很多时候抛开大佬们成功的表象背后，总有一些惊人的相似的地方。&lt;/p&gt;

&lt;p&gt;首先是兴趣和专注，吴军说“努力只能让你成为行业的前20%，而兴趣决定了你是否能走完最后的10%”，自己不知道从什么时候起养成了一种选择困难症，虽然确定了计算机机器学习这个大方向，但是却没有一个具体的目标，导致近一个月来学习重心的摇摆不定，在机器学习，爬虫和区块链中摇摆不定。虽然他们学习的进度都还不错，爬虫接了北京高教所得项目，区块链得到了Hackathon竞赛的奖，机器学习算是初步入门正在踏入深度学习的领域，但是真真切切的感受到了自己能力的天花板，即“我没有能力去同时专精所有的方向”，方向仍需做减法。&lt;/p&gt;

&lt;p&gt;其次是“复利”，可叠加性的增长最有意思的是，往往这种复利型的进步开始的时候很困难，很累很苦，需要一个很强的内心去坚持，但随着飞轮开始转动一切就会变得理所当然。这点有一点小小的体会吧，这点体现在VIM的配置上面，开始的时候很艰难，各种快捷键都不知道是什么，觉得无比难用。不过，在了解到真的很多大佬是真的使用VIM或者类似的我认为超级难用的编辑器作为生产工具的时候，觉得也许应该坚持下来，不过没想到真的VIM在日后成为了我主要的生产工具。开始的时候只是配置了代码高亮，后来渐渐加入了代码补全，再后来补充了各种各样插件并学习了更多的快捷键，最后甚至自己能编一些脚本来实现自己想要的功能。那么它给我带来了什么“复利增长呢”，减少了不必要的分心，所有的学习需要的工具大多都能通过统一的途径获得，操作上不需要太多的改动，从论文的书写到代码的规范，同时由于打开很快的特性，能让我在有想法的时候就快速的写代码来测试，这些收获让我觉得之前的小努力都是值得的。同样的，我也相信算法，统计学和离散数学，包括博弈论，这些很多难啃又不会有立竿见影成效的东西最终会成为自己职业生涯复利增长的第一步。&lt;/p&gt;

&lt;p&gt;最后是内心自洽，个人自身的感受是，如果内心不能够自洽的话，很多精力都在花费在平复情绪上进行了没必要的经历开支，甚至“因为别人都这么做所以我也要这么做的”作出欠思考的行动，或者因为“情谊”做出了完全违背自己大利益的行动。行事内心要一把尺子，这把尺子要拿稳了。气场和魄力的起点便是内心自洽，如果自己都不能坚信自己的决定，又谈何让别人相信你的决定呢？&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="Reading" /><summary type="html">《滚雪球》 - 爱丽丝·查理芒格 读书笔记 在巴菲特看来，真正富有的人生应该是这样的：做一份自己喜欢的工作，找到兴趣相投的朋友。只要能做到这些，你的人生也一样是成功的。</summary></entry><entry><title type="html">回归算法下</title><link href="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/07/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E4%B8%8B/" rel="alternate" type="text/html" title="回归算法下" /><published>2018-08-07T00:00:00+08:00</published><updated>2018-08-07T00:00:00+08:00</updated><id>http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/07/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E4%B8%8B</id><content type="html" xml:base="http://localhost:4000/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/07/%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95%E4%B8%8B/">&lt;!DOCTYPE html&gt;
&lt;html&gt;
	&lt;head&gt;
		&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;
		&lt;meta charset=&quot;utf-8&quot; /&gt;
		&lt;title&gt;回归算法下&lt;/title&gt;
	&lt;/head&gt;
&lt;body&gt;
&lt;h2&gt;序言&lt;/h2&gt;
&lt;p&gt; 在机器学习02（回归算法上）中我们介绍了普通最小二乘线性回归算法，并进行了较为详细的推导，并通过分析其过拟合的问题，推导出了另外三个算法，Ridge回归算法，LASSO回归算法，以及弹性网络。并简要的分析了他们的优缺点。今天我们来接着介绍算法。为什么说是算法而不是回归算法呢，是因为在研究了逻辑回归和Softmax回归算法以后，惊讶的发现这两个算法是分类算法，所以这个回归算法下的说法就不是很严谨了。&lt;/p&gt;

&lt;h2&gt;Logistic回归&lt;/h2&gt;

&lt;p&gt;在回归算法上中我们介绍了，线性回归算法。我个人是这么拆分的，线性-回归-算法，回归指的整体的算法是回归算法而不是分类算法，线性指的是在算法回归中，假定输入输出之间关系函数是$y = kx + b$这个线性方程。&lt;/p&gt;

&lt;p&gt;那么再看看Logistic回归，显然这里肯定就是换了另一个种类的方程咯。对的，不过在逻辑回归中这里有一点不太一样，它的输出只有0和1，它假定输出 $y = 1$的概率$P$，自然 $y = 0$的概率就是 $1-P$，而逻辑回归中假定的方程就是这个$P$。&lt;/p&gt;

&lt;p&gt;现在我们来看看这个方程是什么，如下公式1.1所示。这个公式没有接触过的朋友可能不太好想这是一个什么函数，但是结合着我们高数的极限知识，我们肯定知道这个函数在$+\infty $的时候去趋近于1，在$-\infty$的时候趋近于0，而且是快速收敛的。建议百度sigmoid函数看看这个函数的图（Logistic函数就是sigmoid函数）。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;$p=h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $		公式1.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;之前在线性回归算法中我们知道了求最佳$\theta$的流程就是先写出他的似然函数（这个公式表示了预测正确的概率），然后求最大似然估计（想办法让这个预测正确的概率最大），最终通过求最大似然估计的结果，得到一种调整$\theta$的最佳方案。这个逻辑回归的似然函数为公式1.3(由公式1.2易得)。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;$L(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}$	公式1.2&lt;/li&gt;
	&lt;li&gt;$L(\vec y|x;\theta)=\prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}$					公式1.3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;之后对这个似然函数进行求导，我们都学过导数，在学导数的时候都知道，导数代表着值变化的趋势和速度。这里我们通过公式1.3对求对数，然后再对 $\theta_j $求导得到导数（公式1.4），这个导数就代表了针对$\theta_j $的正确预测的概率的变化趋势。如果让 $\theta$加上这个导数就可以保证让这个公式变大，直至最大为止。所以逻辑回归的调参如公式1.5，公式1.6所示，那么为什么是两个，在调参章节中会说明。&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;$\frac{\partial \ell(\theta)} {\partial \theta_j} = \sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j$ 公式1.4&lt;/li&gt;
	&lt;li&gt;$\theta_j = \theta_j + \alpha \sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j$	公式1.5	&lt;/li&gt;
	&lt;li&gt;$\theta_j = \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)}))x_j $公式1.6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;好的，到这里逻辑回归的调参我们就知道了，特别的说明的是，这里的 $alpha$就是传说中的学习速率，而这个参数我们要注意的是，它不能太大（会错过最大值而不能收敛），也不能太小（会收敛到局部最小值）。&lt;/p&gt;

&lt;p&gt;那么我们有了调参，这里肯定也有一个损失函数来表示现在预测的怎么样吧，这里的思路很简单，你不是似然函数是越大越好么，那么我把你来个倒数不就好了么，最简单的做法就是给求对数之后的公式1.3，添加一个负号。所以它的损失函数如公式1.7。&lt;/p&gt;

$loss(y^{(i)},\hat y^{(i)}) = -\ell(\theta) = \sum_{i=1}^m ln(1+e^{(1-2y^{(i)})\theta^T x^{(i)}}) $	公式1.7
&lt;p&gt;最后，想说从那个 只能为1或者0就可以看出逻辑回归虽然名字带了回归两个字，但是其实是一个分类算法。而接下来的SoftMax回归算法则是逻辑回归算法的一种拓展，这个在下一节里说。&lt;/p&gt;

&lt;h2&gt;SoftMax回归&lt;/h2&gt;

&lt;p&gt;在逻辑回归中，最终的分类结果，只有两类。这显然不是适用于很多其他的情况。所以SoftMax对逻辑回归进行了一般化，适用于K分类的问题。&lt;/p&gt;

&lt;p&gt;针对K分类的问题，我们的小伙伴 $\theta$参数就不在是一个向量了，我们设第K类的参数为向量$\theta_k$， 则有n个属性的参数就成了一个二维矩阵$\theta_{k*n}$。&lt;/p&gt;

&lt;p&gt;在Softmax回归中，我们设预测对第k类的概率为公式2.1。&lt;/p&gt;

&lt;p&gt;$p(y=k|x;\theta)=\frac{e^{\theta_k^T x}}{\sum_{i=1}^K e^{\theta_i^T x}} $, $k=1,2 ...,K $公式2.1&lt;/p&gt;

&lt;p&gt;剩下的分析同逻辑回归一样就不赘述了。&lt;/p&gt;

&lt;h2&gt;机器学习调参&lt;/h2&gt;

&lt;p&gt;我们在之前的学习中，了解到机器学习在迭代的过程就是不断的调整$/theta$参数的过程。有些是算法自己就调整了，有些是需要我们的人工的来调整的，这里就要引入超参这个概念了。什么事超参呢，超参就是不能通过算法自动调整的参数，比如Ridige回归和LASSO回归的 $\lambda$，弹性网络中的 $\alpha$。&lt;/p&gt;

&lt;p&gt;除了这些，在Logistic回归这一章节中，我们发现了调参函数有两个，但是并没有说明为什么。在这里将进行详细介绍。在Logistic和Softmax回归中我们用到的调参方式，只要你稍微了解过深度学习就一定听过，这种调参方式就是大名鼎鼎的梯度下降算法。而这两个公式，公式1.5是批量梯度下降算法（BGD），公式1.6是随机梯度下降算法（SGD）。&lt;/p&gt;

&lt;p&gt;通过公式1.6我们可以看出，每次迭代SGD调整一个$\theta_j$只需要和其中一条属性的比，而BGD每次迭代每调整一个$\theta_j$需要和所有属性的比较，所以SGD迭代速度快。&lt;/p&gt;

&lt;p&gt;那么SGD每次迭代考虑的属性少会不会没有BGD准呢，这是不一定的。梯度下降算法就像大雾天气下山，我们只能看清眼前的一小部分，并以此为依据下山，我们很有可能最后在山的上的一个小坑里出不来，陷入局部最优解的问题。SGD在全局餐在多个相对最优解的情况下，SGD很有可能跳出某些局部最优解，所以不一定会比BGD坏。而BGD一定能够得到一个局部最优解（在线性回归中一定是得到一个全局最优解）。不过由于SGD少考虑一些情况，所以有随机性，因而最终结果可能会比BGD差，一般情况下我们会优先使用SGD。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;注：&lt;/strong&gt;个人的简单理解是，SGD每次只会往某一个坐标轴方向走一步，而BGD则是结合所有坐标轴的情况，往一个空间内的一个方向走一步。所以理论上BGD很“理性”。&lt;/p&gt;

&lt;p&gt;SGD和BGD出现的优缺点的情况是不是很熟悉，是不是很像Ridge回归和LASSSO回归的抉择，最终出现了弹性网络。那么是不是也有一个类似梯度下降算法的“弹性网络”呢，答案显然是有的，那就是MBGD，我不全不考虑完不就好了嘛。MBGD中每次拿b个样本的平均梯度作为更新方向，这里的b一般为10。这样子就既保证了速度，也一定程度上保证了准确度。&lt;/p&gt;

&lt;h2&gt;模型效果判断&lt;/h2&gt;

&lt;p&gt;最后，我们模型也训练好了，那么怎么来判定我的模型是不是合乎标准的，就像吴军老师在Google方法论说的一样，在工程中不是只有对错，只有相对的好和相对的不好，这个模型是否符合项目的需要也是有几个标准的，在训练的时候需要注意最终目标需要达到的标准是什么。常用的有MSE（公式4.1）、RMSE（公式4.2）、$R^2$（公式4.3）。&lt;/p&gt;

&lt;p&gt;在看公式前先说明TSS和RSS的概念，用在算$R^2$上。&lt;/p&gt;

&lt;p&gt;TSS(Total SUm of Squares)：总平方和TTS，样本之间的差异性。是伪方差的m倍。RSS：是预测值和样本值之间的差异，是MSE的m倍。&lt;/p&gt;

$$
MSE=\frac{1}{m}\sum_{i=1}^m(y_i - \hat y_i)^2\ \ \ 公式4.1
$$
$$
RMSE=\sqrt{MSE} \ \ \ 公式4.2
$$
$$
R^2 = 1 - \frac{RSS}{TSS} = 1-\frac{\sum_{i=1}^m(y_i-\hat y_i)^2}{\sum_{i=1}^m(y_i-\overline y)^2}\ \ \ 公式4.3
$$				
&lt;p&gt;通过看公式我们可以得到以下的结论。&lt;/p&gt;

&lt;p&gt;MSE：误差平方和，越趋近于0越拟合。&lt;/p&gt;

&lt;p&gt;RMSE：就是对MSE开根号，所以和MSE的判别一样。&lt;/p&gt;

&lt;p&gt;$R^2$：值域为，最优解是1，若预测值恒为样本期望则为0&lt;/p&gt;

&lt;p&gt;除了这些，还有一个叫做混淆矩阵的东西，也是评价模型的一种手段，可以之后搜索看看。&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</content><author><name>DeamoV</name></author><category term="MachineLearning" /><summary type="html">回归算法下 序言 在机器学习02（回归算法上）中我们介绍了普通最小二乘线性回归算法，并进行了较为详细的推导，并通过分析其过拟合的问题，推导出了另外三个算法，Ridge回归算法，LASSO回归算法，以及弹性网络。并简要的分析了他们的优缺点。今天我们来接着介绍算法。为什么说是算法而不是回归算法呢，是因为在研究了逻辑回归和Softmax回归算法以后，惊讶的发现这两个算法是分类算法，所以这个回归算法下的说法就不是很严谨了。</summary></entry></feed>