---
layout: post
title: "EM-最大期望算法"
date: 2020-11-04
categories: 机器学习
tags: [MachineLearning]
---
## 必要的前置知识
1. 最大似然估计（MLE）：找出一组参数(模型中的参数)，使得似然函数（模型的输出概率）的值最大。
2. 贝叶斯算法估计：从先验概率和样本分布情况来计算后验概率的一种方式。
3. 最大后验概率估计（MAP）：求 $\theta$ 使 $P(x\|\theta)P(\theta)$ 的值最大，这也就是要求 $\theta$ 值不仅仅是让似然函数最大，同时要求 $\theta$ 本身出现的先验概率也得比较大。
4. Jensen 不等式：如果函数 $f$ 为凸函数，那么存在公式$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta) f(y)$，进一步推论得到若 $\theta_1,...\theta_k \geq 0$  且$\theta_1+\theta_2+...+\theta_k = 1 $，则有 $f(\theta_1 x_1 + ... + \theta_k x_k) \leq \theta_1 f(x_1) + ... + \theta_k f(x_k)$。这里会在后续 EM 的公式推导中使用到，证明可以看[这里](https://en.wikipedia.org/wiki/Jensen%27s_inequality)。

## EM 算法的作用
最大期望演算法（Expectation-maximization algorithm）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。

EM 算法和 MLE 和 MAP 的最重要的一个区别是待求的参数 $\theta$ 依赖于无法观测的隐变量。因而无法直接用 MLE 或者 MAP 直接求解。值得一提的是，其针对隐变量和待求参数的交替更新来最大化似然函数是，可靠的存在完整的证明过程的。

## EM 算法的使用
正如上一段所说，EM 算法是一种交替更新的算法，也求参数的时候一种较为常用的算法思路。这种算法主要分为以下两个循环操作，直到似然函数收敛：
1. E 步骤：利用对隐藏变量的现有估计值，计算其最大似然估计值。
$$
Q(z) := p(z|x;\theta)
$$

2. M 步骤：最大化在 E 步上求得的最大似然值来计算参数的值 $\theta$。
$$
\theta:=\arg \max _{\theta} \sum_{z} Q_{i}\left(z\right) \log \frac{p\left(x, z ; \theta\right)}{Q\left(z\right)}
$$

这个算法的证明会在下一个章节中详细阐述，这里就简单的阐述下思想。就是最终的似然函数 $log(p(x;\theta))$ 中参数 $\theta$ 依赖于某个隐藏变量 $z$ ，我们每次交替更新 $z$ 和 $\theta$ 都能使得最终的似然函数变大。

*注：这里的隐藏变量 $z$ 不是使用 loss 更新的，第一次学 EM 的时候在这里卡了很久，特此纪念。*

## EM 算法证明
根据 EM 算法的使用，我们知道 EM 算法证明的核心在于为什么当 $Q(z) := p(z|x;\theta)$ 的时候，执行 M 步骤时能够保证似然函数的值单调递增。这里主要看的就是 Jessan 不等式的取等条件了。接下来就是证明步骤了，建议把 jessan 不等式的一些结论放在旁边：
$$
\begin{split}
l(\theta) &= \sum_{i=1}^m log\sum_z p(x_i,z;\theta) \\
&= \sum_{i=1}^m log \sum_z Q(z;\theta) * \frac{p(x_i,z;\theta)}{Q(z;\theta)}\ \ \ 步骤1 \\
&= \sum_{i=1}^m log(E_Q(\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤2 \\
&\geq \sum_{i=1}^m E_Q(log(\frac{p(x_i,z;\theta)}{Q(z;\theta)}))\ \ \ 步骤3 \\
&= \sum_{i=1}^m \sum_z Q(z;\theta) log (\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤3
\end{split}
$$

首先，我们明确的目标是得到最佳参数 $\theta$ 也就是说，我们企图得到一种可以把似然函数写成纯 $\theta$ 的公式，之后进行通过求偏导的方式来得到我们要求的最佳参数 $\theta$。

步骤一中，我们引入了隐藏数据的分布 $Q(z;\theta) $ , 根据 $Q(z;\theta) > 0$ ， 所以我们加入这个参数是不会改变原函数的数值。又由于 分布函数的性质 $\sum_z Q(z;\theta)=1 $ ，之后根据数学期望的定义将右边式子写成了数学期望的形式，于是得到了步骤二。

在步骤三中，根据根据 Jensen 不等式，我们可以得到 $f(E(x)) \leq E(f(x))$，所以我们将 $log$ 塞到期望函数里面，最终得到了步骤四。

到这里为止，我们的核心步骤已经出现了，如果等号取等的话，根据 Jensen 不等式的取等条件 $\frac{p(x,z;\theta)}{Q(z;\theta)} = c$ ，可以得到下面的关系公式：

$$
Q(z,\theta) = \frac{p(x,z;\theta)}{c} = \frac{p(x,z;\theta)}{c*\sum_{z_i}p(x,z_i;\theta)} = \frac{p(x,z;\theta)}{p(x;\theta)} = p(z\|x;\theta)
$$

到这里，我们就可以看到整个似然函数的下界就是以 $\theta$ 为参数的后验概率（条件概率）。所以我们写出来 $Q(z,\theta)$ 以后直接更新 $\theta$ 就好了。按照个人的理解中，我们使用模型预测结果的时候，那个 softmax 就可以理解为 $Q$ 函数。

*注：之前总是被很多博客中那个下确界不断提升的图搞的晕头转向。个人觉得这里把住 EM 算法中就只有 M 步骤中存在一次参数更新就可以了。*

## 结语
整体而言，对于实际应用的角度来看，EM 算法算是一种思想吧。不论搞算法的小伙伴实际理不理解其实大家都在用，比如我们常用的更新模型参数的时候实际上就用了这个思想。不过，虽然理解这个算法没有对实际实践中带来太多的启发，但是搞清楚了一个之前模模糊糊的一个概念还是值得开心的。

## 参考资料
- [EM-最大期望算法](http://www.csuldw.com/2015/12/02/2015-12-02-EM-algorithms/)

## ChangeLog
- 2020-11-04：加入了自己对 EM 算法的理解
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
