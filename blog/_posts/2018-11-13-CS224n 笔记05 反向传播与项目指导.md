---
layout: post
title: "CS224n 笔记05 反向传播与项目指导"
date: 2018-11-13
categories: NLP
tags: ["NLP" , "CS224"]
---

这次的课比较轻松，简单的来说就是链式法则，帅小哥为了让学生更能理解所以用了不同的方式来讲解这个 BP 的推导。流程图的形式的推导对之后的 Tensorflow 的直观理解很有帮助，不过这里就只保留 BP 推导后的一些经验的总结。
## 为什么要推导 BP 算法呢
1. 真正理解神经网络背后的数学 - **反向传播算法**
2. Backprop can be an imperfect abstraction - **BP 还是不完善的**
	- 如会出现梯度消失、梯度爆炸等
3. 当你 Debug 模型，和设计新模型的时候会需要懂它

## 需要记忆的内容
BP 整体就是链式法则的推导，但是作为以后可能会经常使用的数学工具，我们希望有一些经验性的总结来方便我们之后的计算。
- 第 $l$ 层的残差：
	- $\delta^{(l)} = (W^{(l)T}\delta^{(l+1)}) \odot f'(z^{(l)})$  
	其中残差为根据某个损失函数得到的误差，而 $\odot$ 为相同大小的向量之间的 element wise product。同时 $f$ 为激活函数， $z$ 为线性函数 。
- 对第 $l$ 层的权值矩阵的梯度：
	-  $\frac{\partial E_R}{\partial W^{(l)}} = \delta^{(l+1)}(a^{(l+1)})^T + \lambda W^{(l)}$ 
 - 对第 $l$ 层的偏置的梯度：
	- $\frac{\partial E_R}{\partial b^{(l)}} = \delta^{(l+1)} + \lambda b^{(l)}$

	*注：其中 $a$ 是激活值： $a^{(l)} = f(z^{(l)})$* 

## BP 的细节理解
除了通用的数学推导，帅小哥还是用了另外三种方式来推理。其中个人感觉流程图中的介绍更适用于理解画出来的神经网络，而电路的模式更接近 tensorflow 的编程思想。具体的内容就不赘述了，内容对之后反过来查找回忆帮助不大。感兴趣的读者可以参考大佬的[这一章笔记](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html)的博客。

## 课程项目(直接转自大佬的总结)
- 不要想着一上来就发明个新模型搞个大新闻
- 也不要浪费大部分时间在爬虫上面，本末倒置
- 把旧模型用于新领域\\新数据也是不错的项目
- 先要按部就班地熟悉数据、熟悉评测标准、实现基线方法
- 再根据基线方法的不足之处思考深度学习如何能带来改进
- 再实现一个已有的较为前沿的模型
- 观察该模型犯的错误，思考如何改进
- 这时才能没准就福至心灵发明一个新方法

## 参考
- [大佬的博客](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html)
- [课程第五课](https://www.bilibili.com/video/av30326868/?p=5)
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
