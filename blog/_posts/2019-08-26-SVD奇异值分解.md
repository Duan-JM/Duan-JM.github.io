---
layout: post
title: "SVD 奇异值分解"
date: 2019-08-26
categories: 数据分析
tags: ["数据降维" , "MachineLearning", "MathTools"]
---
### 简述
SVD（Singular Value Decomposition），即奇异值分解，本质上是一种矩阵分解技术，在应用领域已经出现了进百年。矩阵分解技术指的是将一个原始矩阵表示成新的易于处理的形式，这种形式是两个或多个矩阵的乘积，可以简单的理解为因式分解。最早使用 SVD 的应用的领域为信息检索，使用 SVD 的检索方法称为隐性语义索引（Latent Semantic Index，**LSI**）或隐性语义分析（Latent Semantic Analysis，**LSA**）。

### 前置知识
- 特征值与特征矩阵
- 基本的线性代数矩阵基础

### 细节说明

SVD 将原始的数据集矩阵  $A$  分解成三个矩阵  $U$、 $\Sigma$ 、 $V^T$。若原始矩阵为  $m*n$ 的矩阵，那么 $U$ 的维度为 $m*m$， $\Sigma$ 的维度为 $m*n$， $V^T$ 的维度微 $n*n$，写成公式如下：

$$
A_{m*n} = U_{m*m}\Sigma_{m*n}V^T_{n*n} 
$$

其中 $\Sigma$ 为对角矩阵，这些对角上的值为**奇异值**，这个奇异值和特征值是有关系的，它是矩阵  $A^TA$ 的特征值的平方根。 左奇异矩阵 $U$ 为  $A^T A$ 的所有特征向量，右奇异矩阵  $V$ 为 $A A^T$ 的所有特征向量。

### 代码使用

实现方式已经很成熟啦，使用 numpy 的库就好啦，函数如下：

```python
from numpy import *
U,Sigma,VT = linalg.svd(Matrix)    
```

*注：详细实现过程参考《Numerical Linear Algebra》*

### 详细的博客
- [知乎琥珀躺对 SVD 的讲解](https://zhuanlan.zhihu.com/p/31386807)（很详细）

<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath:[['$','$']]}});</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
